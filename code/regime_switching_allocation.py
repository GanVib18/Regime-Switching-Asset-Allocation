# -*- coding: utf-8 -*-
"""Regime_switching_allocation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X0xw0YB16zsQXErGaJQMG_k-W4CspHpo

# Regime-Switching Asset Allocation - Consolidated Implementation
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Regime-Switching Asset Allocation - Consolidated Implementation

This script combines data preparation, HMM modeling, allocation strategy,
and model comparison into a single efficient workflow.

Designed for Google Colab - just run the cells in order!

IMPORTANT FOR COLAB:
If you get a TypeError about 'include_optimized', it means you're running
old cached code. FIX: Go to Runtime → Restart runtime, then re-run this cell.

Usage in Colab:
    1. Run this cell to define all functions
    2. Call quick_start_with_walk_forward() to execute full analysis
    3. Or call individual phases as needed
"""

import os
import sys
import warnings
from datetime import datetime
from typing import Dict, List, Tuple, Optional

warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import matplotlib.patches as mpatches
import seaborn as sns
import joblib

try:
    import yfinance as yf
    from hmmlearn.hmm import GaussianHMM
    from sklearn.preprocessing import StandardScaler
    from scipy.optimize import minimize
except ImportError as e:
    print(f"Missing required package: {e}")
    print("Installing required packages...")
    import subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q",
                          "yfinance", "hmmlearn", "scikit-learn", "scipy"])
    import yfinance as yf
    from hmmlearn.hmm import GaussianHMM
    from sklearn.preprocessing import StandardScaler
    from scipy.optimize import minimize
    print("Packages installed successfully!")


# ══════════════════════════════════════════════════════════════════════════════
# CONFIGURATION
# ══════════════════════════════════════════════════════════════════════════════

class Config:
    """Centralized configuration for all modules."""

    # Data parameters
    ETF_CONFIG = {
        "XIU.TO": "iShares S&P/TSX 60 (XIU)",
        "VFV.TO": "Vanguard S&P 500 Index (VFV)",
        "XEF.TO": "iShares MSCI EAFE (XEF)",
        "XBB.TO": "iShares Cdn Bond (XBB)",
        "CGL-C.TO": "iShares Gold Bullion (CGL)",
    }

    EQUITY_TICKERS = ["XIU.TO", "VFV.TO", "XEF.TO"]
    BOND_TICKERS = ["XBB.TO"]
    GOLD_TICKERS = ["CGL-C.TO"]

    OIL_TICKER = "CL=F"
    START_DATE = "2015-01-01"
    END_DATE = "2025-01-01"

    # Feature engineering
    ROLLING_WINDOW = 20
    MOMENTUM_WINDOW = 60

    # HMM parameters
    N_REGIMES = 3
    N_ITER = 2000
    N_INIT = 20
    RANDOM_STATE = 42

    # Data splits
    TRAIN_END = "2023-12-31"
    VAL_END = "2024-12-31"

    # Regime labels and colors
    REGIME_LABELS = {
        0: "Low-Vol (Bull)",
        1: "Medium-Vol (Normal)",
        2: "High-Vol (Crisis)"
    }
    REGIME_COLORS = {
        0: "#2ECC71",
        1: "#F39C12",
        2: "#E74C3C"
    }

    # Allocation parameters
    TARGET_WEIGHTS = {
        0: {  # Bull
            "XIU.TO": 0.30, "VFV.TO": 0.25, "XEF.TO": 0.15,
            "XBB.TO": 0.20, "CGL-C.TO": 0.10
        },
        1: {  # Normal
            "XIU.TO": 0.25, "VFV.TO": 0.20, "XEF.TO": 0.15,
            "XBB.TO": 0.28, "CGL-C.TO": 0.12
        },
        2: {  # Crisis
            "XIU.TO": 0.15, "VFV.TO": 0.13, "XEF.TO": 0.12,
            "XBB.TO": 0.40, "CGL-C.TO": 0.20
        }
    }

    # Backtest parameters
    INITIAL_CAPITAL = 100_000.0
    RISK_FREE_RATE = 0.02
    REBALANCE_THRESHOLD = 0.05
    BID_ASK_SPREAD = 0.0008
    FORWARD_DAYS = 30
    CRISIS_REGIME = 2

    # Stress events for validation
    STRESS_EVENTS = {
        "COVID Crash": ("2020-02-01", "2020-05-31"),
        "2022 Rate Shock": ("2022-01-01", "2022-12-31"),
        "2018 Q4": ("2018-10-01", "2018-12-31"),
    }

    # Output directory - Colab-friendly
    OUTPUT_DIR = "/content/regime_switching_output"

    # Feature columns for HMM
    FEATURE_COLS = [
        "Market_RolVol20",
        "Bond_RolVol20",
        "Equity_Bond_Corr20",
        "Equity_Mom60",
        "Bond_Mom60",
        "Gold_Mom60",
    ]

    OPTIONAL_FEATURES = [
        "Gold_Equity_Corr20",
        "WTI_Oil_Return",
        "Credit_Spread",
        "Yield_Curve_Slope",
    ]

    # FRED macro tickers (via yfinance)
    # ^IRX = 13-week T-bill annualised yield (risk-free rate proxy)
    # ^TNX = 10-year Treasury yield
    # ^FVX = 5-year Treasury yield  (used for yield curve slope: 10Y - 2Y proxy)
    # HYG  = iShares HY Bond ETF; LQD = iShares IG Bond ETF
    #   credit spread proxy = rolling vol ratio of HYG vs LQD
    TBILL_TICKER  = "^IRX"   # 13-week T-bill yield (annualised %)
    SPREAD_TICKERS = {"HYG": "HY_Bond", "LQD": "IG_Bond"}   # US proxies available freely
    YIELD_10Y_TICKER = "^TNX"   # 10-year Treasury yield
    YIELD_2Y_TICKER  = "^IRX"   # Use 13-week as short-end proxy (2Y not freely available)


# ══════════════════════════════════════════════════════════════════════════════
# UTILITY FUNCTIONS
# ══════════════════════════════════════════════════════════════════════════════

class Utils:
    """Common utility functions used across modules."""

    @staticmethod
    def ensure_output_dir(path: str) -> None:
        """Create output directory if it doesn't exist."""
        os.makedirs(path, exist_ok=True)

    @staticmethod
    def add_year_grid(ax: plt.Axes) -> None:
        """Add year-based grid to plot."""
        ax.xaxis.set_major_formatter(mdates.DateFormatter("%Y"))
        ax.xaxis.set_major_locator(mdates.YearLocator())
        ax.grid(alpha=0.3)
        plt.setp(ax.xaxis.get_majorticklabels(), rotation=0)

    @staticmethod
    def pct_fmt(ax: plt.Axes, axis: str = "y") -> None:
        """Format axis as percentage."""
        fmt = plt.FuncFormatter(lambda x, _: f"{x:.0%}")
        if axis == "y":
            ax.yaxis.set_major_formatter(fmt)
        else:
            ax.xaxis.set_major_formatter(fmt)

    @staticmethod
    def regime_legend_patches(labels: Dict, colors: Dict) -> List:
        """Create regime legend patches."""
        return [mpatches.Patch(color=colors[r], label=labels[r])
                for r in sorted(labels)]

    @staticmethod
    def annotate_stress(ax: plt.Axes, alpha: float = 0.08) -> None:
        """Shade stress event periods."""
        for label, (start, end) in Config.STRESS_EVENTS.items():
            ax.axvspan(pd.Timestamp(start), pd.Timestamp(end),
                      alpha=alpha, color="black", zorder=0)


# ══════════════════════════════════════════════════════════════════════════════
# MODULE 1: DATA PREPARATION
# ══════════════════════════════════════════════════════════════════════════════

class DataPreparation:
    """Handle data download and feature engineering."""

    def __init__(self, config: Config):
        self.config = config

    @staticmethod
    def _normalize_index(df: pd.DataFrame) -> pd.DataFrame:
        """Normalize index to tz-naive midnight timestamps."""
        idx = df.index
        if isinstance(idx, pd.DatetimeIndex):
            if idx.tz is not None:
                idx = idx.tz_convert("UTC").tz_localize(None)
            df.index = idx.normalize()
        df.index.name = "Date"
        return df

    @staticmethod
    def _flatten_columns(df: pd.DataFrame) -> pd.DataFrame:
        """Flatten MultiIndex columns from yfinance."""
        if isinstance(df.columns, pd.MultiIndex):
            df.columns = df.columns.get_level_values(0)
        return df

    def download_single(self, ticker: str, start: str, end: str) -> Optional[pd.Series]:
        """Download adjusted close prices for one ticker."""
        try:
            raw = yf.download(
                ticker, start=start, end=end,
                auto_adjust=True, progress=False,
                multi_level_index=False
            )
        except TypeError:
            raw = yf.download(
                ticker, start=start, end=end,
                auto_adjust=True, progress=False
            )

        if raw is None or raw.empty:
            return None

        raw = self._flatten_columns(raw)
        raw = self._normalize_index(raw)

        if "Close" not in raw.columns:
            return None

        return raw["Close"].rename(ticker)

    def download_prices(self) -> pd.DataFrame:
        """Download all ETF prices."""
        print(f"\n[Data] Downloading price data ({self.config.START_DATE} to {self.config.END_DATE})...")

        price_series = {}
        failed = []

        for ticker in self.config.ETF_CONFIG.keys():
            print(f"  Fetching {ticker}...", end=" ", flush=True)
            s = self.download_single(ticker, self.config.START_DATE, self.config.END_DATE)
            if s is None:
                print("FAILED")
                failed.append(ticker)
            else:
                price_series[ticker] = s
                print(f"OK ({len(s)} rows)")

        if failed:
            print(f"  WARNING: Failed tickers: {failed}")
        if not price_series:
            raise RuntimeError("No ETF data downloaded. Check internet connection.")

        # Inner join
        prices = pd.concat(price_series.values(), axis=1, join="inner")
        prices.sort_index(inplace=True)

        print(f"  Shape: {prices.shape} | {prices.index[0].date()} → {prices.index[-1].date()}")
        return prices

    def calculate_features(self, prices: pd.DataFrame, daily_returns: pd.DataFrame) -> pd.DataFrame:
        """Engineer features from prices and returns."""
        print("\n[Data] Engineering features...")

        features = pd.DataFrame(index=daily_returns.index)
        etf_tickers = list(self.config.ETF_CONFIG.keys())

        # Per-ETF rolling volatility
        for ticker in etf_tickers:
            if ticker in daily_returns.columns:
                features[f"{ticker}_RolVol20"] = (
                    daily_returns[ticker]
                    .rolling(self.config.ROLLING_WINDOW)
                    .std() * np.sqrt(252)
                )

        # Equity-Bond correlation
        if "XIU.TO" in daily_returns.columns and "XBB.TO" in daily_returns.columns:
            features["Equity_Bond_Corr20"] = (
                daily_returns["XIU.TO"]
                .rolling(self.config.ROLLING_WINDOW)
                .corr(daily_returns["XBB.TO"])
            )

        # Market-wide volatility (average equity)
        equity_present = [t for t in self.config.EQUITY_TICKERS if t in daily_returns.columns]
        if equity_present:
            avg_equity = daily_returns[equity_present].mean(axis=1)
            features["Market_RolVol20"] = (
                avg_equity.rolling(self.config.ROLLING_WINDOW).std() * np.sqrt(252)
            )

        # Bond volatility (average)
        bond_present = [t for t in self.config.BOND_TICKERS if t in daily_returns.columns]
        if bond_present:
            avg_bond = daily_returns[bond_present].mean(axis=1)
            features["Bond_RolVol20"] = (
                avg_bond.rolling(self.config.ROLLING_WINDOW).std() * np.sqrt(252)
            )

        # Momentum (60-day cumulative return)
        if equity_present:
            features["Equity_Mom60"] = daily_returns[equity_present].rolling(
                self.config.MOMENTUM_WINDOW).apply(lambda x: (1 + x).prod() - 1).mean(axis=1)

        if bond_present:
            features["Bond_Mom60"] = daily_returns[bond_present].rolling(
                self.config.MOMENTUM_WINDOW).apply(lambda x: (1 + x).prod() - 1).mean(axis=1)

        gold_present = [t for t in self.config.GOLD_TICKERS if t in daily_returns.columns]
        if gold_present:
            features["Gold_Mom60"] = daily_returns[gold_present].rolling(
                self.config.MOMENTUM_WINDOW).apply(lambda x: (1 + x).prod() - 1).mean(axis=1)

        # Gold-Equity correlation
        if "XIU.TO" in daily_returns.columns and gold_present:
            features["Gold_Equity_Corr20"] = (
                daily_returns["XIU.TO"]
                .rolling(self.config.ROLLING_WINDOW)
                .corr(daily_returns[gold_present[0]])
            )

        print(f"  Created {len(features.columns)} features")
        return features

    def download_oil(self) -> Optional[pd.Series]:
        """Download oil prices."""
        print("\n[Data] Downloading WTI crude oil prices...")
        try:
            s_oil = self.download_single(self.config.OIL_TICKER,
                                        self.config.START_DATE,
                                        self.config.END_DATE)
            if s_oil is not None and not s_oil.empty:
                oil_returns = s_oil.pct_change().rename("WTI_Oil_Return")
                print(f"  OK ({len(s_oil)} rows)")
                return oil_returns
        except Exception as e:
            print(f"  WARNING: {e}")
        return None

    def download_tbill_rate(self) -> Optional[pd.Series]:
        """
        Download 13-week T-bill annualised yield from Yahoo Finance (^IRX).
        Returns a daily series of the ANNUALISED rate as a decimal (e.g. 0.05 = 5%).
        Used as the risk-free rate in Sharpe calculations and as a macro feature.
        """
        print("\n[Data] Downloading T-bill risk-free rate (^IRX)...")
        try:
            s = self.download_single(self.config.TBILL_TICKER,
                                     self.config.START_DATE,
                                     self.config.END_DATE)
            if s is not None and not s.empty:
                # ^IRX is quoted as annualised percentage (e.g. 5.0 means 5%)
                tbill = (s / 100.0).rename("RiskFree_Ann")
                tbill = tbill.ffill().bfill()
                print(f"  OK ({len(tbill)} rows, range {tbill.min():.2%} to {tbill.max():.2%})")
                return tbill
        except Exception as e:
            print(f"  WARNING: T-bill download failed: {e}")
        print("  Falling back to constant 2% risk-free rate")
        return None

    def download_macro_features(self, daily_returns: pd.DataFrame) -> pd.DataFrame:
        """
        Download and compute two exogenous macro regime features:

          1. Credit Spread proxy  - 20-day rolling volatility of HYG relative to LQD.
             When HY bonds become more volatile than IG bonds, credit risk is rising.
             This is a forward-looking stress indicator independent of our ETF universe.

          2. Yield Curve Slope   - 10-year Treasury yield minus 13-week T-bill yield.
             A flattening or inverted curve historically precedes recessions by 12-18 months,
             giving the regime model a macro anticipatory signal.

        Both features are available freely via Yahoo Finance.
        """
        print("\n[Data] Downloading macro features (credit spread, yield curve)...")
        macro = pd.DataFrame(index=daily_returns.index)

        # Credit spread proxy
        try:
            hyg = self.download_single("HYG", self.config.START_DATE, self.config.END_DATE)
            lqd = self.download_single("LQD", self.config.START_DATE, self.config.END_DATE)
            if hyg is not None and lqd is not None:
                hyg_ret = hyg.pct_change()
                lqd_ret = lqd.pct_change()
                hyg_vol = hyg_ret.rolling(20).std() * np.sqrt(252)
                lqd_vol = lqd_ret.rolling(20).std() * np.sqrt(252)
                credit_spread = (hyg_vol / lqd_vol.replace(0, np.nan)).rename("Credit_Spread")
                macro = macro.join(credit_spread, how="left")
                print("  OK Credit spread proxy (HYG/LQD vol ratio)")
            else:
                print("  ! Credit spread: HYG or LQD download failed, skipping")
        except Exception as e:
            print(f"  ! Credit spread failed: {e}")

        # Yield curve slope
        try:
            tnx = self.download_single("^TNX", self.config.START_DATE, self.config.END_DATE)
            irx = self.download_single("^IRX", self.config.START_DATE, self.config.END_DATE)
            if tnx is not None and irx is not None:
                slope = ((tnx - irx) / 100.0).rename("Yield_Curve_Slope")
                macro = macro.join(slope, how="left")
                print("  OK Yield curve slope (10Y minus 3M, decimal)")
            else:
                print("  ! Yield curve: ^TNX or ^IRX download failed, skipping")
        except Exception as e:
            print(f"  ! Yield curve failed: {e}")

        macro = macro.reindex(daily_returns.index).ffill(limit=5)
        print(f"  Macro features shape: {macro.shape}")
        return macro

    def prepare_data(self) -> pd.DataFrame:
        """Complete data preparation pipeline."""
        print("\n" + "="*80)
        print("PHASE 1: DATA PREPARATION")
        print("="*80)

        # Download prices
        prices = self.download_prices()

        # Calculate returns
        print("\n[Data] Calculating daily returns...")
        daily_returns = prices.pct_change().iloc[1:]

        # Check for extreme moves
        extreme = (daily_returns.abs() > 0.15).any(axis=1)
        if extreme.any():
            print(f"  WARNING: {extreme.sum()} days with >15% moves")
        else:
            print("  OK: No extreme moves detected")

        # Download oil
        oil_returns = self.download_oil()

        # Download T-bill rate and store on Config for Sharpe calculations
        tbill_rate = self.download_tbill_rate()
        if tbill_rate is not None:
            Config._tbill_series = tbill_rate
        else:
            Config._tbill_series = None

        # Download macro features (credit spread, yield curve)
        macro_features = self.download_macro_features(daily_returns)

        # Engineer features
        features = self.calculate_features(prices, daily_returns)

        # Combine everything
        print("\n[Data] Combining data...")

        # Add price suffix
        prices.columns = [f"{c}_Price" for c in prices.columns]

        # Add return suffix
        daily_returns.columns = [f"{c}_Return" for c in daily_returns.columns]

        # Merge
        master = pd.concat([prices, daily_returns, features], axis=1, join="inner")

        # Add oil if available
        if oil_returns is not None:
            master = master.join(oil_returns, how="left")

        # Add macro features (credit spread, yield curve)
        if not macro_features.empty:
            master = master.join(macro_features, how="left")

        # Add T-bill rate column for time-varying Sharpe calculation
        if Config._tbill_series is not None:
            master = master.join(Config._tbill_series.rename("RiskFree_Ann"), how="left")
            master["RiskFree_Ann"] = master["RiskFree_Ann"].ffill().fillna(self.config.RISK_FREE_RATE)

        master.sort_index(inplace=True)

        # Clean data
        burn_in = max(self.config.ROLLING_WINDOW, self.config.MOMENTUM_WINDOW)
        master = master.iloc[burn_in:]
        master = master.ffill(limit=3)

        data_cols = [c for c in master.columns if master[c].notna().any()]
        master = master.dropna(subset=data_cols)

        print(f"  Final shape: {master.shape}")
        print(f"  Date range: {master.index[0].date()} → {master.index[-1].date()}")

        # Save
        Utils.ensure_output_dir(self.config.OUTPUT_DIR)
        output_path = os.path.join(self.config.OUTPUT_DIR, "cleaned_data.csv")
        master.to_csv(output_path)
        print(f"\n  Saved: {output_path}")

        return master


# ══════════════════════════════════════════════════════════════════════════════
# MODULE 2: HMM MODELING
# ══════════════════════════════════════════════════════════════════════════════

class HMMModel:
    """Train and apply Hidden Markov Model for regime detection."""

    def __init__(self, config: Config):
        self.config = config
        self.hmm = None
        self.scaler = None
        self.state_to_regime = None

    def prepare_features(self, master: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:
        """Select and prepare features for HMM."""
        print("\n[Model] Selecting HMM features...")

        # Core features
        missing_core = [c for c in self.config.FEATURE_COLS if c not in master.columns]
        if missing_core:
            raise KeyError(f"Missing core features: {missing_core}")

        # Optional features
        optional_present = [c for c in self.config.OPTIONAL_FEATURES if c in master.columns]
        all_features = self.config.FEATURE_COLS + optional_present

        print(f"  Core: {len(self.config.FEATURE_COLS)} features")
        print(f"  Optional: {len(optional_present)} features")
        print(f"  Total: {len(all_features)} features")

        data = master[all_features].dropna()
        print(f"  Rows after dropna: {len(data)}")

        return data, all_features

    def split_data(self, data: pd.DataFrame) -> np.ndarray:
        """
        Extract training data for HMM fitting.

        The HMM is trained on data up to TRAIN_END. All regime predictions
        (including post-TRAIN_END dates) are generated via predict_regimes(),
        which transforms the full dataset using the scaler fitted here.
        Out-of-sample validation is handled separately by WalkForwardAnalysis,
        which re-trains the model on progressively expanding windows.
        """
        print("\n[Model] Extracting training data...")

        train_mask = data.index <= self.config.TRAIN_END
        X_train = data.loc[train_mask].values

        n_total = len(data)
        n_train = len(X_train)
        n_oos = n_total - n_train
        print(f"  Train (up to {self.config.TRAIN_END}): {n_train} rows")
        print(f"  Out-of-sample (post TRAIN_END, for walk-forward): {n_oos} rows")

        if n_train == 0:
            raise RuntimeError("Empty training set")

        return X_train

    def standardize(self, X_train: np.ndarray) -> np.ndarray:
        """Fit scaler on training data and return scaled training array."""
        print("\n[Model] Standardizing features...")

        self.scaler = StandardScaler()
        X_train_sc = self.scaler.fit_transform(X_train)

        print(f"  Scaler mean: {np.round(self.scaler.mean_, 3)}")
        print(f"  Scaler std: {np.round(self.scaler.scale_, 3)}")

        return X_train_sc

    def fit_hmm(self, X_train_sc: np.ndarray) -> GaussianHMM:
        """Fit Gaussian HMM with multiple random initializations."""
        print(f"\n[Model] Fitting HMM (n_regimes={self.config.N_REGIMES}, "
              f"n_init={self.config.N_INIT}, n_iter={self.config.N_ITER})...")

        best_model = None
        best_loglik = -np.inf

        for i in range(self.config.N_INIT):
            model = GaussianHMM(
                n_components=self.config.N_REGIMES,
                covariance_type="full",
                n_iter=self.config.N_ITER,
                random_state=self.config.RANDOM_STATE + i,
                tol=1e-7,
                verbose=False
            )

            try:
                model.fit(X_train_sc)
                ll = model.score(X_train_sc)

                if ll > best_loglik:
                    best_loglik = ll
                    best_model = model

                if (i + 1) % 5 == 0:
                    print(f"  Init {i+1}/{self.config.N_INIT}: best LL = {best_loglik:.4f}")
            except Exception as e:
                print(f"  Init {i+1} failed: {e}")
                continue

        if best_model is None:
            raise RuntimeError("All HMM initializations failed")

        self.hmm = best_model
        print(f"  Best log-likelihood: {best_loglik:.4f}")
        print(f"  Converged: {self.hmm.monitor_.converged}")

        return self.hmm

    def relabel_regimes(self, X_train_sc: np.ndarray) -> Dict[int, int]:
        """Semantically relabel regimes by volatility."""
        print("\n[Model] Relabeling regimes by volatility...")

        # Predict states
        states_train = self.hmm.predict(X_train_sc)

        # Calculate mean volatility per state (assuming Market_RolVol20 is first feature)
        state_vols = {}
        for state in range(self.config.N_REGIMES):
            mask = states_train == state
            if mask.any():
                # Get original (unscaled) volatility
                mean_vol = self.scaler.inverse_transform(
                    X_train_sc[mask].mean(axis=0).reshape(1, -1)
                )[0, 0]
                state_vols[state] = mean_vol

        # Sort by volatility: lowest → 0 (Bull), highest → 2 (Crisis)
        sorted_states = sorted(state_vols.items(), key=lambda x: x[1])
        self.state_to_regime = {old: new for new, (old, _) in enumerate(sorted_states)}

        print("  Mapping:")
        for old_state, new_regime in self.state_to_regime.items():
            print(f"    State {old_state} → Regime {new_regime} "
                  f"({self.config.REGIME_LABELS[new_regime]}), "
                  f"vol={state_vols[old_state]:.1%}")

        return self.state_to_regime

    def predict_regimes(self, data: pd.DataFrame, all_features: List[str]) -> pd.DataFrame:
        """
        Predict regimes and posteriors for all data.

        Applies a minimum dwell filter: the model cannot switch to a new regime
        unless it has predicted that regime for at least MIN_DWELL_DAYS consecutive
        days. This eliminates single-day false regime transitions caused by noisy
        feature inputs, which would otherwise trigger unnecessary rebalancing.
        """
        print("\n[Model] Predicting regimes...")

        X_all = data[all_features].values
        X_all_sc = self.scaler.transform(X_all)

        # Raw state predictions and posteriors
        states = self.hmm.predict(X_all_sc)
        posteriors = self.hmm.predict_proba(X_all_sc)

        # Map to semantic regimes
        raw_regimes = np.array([self.state_to_regime[s] for s in states])

        # Reorder posteriors to match semantic regime labels
        posteriors_ordered = np.zeros_like(posteriors)
        for old_state, new_regime in self.state_to_regime.items():
            posteriors_ordered[:, new_regime] = posteriors[:, old_state]

        # Apply minimum dwell filter (5 days)
        # A regime transition is only accepted after MIN_DWELL_DAYS consecutive
        # days of the new regime signal. Between signal and acceptance, the current
        # regime is held and posteriors are kept as-is (soft transitions still work).
        MIN_DWELL_DAYS = 5
        smoothed_regimes = raw_regimes.copy()
        current_regime = raw_regimes[0]
        pending_regime = raw_regimes[0]
        dwell_count = 0

        for i in range(1, len(raw_regimes)):
            if raw_regimes[i] == current_regime:
                pending_regime = current_regime
                dwell_count = 0
                smoothed_regimes[i] = current_regime
            else:
                if raw_regimes[i] == pending_regime:
                    dwell_count += 1
                else:
                    pending_regime = raw_regimes[i]
                    dwell_count = 1

                if dwell_count >= MIN_DWELL_DAYS:
                    current_regime = pending_regime
                    smoothed_regimes[i] = current_regime
                else:
                    smoothed_regimes[i] = current_regime  # Hold current until dwell met

        n_filtered = int((raw_regimes != smoothed_regimes).sum())
        if n_filtered > 0:
            print(f"  Minimum dwell filter ({MIN_DWELL_DAYS}d): suppressed {n_filtered} "
                  f"premature regime transitions")

        result = data.copy()
        result["Regime"] = smoothed_regimes

        for r in range(self.config.N_REGIMES):
            result[f"P_Regime{r}"] = posteriors_ordered[:, r]

        print(f"  Predicted {len(result)} rows")
        return result

    def train(self, master: pd.DataFrame) -> pd.DataFrame:
        """Complete HMM training pipeline."""
        print("\n" + "="*80)
        print("PHASE 2: HMM MODELING")
        print("="*80)

        # Prepare features
        data, all_features = self.prepare_features(master)

        # Split data (train up to TRAIN_END; walk-forward handles out-of-sample splits)
        X_train = self.split_data(data)

        # Standardize
        X_train_sc = self.standardize(X_train)

        # Fit HMM
        self.fit_hmm(X_train_sc)

        # Relabel regimes
        self.relabel_regimes(X_train_sc)

        # Predict for all data
        result = self.predict_regimes(data, all_features)

        # Merge back with master
        master_with_regimes = master.join(
            result[["Regime"] + [f"P_Regime{r}" for r in range(self.config.N_REGIMES)]],
            how="left"
        )

        # Save
        Utils.ensure_output_dir(self.config.OUTPUT_DIR)

        # Save model
        model_path = os.path.join(self.config.OUTPUT_DIR, "model_hmm.pkl")
        bundle = {
            "hmm": self.hmm,
            "scaler": self.scaler,
            "state_to_regime": self.state_to_regime,
            "n_regimes": self.config.N_REGIMES,
            "feature_names": all_features,
        }
        joblib.dump(bundle, model_path)
        print(f"\n  Saved model: {model_path}")

        # Save data with regimes
        data_path = os.path.join(self.config.OUTPUT_DIR, "data_with_regimes.csv")
        master_with_regimes.to_csv(data_path)
        print(f"  Saved data: {data_path}")

        # Print regime stats
        self.print_regime_stats(master_with_regimes)

        return master_with_regimes

    def print_regime_stats(self, df: pd.DataFrame) -> None:
        """Print regime statistics."""
        print("\n[Model] Regime Statistics:")

        regime_counts = df["Regime"].value_counts().sort_index()
        total = len(df)

        for r in range(self.config.N_REGIMES):
            if r in regime_counts.index:
                count = regime_counts[r]
                pct = count / total
                print(f"  {self.config.REGIME_LABELS[r]:<26}: {count:>4} days ({pct:>6.1%})")


# ══════════════════════════════════════════════════════════════════════════════
# MODULE 3B: OPTIMIZED ALLOCATION STRATEGY (MEAN-VARIANCE OPTIMIZATION)
# ══════════════════════════════════════════════════════════════════════════════

class OptimizedAllocationStrategy:
    """
    Implement regime-switching allocation with mean-variance optimization.

    This optimizes portfolio weights for each regime based on historical
    return/covariance statistics, rather than using fixed weights.
    """

    def __init__(self, config: Config):
        self.config = config

    def calculate_regime_statistics(self, master: pd.DataFrame,
                                   train_end: str = None) -> Dict:
        """
        Calculate mean returns and covariance matrices for each regime.

        Improvements over naive sample statistics:
          - Ledoit-Wolf shrinkage on the covariance matrix: pulls the sample
            covariance toward a structured estimator, dramatically stabilising
            weights when observations are limited (especially for the crisis regime).
          - Mean return shrinkage toward the grand mean: reduces the optimiser's
            confidence in any single asset's historical edge. Raw sample means are
            the noisiest input in MV optimisation; shrinking them toward the cross-
            sectional average produces more stable, out-of-sample-robust weights.
          - Crisis regime hard-coded: with only ~60 days of data, statistical
            estimation is unreliable. We use a pre-set defensive allocation for
            the crisis regime and only run the optimiser on bull and normal regimes.

        Args:
            master: DataFrame with regimes and returns
            train_end: Use only data up to this date for statistics

        Returns:
            regime_stats: dict with keys 0,1,2 containing
                         {'mean': array, 'cov': matrix, 'n_obs': int, 'avg_vol': float}
        """
        from sklearn.covariance import LedoitWolf

        if train_end is None:
            train_end = self.config.TRAIN_END

        # Filter to training period
        train_data = master[master.index <= train_end]
        tickers = list(self.config.ETF_CONFIG.keys())

        print(f"\n[Optimization] Calculating regime statistics with shrinkage "
              f"(train data: {len(train_data)} days)...")

        return_cols = [f"{t}_Return" for t in tickers]

        # Grand mean across ALL regimes — used as the shrinkage target
        all_returns = train_data[return_cols].dropna()
        grand_mean = all_returns.mean().values * 252  # annualised

        regime_stats = {}

        for regime in range(self.config.N_REGIMES):
            regime_mask = train_data["Regime"] == regime
            regime_data = train_data[regime_mask]
            regime_returns = regime_data[return_cols].dropna()

            if len(regime_returns) < 5:
                print(f"  WARNING: Regime {regime} has only {len(regime_returns)} obs, using all data")
                regime_returns = all_returns

            n_obs = len(regime_returns)

            # ── Covariance with Ledoit-Wolf shrinkage ────────────────────────
            lw = LedoitWolf()
            lw.fit(regime_returns.values)
            cov_matrix_annual = lw.covariance_ * 252

            # ── Mean returns with shrinkage toward the grand mean ────────────
            # Shrinkage intensity: higher for crisis (less data, less reliable)
            # Formula: shrunk_mean = (1 - alpha) * sample_mean + alpha * grand_mean
            sample_mean = regime_returns.mean().values * 252
            if regime == 2:
                # Crisis: very thin data — pull heavily toward grand mean
                shrinkage_alpha = 0.70
            elif regime == 1:
                shrinkage_alpha = 0.30
            else:
                shrinkage_alpha = 0.20
            shrunk_mean = (1 - shrinkage_alpha) * sample_mean + shrinkage_alpha * grand_mean

            avg_vol = np.sqrt(np.diag(cov_matrix_annual)).mean()

            regime_stats[regime] = {
                'mean': shrunk_mean,
                'cov': cov_matrix_annual,
                'n_obs': n_obs,
                'avg_vol': avg_vol,
                'shrinkage_alpha': shrinkage_alpha,
            }

            print(f"  Regime {regime} ({self.config.REGIME_LABELS[regime]}): "
                  f"{n_obs} obs, avg vol = {avg_vol:.1%}, "
                  f"mean shrinkage = {shrinkage_alpha:.0%}")

        return regime_stats

    def optimize_portfolio_weights(self, mean_returns: np.ndarray,
                                  cov_matrix: np.ndarray,
                                  risk_aversion: float = 1.0,
                                  min_weight: float = 0.05,
                                  max_weight: float = 0.40,
                                  gold_max: float = 0.25) -> np.ndarray:
        """
        Solve for optimal portfolio weights using mean-variance optimisation.

        Objective: maximise (expected return - risk_aversion * variance)

        Args:
            mean_returns: array of expected returns (annualised, shrinkage-adjusted)
            cov_matrix: covariance matrix (annualised, Ledoit-Wolf shrunk)
            risk_aversion: controls risk/return tradeoff (higher = more conservative)
            min_weight: minimum weight per asset
            max_weight: maximum weight per asset (equity and bonds)
            gold_max: separate maximum for gold (index 4, CGL-C.TO) — kept below 25%
                      to prevent the strategy from becoming a gold-concentration bet

        Returns:
            optimal_weights: array of weights summing to 1
        """
        n_assets = len(mean_returns)

        def objective(w):
            portfolio_return = np.dot(w, mean_returns)
            portfolio_variance = np.dot(w, np.dot(cov_matrix, w))
            return -(portfolio_return - risk_aversion * portfolio_variance)

        constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1.0}

        # Per-asset bounds: gold (index 4) gets its own tighter cap
        tickers = list(Config.ETF_CONFIG.keys())
        bounds = []
        for i in range(n_assets):
            ticker = tickers[i] if i < len(tickers) else None
            if ticker in Config.GOLD_TICKERS:
                bounds.append((min_weight, gold_max))
            else:
                bounds.append((min_weight, max_weight))
        bounds = tuple(bounds)

        initial_weights = np.ones(n_assets) / n_assets

        result = minimize(
            objective,
            initial_weights,
            method='SLSQP',
            bounds=bounds,
            constraints=constraints,
            options={'maxiter': 1000}
        )

        if not result.success:
            print(f"    WARNING: Optimisation failed: {result.message}")
            return initial_weights

        return result.x

    def create_optimized_weights(self, regime_stats: Dict,
                                risk_aversion_by_regime: Dict[int, float] = None) -> Dict:
        """
        Create optimal weights for each regime using mean-variance optimisation.

        Key design decisions:
          - Bull and Normal regimes: weights are solved by the MV optimiser using
            shrinkage-adjusted inputs. Gold is capped at 25% max to avoid the
            strategy becoming a gold bet in disguise.
          - Crisis regime: pre-set defensive allocation. With only ~60 days of
            historical crisis data, statistical optimisation is unreliable.
            A hard-coded conservative allocation is more trustworthy.

        Args:
            regime_stats: dict from calculate_regime_statistics
            risk_aversion_by_regime: dict mapping regime -> risk aversion parameter
                                    Default: {0: 1.0, 1: 2.0, 2: 4.0}

        Returns:
            target_weights: dict mapping regime -> {ticker: weight}
        """
        if risk_aversion_by_regime is None:
            risk_aversion_by_regime = {
                0: 1.0,   # Bull: willing to take risk
                1: 2.0,   # Normal: moderate
                2: 4.0,   # Crisis: very conservative (not used — hard-coded below)
            }

        print(f"\n[Optimization] Creating optimized weights for each regime...")

        tickers = list(self.config.ETF_CONFIG.keys())
        target_weights = {}

        for regime, risk_aversion in risk_aversion_by_regime.items():

            # ── Crisis regime: hard-coded defensive allocation ───────────────
            if regime == 2:
                weights_dict = {
                    "XIU.TO":   0.05,
                    "VFV.TO":   0.10,
                    "XEF.TO":   0.05,
                    "XBB.TO":   0.55,   # Bonds dominate in crisis
                    "CGL-C.TO": 0.25,   # Gold capped at 25%
                }
                print(f"\n  Regime {regime} ({self.config.REGIME_LABELS[regime]}) "
                      f"[hard-coded defensive — insufficient training data for reliable MV]:")
            else:
                # ── Bull and Normal: MV optimisation with gold cap ───────────
                stats = regime_stats[regime]
                # Gold max weight 25% (index 4 = CGL-C.TO)
                gold_max = 0.25
                optimal_w = self.optimize_portfolio_weights(
                    mean_returns=stats['mean'],
                    cov_matrix=stats['cov'],
                    risk_aversion=risk_aversion,
                    min_weight=0.05,
                    max_weight=0.40,
                    gold_max=gold_max,
                )
                weights_dict = {ticker: w for ticker, w in zip(tickers, optimal_w)}
                print(f"\n  Regime {regime} ({self.config.REGIME_LABELS[regime]}) "
                      f"[risk_aversion={risk_aversion}, gold_max={gold_max:.0%}]:")

            target_weights[regime] = weights_dict

            # Print summary
            equity_weight = sum(weights_dict[t] for t in self.config.EQUITY_TICKERS)
            bond_weight   = sum(weights_dict[t] for t in self.config.BOND_TICKERS)
            gold_weight   = sum(weights_dict[t] for t in self.config.GOLD_TICKERS)

            print(f"    Equity: {equity_weight:.1%} | Bonds: {bond_weight:.1%} | Gold: {gold_weight:.1%}")
            for t, w in weights_dict.items():
                ticker_name = self.config.ETF_CONFIG[t].split('(')[0].strip()
                print(f"      {ticker_name}: {w:.1%}")

        return target_weights

    def run_optimized_backtest(self, master: pd.DataFrame,
                              target_weights: Dict = None,
                              risk_aversion_by_regime: Dict = None) -> Tuple:
        """
        Run backtest with optimized regime-specific weights.

        Args:
            master: DataFrame with regimes and returns
            target_weights: Pre-computed optimized weights (optional)
            risk_aversion_by_regime: Risk aversion parameters (optional)

        Returns:
            (portfolio_df, results_df, target_weights)
        """
        print("\n" + "="*80)
        print("PHASE 3B: OPTIMIZED ALLOCATION STRATEGY")
        print("="*80)

        # Fix lookahead bias: calculate regime statistics ONLY on training data
        # (data up to TRAIN_END), then apply the resulting weights to the full period.
        # This mirrors what the walk-forward splits do correctly.
        if target_weights is None:
            regime_stats = self.calculate_regime_statistics(
                master, train_end=self.config.TRAIN_END
            )
            target_weights = self.create_optimized_weights(regime_stats, risk_aversion_by_regime)

        # Build weight series using POSTERIOR PROBABILITY BLENDING
        # Rather than hard-switching to a single regime's weights, we blend
        # across all regimes proportional to the model's confidence.
        # E.g. if P(Bull)=0.7, P(Normal)=0.3, P(Crisis)=0.0, the target
        # allocation is 0.7 * Bull_weights + 0.3 * Normal_weights.
        # This reduces turnover, avoids sharp regime-flip artefacts, and
        # produces smoother, more trustworthy allocations.
        print("\n[Optimization] Building posterior-blended weight series...")
        tickers = list(self.config.ETF_CONFIG.keys())

        weight_series = []
        for _, row in master.iterrows():
            # Get posterior probabilities for each regime
            posteriors = np.array([
                row.get(f"P_Regime{r}", 1.0 / self.config.N_REGIMES)
                for r in range(self.config.N_REGIMES)
            ])
            # Ensure they sum to 1 (handle any NaN edge cases)
            posteriors = np.nan_to_num(posteriors, nan=1.0 / self.config.N_REGIMES)
            posteriors /= posteriors.sum()

            # Blend regime weights by posterior probability
            blended = {t: 0.0 for t in tickers}
            for regime_idx, prob in enumerate(posteriors):
                regime_w = target_weights[regime_idx]
                for t in tickers:
                    blended[t] += prob * regime_w.get(t, 0.0)

            # Renormalise (ensures weights sum to exactly 1.0)
            total = sum(blended.values())
            if total > 0:
                blended = {t: v / total for t, v in blended.items()}

            weight_series.append(blended)

        # Run backtest using same logic as AllocationStrategy
        print("\n[Optimization] Running backtest with optimized weights...")
        dates = master.index
        n_days = len(dates)
        tickers = list(self.config.ETF_CONFIG.keys())

        # Initialize
        portfolio_values = np.zeros(n_days)
        holdings = {}

        # Initial allocation
        initial_weights = weight_series[0]
        for ticker in tickers:
            holdings[ticker] = self.config.INITIAL_CAPITAL * initial_weights.get(ticker, 0.0)

        portfolio_values[0] = self.config.INITIAL_CAPITAL

        total_costs = 0.0
        n_rebalances = 0

        # Simulate
        for i in range(1, n_days):
            date = dates[i]

            # Apply returns
            pv = 0.0
            for ticker in tickers:
                ret_col = f"{ticker}_Return"
                ret = master.loc[date, ret_col] if ret_col in master.columns else 0.0
                if pd.isna(ret):
                    ret = 0.0
                holdings[ticker] *= (1 + ret)
                pv += holdings[ticker]

            portfolio_values[i] = pv

            # Get target weights
            target_w = weight_series[i]

            # Check drift
            actual_weights = {t: holdings[t]/pv if pv > 0 else 0.0 for t in tickers}
            max_drift = max(abs(actual_weights.get(t, 0) - target_w.get(t, 0))
                          for t in tickers)

            # Rebalance if needed
            if max_drift > self.config.REBALANCE_THRESHOLD:
                cost = 0.0
                for ticker in tickers:
                    trade_val = abs(pv * target_w.get(ticker, 0.0) - holdings[ticker])
                    cost += trade_val * self.config.BID_ASK_SPREAD
                    holdings[ticker] = pv * target_w.get(ticker, 0.0)

                # Deduct cost
                scale = (pv - cost) / pv if pv > 0 else 1.0
                for ticker in tickers:
                    holdings[ticker] *= scale

                portfolio_values[i] = sum(holdings.values())
                total_costs += cost
                n_rebalances += 1

        # Create results dataframe
        portfolio_df = pd.DataFrame({
            "Date": dates,
            "Portfolio_Value": portfolio_values
        }).set_index("Date")

        # Add benchmark
        portfolio_df["Benchmark_Value"] = self.compute_benchmark(master)

        print(f"  Completed: {n_rebalances} rebalances, ${total_costs:.2f} costs")
        print(f"  Final value: ${portfolio_values[-1]:,.0f}")

        # Calculate metrics
        results_df = self.calculate_metrics(portfolio_df, n_rebalances, total_costs)

        # Save
        Utils.ensure_output_dir(self.config.OUTPUT_DIR)

        portfolio_path = os.path.join(self.config.OUTPUT_DIR, "portfolio_optimized.csv")
        portfolio_df.to_csv(portfolio_path)
        print(f"\n  Saved portfolio: {portfolio_path}")

        results_path = os.path.join(self.config.OUTPUT_DIR, "performance_optimized.csv")
        results_df.to_csv(results_path, index=False)
        print(f"  Saved metrics: {results_path}")

        weights_path = os.path.join(self.config.OUTPUT_DIR, "optimized_weights.txt")
        with open(weights_path, 'w') as f:
            f.write("OPTIMIZED REGIME-SPECIFIC WEIGHTS\n")
            f.write("="*60 + "\n\n")
            for regime, weights in target_weights.items():
                f.write(f"Regime {regime} ({self.config.REGIME_LABELS[regime]}):\n")
                for ticker, weight in weights.items():
                    f.write(f"  {ticker}: {weight:.2%}\n")
                f.write("\n")
        print(f"  Saved weights: {weights_path}")

        return portfolio_df, results_df, target_weights

    def compute_benchmark(self, master: pd.DataFrame) -> np.ndarray:
        """
        Compute 60/40 benchmark WITH annual rebalancing.

        A non-rebalancing benchmark drifts from its target weights over time,
        giving the active strategy a methodological advantage (rebalancing has
        a well-documented return premium). Annual rebalancing creates a fair
        comparison on equal footing.
        """
        dates = master.index
        n_days = len(dates)
        tickers = list(self.config.ETF_CONFIG.keys())

        n_eq = len(self.config.EQUITY_TICKERS)
        n_bond = len(self.config.BOND_TICKERS)
        target = {t: 0.60 / n_eq for t in self.config.EQUITY_TICKERS}
        target.update({t: 0.40 / n_bond for t in self.config.BOND_TICKERS})
        target.update({t: 0.0 for t in self.config.GOLD_TICKERS})

        holdings = {t: self.config.INITIAL_CAPITAL * target.get(t, 0.0) for t in tickers}
        values = np.zeros(n_days)
        values[0] = self.config.INITIAL_CAPITAL
        last_rebal_year = dates[0].year

        for i in range(1, n_days):
            date = dates[i]
            pv = 0.0
            for ticker in tickers:
                ret_col = f"{ticker}_Return"
                ret = master.loc[date, ret_col] if ret_col in master.columns else 0.0
                if pd.isna(ret):
                    ret = 0.0
                holdings[ticker] *= (1 + ret)
                pv += holdings[ticker]
            values[i] = pv

            # Annual rebalance
            if date.year > last_rebal_year:
                for ticker in tickers:
                    holdings[ticker] = pv * target.get(ticker, 0.0)
                last_rebal_year = date.year

        return values

    def calculate_metrics(self, portfolio_df: pd.DataFrame,
                         n_rebalances: int, total_costs: float) -> pd.DataFrame:
        """
        Calculate performance metrics using time-varying risk-free rate.

        Improvements:
          - Uses the actual T-bill series (downloaded from ^IRX) rather than a
            constant 2% rate. This matters significantly for the 2022-2024 period
            where rates rose from near-zero to 5%+.
          - Adds Sortino ratio, which penalises only downside volatility and is
            more meaningful than Sharpe for asymmetric return distributions.
        """

        def metrics(values, dates=None, rf_series=None, rf_const=None):
            if rf_const is None:
                rf_const = self.config.RISK_FREE_RATE
            returns = pd.Series(values, index=dates).pct_change().dropna()
            if len(returns) == 0:
                return {}

            total_return = (values[-1] - values[0]) / values[0]
            ann_return = (1 + total_return) ** (252 / len(returns)) - 1
            ann_vol = returns.std() * np.sqrt(252)

            # Time-varying risk-free rate for Sharpe
            if rf_series is not None and dates is not None:
                daily_rf = rf_series.reindex(returns.index).ffill().fillna(rf_const) / 252
                excess_returns = returns - daily_rf
                ann_excess = excess_returns.mean() * 252
                sharpe = ann_excess / ann_vol if ann_vol > 0 else np.nan
            else:
                sharpe = (ann_return - rf_const) / ann_vol if ann_vol > 0 else np.nan

            # Sortino ratio — penalises only downside volatility
            downside_returns = returns[returns < 0]
            downside_vol = downside_returns.std() * np.sqrt(252) if len(downside_returns) > 0 else np.nan
            sortino = (ann_return - rf_const) / downside_vol if (downside_vol and downside_vol > 0) else np.nan

            cumulative = pd.Series(values)
            rolling_max = cumulative.cummax()
            drawdown = (cumulative - rolling_max) / rolling_max
            max_dd = drawdown.min()
            calmar = ann_return / abs(max_dd) if max_dd != 0 else np.nan

            return {
                "Total_Return": total_return,
                "Ann_Return": ann_return,
                "Ann_Vol": ann_vol,
                "Sharpe_Ratio": sharpe,
                "Sortino_Ratio": sortino,
                "Max_Drawdown": max_dd,
                "Calmar_Ratio": calmar,
            }

        dates = portfolio_df.index
        rf_series = getattr(Config, '_tbill_series', None)

        strat_metrics = metrics(portfolio_df["Portfolio_Value"].values,
                                dates=dates, rf_series=rf_series)
        bench_metrics = metrics(portfolio_df["Benchmark_Value"].values,
                                dates=dates, rf_series=rf_series)

        # Create comparison table
        results = []
        metric_names = {
            "Total_Return":    "Total Return",
            "Ann_Return":      "Annualized Return",
            "Ann_Vol":         "Annualized Volatility",
            "Sharpe_Ratio":    "Sharpe Ratio",
            "Sortino_Ratio":   "Sortino Ratio",
            "Max_Drawdown":    "Max Drawdown",
            "Calmar_Ratio":    "Calmar Ratio",
        }

        for key, name in metric_names.items():
            s = strat_metrics.get(key, np.nan)
            b = bench_metrics.get(key, np.nan)
            d = s - b
            results.append({
                "Metric": name,
                "Optimized_Strategy": s,
                "Benchmark_60_40": b,
                "Difference": d
            })

        results_df = pd.DataFrame(results)

        print("\n[Optimization] Performance Metrics:")
        print(results_df.to_string(index=False))
        print(f"\n  Rebalances: {n_rebalances}")
        print(f"  Total costs: ${total_costs:.2f}")
        rf_label = "time-varying T-bill" if rf_series is not None else f"constant {self.config.RISK_FREE_RATE:.1%}"
        print(f"  Risk-free rate used: {rf_label}")

        return results_df


# ══════════════════════════════════════════════════════════════════════════════
# MODULE 3: ALLOCATION STRATEGY (FIXED WEIGHTS)
# ══════════════════════════════════════════════════════════════════════════════

class AllocationStrategy:
    """Implement regime-switching allocation strategy."""

    def __init__(self, config: Config):
        self.config = config

    def compute_soft_weights(self, posterior: np.ndarray) -> Dict[str, float]:
        """Compute probability-weighted target allocation."""
        weights = {ticker: 0.0 for ticker in self.config.ETF_CONFIG.keys()}

        for regime, prob in enumerate(posterior):
            regime_weights = self.config.TARGET_WEIGHTS[regime]
            for ticker, w in regime_weights.items():
                weights[ticker] += prob * w

        # Normalize
        total = sum(weights.values())
        if total > 0:
            weights = {k: v/total for k, v in weights.items()}

        return weights

    def run_backtest(self, master: pd.DataFrame) -> pd.DataFrame:
        """Run backtest with rebalancing."""
        print("\n[Allocation] Running backtest...")

        dates = master.index
        n_days = len(dates)
        tickers = list(self.config.ETF_CONFIG.keys())

        # Initialize
        portfolio_values = np.zeros(n_days)
        holdings = {}

        # Get initial weights
        initial_posterior = np.array([
            master.iloc[0].get(f"P_Regime{r}", 1.0/self.config.N_REGIMES)
            for r in range(self.config.N_REGIMES)
        ])
        initial_weights = self.compute_soft_weights(initial_posterior)

        # Initial allocation
        for ticker in tickers:
            holdings[ticker] = self.config.INITIAL_CAPITAL * initial_weights.get(ticker, 0.0)

        portfolio_values[0] = self.config.INITIAL_CAPITAL

        total_costs = 0.0
        n_rebalances = 0

        # Simulate
        for i in range(1, n_days):
            date = dates[i]

            # Apply returns
            pv = 0.0
            for ticker in tickers:
                ret_col = f"{ticker}_Return"
                ret = master.loc[date, ret_col] if ret_col in master.columns else 0.0
                if pd.isna(ret):
                    ret = 0.0
                holdings[ticker] *= (1 + ret)
                pv += holdings[ticker]

            portfolio_values[i] = pv

            # Get target weights
            posterior = np.array([
                master.loc[date].get(f"P_Regime{r}", 1.0/self.config.N_REGIMES)
                for r in range(self.config.N_REGIMES)
            ])
            target_weights = self.compute_soft_weights(posterior)

            # Check drift
            actual_weights = {t: holdings[t]/pv if pv > 0 else 0.0 for t in tickers}
            max_drift = max(abs(actual_weights.get(t, 0) - target_weights.get(t, 0))
                          for t in tickers)

            # Rebalance if needed
            if max_drift > self.config.REBALANCE_THRESHOLD:
                cost = 0.0
                for ticker in tickers:
                    trade_val = abs(pv * target_weights.get(ticker, 0.0) - holdings[ticker])
                    cost += trade_val * self.config.BID_ASK_SPREAD
                    holdings[ticker] = pv * target_weights.get(ticker, 0.0)

                # Deduct cost
                scale = (pv - cost) / pv if pv > 0 else 1.0
                for ticker in tickers:
                    holdings[ticker] *= scale

                portfolio_values[i] = sum(holdings.values())
                total_costs += cost
                n_rebalances += 1

        # Create results dataframe
        result = pd.DataFrame({
            "Date": dates,
            "Portfolio_Value": portfolio_values
        }).set_index("Date")

        # Add benchmark (60/40)
        result["Benchmark_Value"] = self.compute_benchmark(master)

        print(f"  Completed: {n_rebalances} rebalances, ${total_costs:.2f} costs")
        print(f"  Final value: ${portfolio_values[-1]:,.0f}")

        return result, n_rebalances, total_costs

    def compute_benchmark(self, master: pd.DataFrame) -> np.ndarray:
        """Compute 60/40 benchmark WITH annual rebalancing for fair comparison."""
        dates = master.index
        n_days = len(dates)
        tickers = list(self.config.ETF_CONFIG.keys())

        n_eq = len(self.config.EQUITY_TICKERS)
        n_bond = len(self.config.BOND_TICKERS)
        target = {t: 0.60 / n_eq for t in self.config.EQUITY_TICKERS}
        target.update({t: 0.40 / n_bond for t in self.config.BOND_TICKERS})
        target.update({t: 0.0 for t in self.config.GOLD_TICKERS})

        holdings = {t: self.config.INITIAL_CAPITAL * target.get(t, 0.0) for t in tickers}
        values = np.zeros(n_days)
        values[0] = self.config.INITIAL_CAPITAL
        last_rebal_year = dates[0].year

        for i in range(1, n_days):
            date = dates[i]
            pv = 0.0
            for ticker in tickers:
                ret_col = f"{ticker}_Return"
                ret = master.loc[date, ret_col] if ret_col in master.columns else 0.0
                if pd.isna(ret):
                    ret = 0.0
                holdings[ticker] *= (1 + ret)
                pv += holdings[ticker]
            values[i] = pv

            if date.year > last_rebal_year:
                for ticker in tickers:
                    holdings[ticker] = pv * target.get(ticker, 0.0)
                last_rebal_year = date.year

        return values

    def calculate_metrics(self, portfolio_df: pd.DataFrame,
                         n_rebalances: int, total_costs: float) -> pd.DataFrame:
        """Calculate performance metrics."""

        def metrics(values, rf=self.config.RISK_FREE_RATE):
            returns = pd.Series(values).pct_change().dropna()
            if len(returns) == 0:
                return {}

            total_return = (values[-1] - values[0]) / values[0]
            ann_return = (1 + total_return) ** (252 / len(returns)) - 1
            ann_vol = returns.std() * np.sqrt(252)
            sharpe = (ann_return - rf) / ann_vol if ann_vol > 0 else np.nan

            cumulative = pd.Series(values)
            rolling_max = cumulative.cummax()
            drawdown = (cumulative - rolling_max) / rolling_max
            max_dd = drawdown.min()

            calmar = ann_return / abs(max_dd) if max_dd != 0 else np.nan

            return {
                "Total_Return": total_return,
                "Ann_Return": ann_return,
                "Ann_Vol": ann_vol,
                "Sharpe_Ratio": sharpe,
                "Max_Drawdown": max_dd,
                "Calmar_Ratio": calmar,
            }

        strat_metrics = metrics(portfolio_df["Portfolio_Value"].values)
        bench_metrics = metrics(portfolio_df["Benchmark_Value"].values)

        # Create comparison table
        results = []
        metric_names = {
            "Total_Return": "Total Return",
            "Ann_Return": "Annualized Return",
            "Ann_Vol": "Annualized Volatility",
            "Sharpe_Ratio": "Sharpe Ratio",
            "Max_Drawdown": "Max Drawdown",
            "Calmar_Ratio": "Calmar Ratio",
        }

        for key, name in metric_names.items():
            s = strat_metrics.get(key, np.nan)
            b = bench_metrics.get(key, np.nan)
            d = s - b
            results.append({
                "Metric": name,
                "Strategy": s,
                "Benchmark_60_40": b,
                "Difference": d
            })

        results_df = pd.DataFrame(results)

        print("\n[Allocation] Performance Metrics:")
        print(results_df.to_string(index=False))
        print(f"\n  Rebalances: {n_rebalances}")
        print(f"  Total costs: ${total_costs:.2f}")

        return results_df

    def run(self, master: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """Complete allocation pipeline."""
        print("\n" + "="*80)
        print("PHASE 3: ALLOCATION STRATEGY")
        print("="*80)

        # Run backtest
        portfolio_df, n_rebalances, total_costs = self.run_backtest(master)

        # Calculate metrics
        results_df = self.calculate_metrics(portfolio_df, n_rebalances, total_costs)

        # Save
        Utils.ensure_output_dir(self.config.OUTPUT_DIR)

        portfolio_path = os.path.join(self.config.OUTPUT_DIR, "portfolio_backtest.csv")
        portfolio_df.to_csv(portfolio_path)
        print(f"\n  Saved portfolio: {portfolio_path}")

        results_path = os.path.join(self.config.OUTPUT_DIR, "performance_metrics.csv")
        results_df.to_csv(results_path, index=False)
        print(f"  Saved metrics: {results_path}")

        return portfolio_df, results_df


# ══════════════════════════════════════════════════════════════════════════════
# MODULE 5: WALK-FORWARD ANALYSIS & COMPREHENSIVE COMPARISON
# ══════════════════════════════════════════════════════════════════════════════

class WalkForwardAnalysis:
    """
    Implement walk-forward analysis to test out-of-sample performance.

    This trains models on progressively larger training windows and tests
    on subsequent out-of-sample periods to validate strategy robustness.
    """

    def __init__(self, config: Config):
        self.config = config

    def create_walk_forward_splits(self, master: pd.DataFrame,
                                   initial_train_years: int = 5,
                                   test_years: int = 1) -> List[Dict]:
        """
        Create walk-forward training/test splits using EXPANDING windows.

        Proper walk-forward uses expanding training windows:
        - Split 1: Train 2015-2020 (5y) → Test 2020-2021 (1y)
        - Split 2: Train 2015-2021 (6y) → Test 2021-2022 (1y)
        - Split 3: Train 2015-2022 (7y) → Test 2022-2023 (1y)
        - Split 4: Train 2015-2023 (8y) → Test 2023-2024 (1y)

        This prevents data leakage and simulates realistic deployment.

        Args:
            master: Full dataset
            initial_train_years: Years for first training window
            test_years: Years of data for each test period

        Returns:
            List of dicts with 'train_start', 'train_end', 'test_start', 'test_end'
        """
        print(f"\n[Walk-Forward] Creating EXPANDING window splits...")
        print(f"  Initial train: {initial_train_years}y, Test period: {test_years}y")

        dates = master.index
        start_date = dates[0]
        end_date = dates[-1]

        splits = []

        # First training window ends after initial_train_years
        first_train_end = start_date + pd.DateOffset(years=initial_train_years)

        # Find actual date in dataset
        if first_train_end > end_date:
            print(f"  ERROR: Not enough data. Need at least {initial_train_years} years.")
            return []

        current_test_start = dates[dates >= first_train_end][0]

        split_num = 1
        while True:
            # Test period
            test_end = current_test_start + pd.DateOffset(years=test_years)

            # Check if we have enough data for full test period
            if test_end > end_date:
                break

            # Find actual dates in dataset
            test_end_actual = dates[dates <= test_end][-1]

            # Training period: from start to just before test
            train_end_actual = dates[dates < current_test_start][-1]

            splits.append({
                'train_start': start_date,
                'train_end': train_end_actual,
                'test_start': current_test_start,
                'test_end': test_end_actual,
                'label': f"Split{split_num}_{train_end_actual.year}→{test_end_actual.year}"
            })

            # Move to next test period (expanding window)
            current_test_start = test_end_actual + pd.DateOffset(days=1)
            current_test_start = dates[dates >= current_test_start][0] if current_test_start <= end_date else None

            if current_test_start is None:
                break

            split_num += 1

        print(f"  Created {len(splits)} expanding-window splits:")
        for i, split in enumerate(splits, 1):
            train_days = len(master.loc[split['train_start']:split['train_end']])
            test_days = len(master.loc[split['test_start']:split['test_end']])
            train_years = (split['train_end'] - split['train_start']).days / 365.25
            print(f"    Split {i}: Train {split['train_start'].date()}→{split['train_end'].date()} "
                  f"({train_years:.1f}y, {train_days}d), "
                  f"Test {split['test_start'].date()}→{split['test_end'].date()} ({test_days}d)")

        if len(splits) == 0:
            print(f"  ERROR: Not enough data to create any splits.")
            print(f"  Need at least {initial_train_years + test_years} years total.")

        return splits

    def run_walk_forward(self, master: pd.DataFrame,
                        initial_train_years: int = 5,
                        test_years: int = 1) -> pd.DataFrame:
        """
        Run complete walk-forward analysis.

        Returns:
            DataFrame with performance metrics for each split and strategy
        """
        print("\n" + "="*80)
        print("WALK-FORWARD ANALYSIS")
        print("="*80)

        splits = self.create_walk_forward_splits(master, initial_train_years, test_years)

        results = []

        for split_idx, split in enumerate(splits, 1):
            print(f"\n[Split {split_idx}/{len(splits)}] {split['label']}...")

            # Extract train and test data
            train_data = master.loc[split['train_start']:split['train_end']].copy()
            test_data = master.loc[split['test_start']:split['test_end']].copy()

            # Train HMM on training data
            hmm_model = HMMModel(self.config)
            data, all_features = hmm_model.prepare_features(train_data)
            X_train = data.values

            # Initialize and fit scaler
            hmm_model.scaler = StandardScaler()
            X_train_sc = hmm_model.scaler.fit_transform(X_train)

            # Fit and relabel HMM
            hmm_model.hmm = hmm_model.fit_hmm(X_train_sc)
            state_to_regime = hmm_model.relabel_regimes(X_train_sc)

            # Predict regimes for test data
            test_features = test_data[all_features].dropna()
            X_test = test_features.values
            X_test_sc = hmm_model.scaler.transform(X_test)

            states = hmm_model.hmm.predict(X_test_sc)
            posteriors = hmm_model.hmm.predict_proba(X_test_sc)

            regimes = np.array([state_to_regime[s] for s in states])
            posteriors_ordered = np.zeros_like(posteriors)
            for old_state, new_regime in state_to_regime.items():
                posteriors_ordered[:, new_regime] = posteriors[:, old_state]

            # Add regime predictions to test data
            test_data_with_regimes = test_data.copy()
            test_data_with_regimes.loc[test_features.index, 'Regime'] = regimes
            for r in range(self.config.N_REGIMES):
                test_data_with_regimes.loc[test_features.index, f'P_Regime{r}'] = posteriors_ordered[:, r]

            # Run strategies on test period
            strategies = self._run_all_strategies(test_data_with_regimes, train_data)

            # Calculate metrics for each strategy
            test_dates = test_data_with_regimes.index
            for strategy_name, values in strategies.items():
                metrics = self._calculate_metrics(values, dates=test_dates)
                results.append({
                    'Split': split['label'],
                    'Train_Start': split['train_start'].date(),
                    'Train_End': split['train_end'].date(),
                    'Test_Start': split['test_start'].date(),
                    'Test_End': split['test_end'].date(),
                    'Strategy': strategy_name,
                    **metrics
                })

        results_df = pd.DataFrame(results)

        # Save results
        Utils.ensure_output_dir(self.config.OUTPUT_DIR)
        output_path = os.path.join(self.config.OUTPUT_DIR, "walk_forward_results.csv")
        results_df.to_csv(output_path, index=False)
        print(f"\n  Saved: {output_path}")

        # Print summary
        self._print_walk_forward_summary(results_df)

        return results_df

    def _run_all_strategies(self, test_data: pd.DataFrame, train_data: pd.DataFrame) -> Dict:
        """Run all strategies on test period."""
        strategies = {}

        # 1. Benchmark 60/40
        strategies['Benchmark_60_40'] = self._run_benchmark(test_data, equity_pct=0.60)

        # 2. Benchmark 80/20
        strategies['Benchmark_80_20'] = self._run_benchmark(test_data, equity_pct=0.80)

        # 3. Vol Threshold
        strategies['Vol_Threshold'] = self._run_vol_threshold(test_data)

        # 4. 3-Regime HMM (fixed weights)
        strategies['3-Regime_HMM'] = self._run_fixed_hmm(test_data)

        # 5. 3-Regime HMM (Optimized)
        strategies['3-Regime_HMM_Optimized'] = self._run_optimized_hmm(test_data, train_data)

        return strategies

    def _run_benchmark(self, test_data: pd.DataFrame, equity_pct: float) -> np.ndarray:
        """Run static benchmark strategy WITH annual rebalancing."""
        dates = test_data.index
        n_days = len(dates)
        tickers = list(self.config.ETF_CONFIG.keys())

        n_eq = len(self.config.EQUITY_TICKERS)
        n_bond = len(self.config.BOND_TICKERS)
        bond_pct = 1.0 - equity_pct

        target = {t: equity_pct / n_eq for t in self.config.EQUITY_TICKERS}
        target.update({t: bond_pct / n_bond for t in self.config.BOND_TICKERS})
        target.update({t: 0.0 for t in self.config.GOLD_TICKERS})

        holdings = {t: self.config.INITIAL_CAPITAL * target.get(t, 0.0) for t in tickers}
        values = np.zeros(n_days)
        values[0] = self.config.INITIAL_CAPITAL
        last_rebal_year = dates[0].year

        for i in range(1, n_days):
            date = dates[i]
            pv = 0.0
            for ticker in tickers:
                ret_col = f"{ticker}_Return"
                ret = test_data.loc[date, ret_col] if ret_col in test_data.columns else 0.0
                if pd.isna(ret):
                    ret = 0.0
                holdings[ticker] *= (1 + ret)
                pv += holdings[ticker]
            values[i] = pv

            if date.year > last_rebal_year:
                for ticker in tickers:
                    holdings[ticker] = pv * target.get(ticker, 0.0)
                last_rebal_year = date.year

        return values

    def _run_vol_threshold(self, test_data: pd.DataFrame) -> np.ndarray:
        """Run volatility threshold strategy."""
        aggressive = {"XIU.TO": 0.30, "VFV.TO": 0.25, "XEF.TO": 0.15,
                     "XBB.TO": 0.20, "CGL-C.TO": 0.10}
        defensive = {"XIU.TO": 0.15, "VFV.TO": 0.13, "XEF.TO": 0.12,
                    "XBB.TO": 0.40, "CGL-C.TO": 0.20}

        dates = test_data.index
        n_days = len(dates)
        tickers = list(self.config.ETF_CONFIG.keys())

        holdings = {t: self.config.INITIAL_CAPITAL * aggressive.get(t, 0.0) for t in tickers}
        values = np.zeros(n_days)
        values[0] = self.config.INITIAL_CAPITAL

        current_weights = aggressive.copy()

        for i in range(1, n_days):
            date = dates[i]

            # Apply returns
            pv = 0.0
            for ticker in tickers:
                ret_col = f"{ticker}_Return"
                ret = test_data.loc[date, ret_col] if ret_col in test_data.columns else 0.0
                if pd.isna(ret):
                    ret = 0.0
                holdings[ticker] *= (1 + ret)
                pv += holdings[ticker]
            values[i] = pv

            # Determine target weights
            vol = test_data.loc[date, "Market_RolVol20"] if "Market_RolVol20" in test_data.columns else 0.15
            target_weights = defensive if (not pd.isna(vol) and vol > 0.20) else aggressive

            # Check drift
            actual_weights = {t: holdings[t]/pv if pv > 0 else 0.0 for t in tickers}
            max_drift = max(abs(actual_weights.get(t, 0) - target_weights.get(t, 0)) for t in tickers)

            if max_drift > self.config.REBALANCE_THRESHOLD:
                for ticker in tickers:
                    holdings[ticker] = pv * target_weights.get(ticker, 0.0)
                current_weights = target_weights.copy()

        return values

    def _run_fixed_hmm(self, test_data: pd.DataFrame) -> np.ndarray:
        """Run 3-regime HMM with fixed weights."""
        dates = test_data.index
        n_days = len(dates)
        tickers = list(self.config.ETF_CONFIG.keys())

        # Use config target weights
        target_weights_by_regime = self.config.TARGET_WEIGHTS

        # Build weight series
        weight_series = []
        for _, row in test_data.iterrows():
            regime = int(row['Regime']) if 'Regime' in row and not pd.isna(row['Regime']) else 1
            weight_series.append(target_weights_by_regime[regime])

        # Simulate
        holdings = {t: self.config.INITIAL_CAPITAL * weight_series[0].get(t, 0.0) for t in tickers}
        values = np.zeros(n_days)
        values[0] = self.config.INITIAL_CAPITAL

        for i in range(1, n_days):
            date = dates[i]

            # Apply returns
            pv = 0.0
            for ticker in tickers:
                ret_col = f"{ticker}_Return"
                ret = test_data.loc[date, ret_col] if ret_col in test_data.columns else 0.0
                if pd.isna(ret):
                    ret = 0.0
                holdings[ticker] *= (1 + ret)
                pv += holdings[ticker]
            values[i] = pv

            # Check drift
            target_w = weight_series[i]
            actual_w = {t: holdings[t]/pv if pv > 0 else 0.0 for t in tickers}
            max_drift = max(abs(actual_w.get(t, 0) - target_w.get(t, 0)) for t in tickers)

            if max_drift > self.config.REBALANCE_THRESHOLD:
                for ticker in tickers:
                    holdings[ticker] = pv * target_w.get(ticker, 0.0)

        return values

    def _run_optimized_hmm(self, test_data: pd.DataFrame, train_data: pd.DataFrame) -> np.ndarray:
        """Run 3-regime HMM with optimised weights and posterior probability blending."""
        # Calculate optimised weights on training data only (no lookahead)
        opt_strategy = OptimizedAllocationStrategy(self.config)
        regime_stats = opt_strategy.calculate_regime_statistics(
            train_data, train_end=train_data.index[-1]
        )
        optimized_weights = opt_strategy.create_optimized_weights(regime_stats)

        dates = test_data.index
        n_days = len(dates)
        tickers = list(self.config.ETF_CONFIG.keys())

        # Build posterior-blended weight series
        weight_series = []
        for _, row in test_data.iterrows():
            posteriors = np.array([
                row.get(f"P_Regime{r}", 1.0 / self.config.N_REGIMES)
                for r in range(self.config.N_REGIMES)
            ])
            posteriors = np.nan_to_num(posteriors, nan=1.0 / self.config.N_REGIMES)
            posteriors /= posteriors.sum()

            blended = {t: 0.0 for t in tickers}
            for rid, prob in enumerate(posteriors):
                for t in tickers:
                    blended[t] += prob * optimized_weights[rid].get(t, 0.0)
            total_w = sum(blended.values())
            if total_w > 0:
                blended = {t: v / total_w for t, v in blended.items()}
            weight_series.append(blended)

        # Simulate
        holdings = {t: self.config.INITIAL_CAPITAL * weight_series[0].get(t, 0.0) for t in tickers}
        values = np.zeros(n_days)
        values[0] = self.config.INITIAL_CAPITAL

        for i in range(1, n_days):
            date = dates[i]

            # Apply returns
            pv = 0.0
            for ticker in tickers:
                ret_col = f"{ticker}_Return"
                ret = test_data.loc[date, ret_col] if ret_col in test_data.columns else 0.0
                if pd.isna(ret):
                    ret = 0.0
                holdings[ticker] *= (1 + ret)
                pv += holdings[ticker]
            values[i] = pv

            # Check drift
            target_w = weight_series[i]
            actual_w = {t: holdings[t]/pv if pv > 0 else 0.0 for t in tickers}
            max_drift = max(abs(actual_w.get(t, 0) - target_w.get(t, 0)) for t in tickers)

            if max_drift > self.config.REBALANCE_THRESHOLD:
                for ticker in tickers:
                    holdings[ticker] = pv * target_w.get(ticker, 0.0)

        return values

    def _calculate_metrics(self, values: np.ndarray, dates: pd.DatetimeIndex = None) -> Dict:
        """
        Calculate performance metrics for a value series.

        Uses the time-varying T-bill rate (Config._tbill_series) when available,
        consistent with OptimizedAllocationStrategy.calculate_metrics. Falls back
        to the constant RISK_FREE_RATE if the T-bill series was not downloaded.
        """
        returns = pd.Series(values).pct_change().dropna()
        if len(returns) == 0:
            return {}

        total_return = (values[-1] - values[0]) / values[0]
        ann_return = (1 + total_return) ** (252 / len(returns)) - 1
        ann_vol = returns.std() * np.sqrt(252)

        # Time-varying Sharpe — same approach as OptimizedAllocationStrategy
        rf_series = getattr(Config, '_tbill_series', None)
        if rf_series is not None and dates is not None:
            daily_rf = rf_series.reindex(returns.index).ffill().fillna(self.config.RISK_FREE_RATE) / 252
            excess_returns = returns - daily_rf
            ann_excess = excess_returns.mean() * 252
            sharpe = ann_excess / ann_vol if ann_vol > 0 else np.nan
        else:
            sharpe = (ann_return - self.config.RISK_FREE_RATE) / ann_vol if ann_vol > 0 else np.nan

        cumulative = pd.Series(values)
        rolling_max = cumulative.cummax()
        drawdown = (cumulative - rolling_max) / rolling_max
        max_dd = drawdown.min()
        calmar = ann_return / abs(max_dd) if max_dd != 0 else np.nan

        return {
            'Total_Return': total_return,
            'Ann_Return': ann_return,
            'Ann_Vol': ann_vol,
            'Sharpe_Ratio': sharpe,
            'Max_Drawdown': max_dd,
            'Calmar_Ratio': calmar,
        }

    def _print_walk_forward_summary(self, results_df: pd.DataFrame) -> None:
        """Print summary of walk-forward results."""
        print("\n" + "="*80)
        print("WALK-FORWARD ANALYSIS SUMMARY")
        print("="*80)

        # Average metrics by strategy
        print("\n[Average Performance Across All Splits]")
        print("-"*80)

        avg_metrics = results_df.groupby('Strategy').agg({
            'Sharpe_Ratio': 'mean',
            'Ann_Return': 'mean',
            'Ann_Vol': 'mean',
            'Max_Drawdown': 'mean',
            'Calmar_Ratio': 'mean'
        }).round(4)

        # Sort by Sharpe ratio
        avg_metrics = avg_metrics.sort_values('Sharpe_Ratio', ascending=False)

        print(f"\n{'Strategy':<30} {'Sharpe':>8} {'Ann Ret':>9} {'Ann Vol':>9} {'Max DD':>9} {'Calmar':>8}")
        print("-"*80)
        for strategy, row in avg_metrics.iterrows():
            print(f"{strategy:<30} {row['Sharpe_Ratio']:>8.3f} {row['Ann_Return']:>8.1%} "
                  f"{row['Ann_Vol']:>8.1%} {row['Max_Drawdown']:>8.1%} {row['Calmar_Ratio']:>8.3f}")

        # Consistency analysis
        print("\n[Consistency: % of Splits Where Strategy Outperformed 60/40]")
        print("-"*80)

        # Pivot to compare
        pivot = results_df.pivot(index='Split', columns='Strategy', values='Sharpe_Ratio')
        if 'Benchmark_60_40' in pivot.columns:
            for strategy in ['Benchmark_80_20', 'Vol_Threshold', '3-Regime_HMM', '3-Regime_HMM_Optimized']:
                if strategy in pivot.columns:
                    outperform_pct = (pivot[strategy] > pivot['Benchmark_60_40']).mean() * 100
                    avg_excess = (pivot[strategy] - pivot['Benchmark_60_40']).mean()
                    print(f"  {strategy:<30}: {outperform_pct:>5.1f}% of splits  (Avg excess Sharpe: {avg_excess:>+.3f})")


# ══════════════════════════════════════════════════════════════════════════════
# MODULE 4: ENHANCED MODEL COMPARISON WITH CHARTS
# ══════════════════════════════════════════════════════════════════════════════

class EnhancedModelComparison:
    """Compare all strategies with comprehensive metrics and visualizations."""

    def __init__(self, config: Config):
        self.config = config
        self.strategy_colors = {
            "Benchmark_60_40": "#95A5A6",
            "Benchmark_80_20": "#7F8C8D",
            "Vol_Threshold": "#3498DB",
            "3-Regime_HMM": "#F39C12",
            "3-Regime_HMM_Optimized": "#2ECC71",
        }
        self.strategy_styles = {
            "Benchmark_60_40": ":",
            "Benchmark_80_20": "--",
            "Vol_Threshold": "-.",
            "3-Regime_HMM": "-",
            "3-Regime_HMM_Optimized": "-",
        }

    def run_comparison(self, master: pd.DataFrame,
                      include_optimized: bool = True) -> Tuple[pd.DataFrame, Dict]:
        """
        Compare all strategies on full dataset.

        Returns:
            (summary_df, strategy_results_dict)
        """
        print("\n" + "="*80)
        print("COMPREHENSIVE MODEL COMPARISON")
        print("="*80)

        # Build strategies
        strategies = self._build_all_strategies(master, include_optimized)

        # Run backtests
        results = {}

        print("\n[Comparison] Running backtests...")
        dates = master.index
        for name, weight_series in strategies.items():
            values = self._simulate(master, weight_series)
            metrics = self._calculate_metrics(values, dates=dates)
            results[name] = {
                'values': values,
                'metrics': metrics,
                'weight_series': weight_series
            }
            print(f"  {name:<30}: Sharpe={metrics['Sharpe_Ratio']:.3f}, "
                  f"Return={metrics['Ann_Return']:.1%}, MaxDD={metrics['Max_Drawdown']:.1%}")

        # Create summary table
        summary_df = self._create_summary_table(results)

        # Save results
        Utils.ensure_output_dir(self.config.OUTPUT_DIR)
        summary_path = os.path.join(self.config.OUTPUT_DIR, "model_comparison_summary.csv")
        summary_df.to_csv(summary_path, index=False)
        print(f"\n  Saved: {summary_path}")

        # Generate charts
        self._generate_charts(master, results)

        return summary_df, results

    def _build_all_strategies(self, master: pd.DataFrame,
                             include_optimized: bool) -> Dict[str, List[Dict]]:
        """Build weight series for all strategies."""
        strategies = {}
        tickers = list(self.config.ETF_CONFIG.keys())

        # 1. Benchmark 60/40
        n_eq = len(self.config.EQUITY_TICKERS)
        n_bond = len(self.config.BOND_TICKERS)
        weights_60_40 = {t: 0.60/n_eq for t in self.config.EQUITY_TICKERS}
        weights_60_40.update({t: 0.40/n_bond for t in self.config.BOND_TICKERS})
        weights_60_40.update({t: 0.0 for t in self.config.GOLD_TICKERS})
        strategies['Benchmark_60_40'] = [weights_60_40.copy() for _ in range(len(master))]

        # 2. Benchmark 80/20
        weights_80_20 = {t: 0.80/n_eq for t in self.config.EQUITY_TICKERS}
        weights_80_20.update({t: 0.20/n_bond for t in self.config.BOND_TICKERS})
        weights_80_20.update({t: 0.0 for t in self.config.GOLD_TICKERS})
        strategies['Benchmark_80_20'] = [weights_80_20.copy() for _ in range(len(master))]

        # 3. Vol Threshold
        aggressive = {"XIU.TO": 0.30, "VFV.TO": 0.25, "XEF.TO": 0.15,
                     "XBB.TO": 0.20, "CGL-C.TO": 0.10}
        defensive = {"XIU.TO": 0.15, "VFV.TO": 0.13, "XEF.TO": 0.12,
                    "XBB.TO": 0.40, "CGL-C.TO": 0.20}
        vol_weights = []
        for _, row in master.iterrows():
            vol = row.get("Market_RolVol20", 0.15)
            w = defensive if (not pd.isna(vol) and vol > 0.20) else aggressive
            vol_weights.append(w)
        strategies['Vol_Threshold'] = vol_weights

        # 4. 3-Regime HMM (fixed weights)
        hmm_weights = []
        for _, row in master.iterrows():
            regime = int(row.get('Regime', 1)) if not pd.isna(row.get('Regime')) else 1
            hmm_weights.append(self.config.TARGET_WEIGHTS[regime])
        strategies['3-Regime_HMM'] = hmm_weights

        # 5. 3-Regime HMM (Optimized) - if requested
        if include_optimized:
            opt_strategy = OptimizedAllocationStrategy(self.config)
            # Use only training data for regime stats — no lookahead
            regime_stats = opt_strategy.calculate_regime_statistics(
                master, train_end=self.config.TRAIN_END
            )
            optimized_weights = opt_strategy.create_optimized_weights(regime_stats)

            # Posterior-blended weights (same logic as run_optimized_backtest)
            opt_hmm_weights = []
            tickers_inner = list(self.config.ETF_CONFIG.keys())
            for _, row in master.iterrows():
                posteriors = np.array([
                    row.get(f"P_Regime{r}", 1.0 / self.config.N_REGIMES)
                    for r in range(self.config.N_REGIMES)
                ])
                posteriors = np.nan_to_num(posteriors, nan=1.0 / self.config.N_REGIMES)
                posteriors /= posteriors.sum()
                blended = {t: 0.0 for t in tickers_inner}
                for rid, prob in enumerate(posteriors):
                    for t in tickers_inner:
                        blended[t] += prob * optimized_weights[rid].get(t, 0.0)
                total_w = sum(blended.values())
                if total_w > 0:
                    blended = {t: v / total_w for t, v in blended.items()}
                opt_hmm_weights.append(blended)
            strategies['3-Regime_HMM_Optimized'] = opt_hmm_weights

        return strategies

    def _simulate(self, master: pd.DataFrame, weight_series: List[Dict]) -> np.ndarray:
        """Simulate portfolio with given weight series."""
        dates = master.index
        n_days = len(dates)
        tickers = list(self.config.ETF_CONFIG.keys())

        values = np.zeros(n_days)
        holdings = {t: self.config.INITIAL_CAPITAL * weight_series[0].get(t, 0.0) for t in tickers}
        values[0] = self.config.INITIAL_CAPITAL

        for i in range(1, n_days):
            date = dates[i]
            pv = 0.0
            for ticker in tickers:
                ret_col = f"{ticker}_Return"
                ret = master.loc[date, ret_col] if ret_col in master.columns else 0.0
                if pd.isna(ret):
                    ret = 0.0
                holdings[ticker] *= (1 + ret)
                pv += holdings[ticker]
            values[i] = pv

            # Rebalance check
            target = weight_series[i]
            actual = {t: holdings[t]/pv if pv > 0 else 0.0 for t in tickers}
            max_drift = max(abs(actual.get(t, 0) - target.get(t, 0)) for t in tickers)

            if max_drift > self.config.REBALANCE_THRESHOLD:
                for ticker in tickers:
                    holdings[ticker] = pv * target.get(ticker, 0.0)

        return values

    def _calculate_metrics(self, values: np.ndarray, dates: pd.DatetimeIndex = None) -> Dict:
        """
        Calculate performance metrics.

        Uses the time-varying T-bill rate (Config._tbill_series) when available,
        consistent with OptimizedAllocationStrategy.calculate_metrics.
        """
        returns = pd.Series(values).pct_change().dropna()
        if len(returns) == 0:
            return {}

        total_return = (values[-1] - values[0]) / values[0]
        ann_return = (1 + total_return) ** (252 / len(returns)) - 1
        ann_vol = returns.std() * np.sqrt(252)

        # Time-varying Sharpe — consistent with OptimizedAllocationStrategy
        rf_series = getattr(Config, '_tbill_series', None)
        if rf_series is not None and dates is not None:
            daily_rf = rf_series.reindex(returns.index).ffill().fillna(self.config.RISK_FREE_RATE) / 252
            excess_returns = returns - daily_rf
            ann_excess = excess_returns.mean() * 252
            sharpe = ann_excess / ann_vol if ann_vol > 0 else np.nan
        else:
            sharpe = (ann_return - self.config.RISK_FREE_RATE) / ann_vol if ann_vol > 0 else np.nan

        cumulative = pd.Series(values)
        rolling_max = cumulative.cummax()
        drawdown = (cumulative - rolling_max) / rolling_max
        max_dd = drawdown.min()
        calmar = ann_return / abs(max_dd) if max_dd != 0 else np.nan

        return {
            'Total_Return': total_return,
            'Ann_Return': ann_return,
            'Ann_Vol': ann_vol,
            'Sharpe_Ratio': sharpe,
            'Max_Drawdown': max_dd,
            'Calmar_Ratio': calmar,
        }

    def _create_summary_table(self, results: Dict) -> pd.DataFrame:
        """Create summary comparison table."""
        summary = []

        strategy_order = [
            'Benchmark_60_40',
            'Benchmark_80_20',
            'Vol_Threshold',
            '3-Regime_HMM',
            '3-Regime_HMM_Optimized'
        ]

        for strategy in strategy_order:
            if strategy in results:
                m = results[strategy]['metrics']
                summary.append({
                    'Strategy': strategy,
                    'Sharpe_Ratio': m['Sharpe_Ratio'],
                    'Ann_Return': m['Ann_Return'],
                    'Ann_Vol': m['Ann_Vol'],
                    'Max_Drawdown': m['Max_Drawdown'],
                    'Calmar_Ratio': m['Calmar_Ratio'],
                    'Total_Return': m['Total_Return'],
                })

        summary_df = pd.DataFrame(summary)

        print("\n" + "="*90)
        print("PERFORMANCE SUMMARY - ALL STRATEGIES")
        print("="*90)
        print(f"\n{'Strategy':<30} {'Sharpe':>8} {'Ann Ret':>9} {'Ann Vol':>9} {'Max DD':>9} {'Calmar':>8}")
        print("-"*90)
        for _, row in summary_df.iterrows():
            print(f"{row['Strategy']:<30} {row['Sharpe_Ratio']:>8.3f} {row['Ann_Return']:>8.1%} "
                  f"{row['Ann_Vol']:>8.1%} {row['Max_Drawdown']:>8.1%} {row['Calmar_Ratio']:>8.3f}")
        print("="*90)

        return summary_df

    def _generate_charts(self, master: pd.DataFrame, results: Dict) -> None:
        """Generate all comparison charts."""
        print("\n[Comparison] Generating charts...")

        Utils.ensure_output_dir(self.config.OUTPUT_DIR)
        dates = master.index

        # Chart 1: Cumulative Returns
        self._plot_cumulative_returns(dates, results)

        # Chart 2: Drawdowns
        self._plot_drawdowns(dates, results)

        # Chart 3: Rolling Sharpe Ratios
        self._plot_rolling_sharpe(dates, results)

        # Chart 4: Performance Metrics Bar Chart
        self._plot_metrics_comparison(results)

        # Chart 5: Risk-Return Scatter
        self._plot_risk_return_scatter(results)

        print("  ✓ All charts saved")

    def _plot_cumulative_returns(self, dates: pd.DatetimeIndex, results: Dict) -> None:
        """Plot cumulative returns for all strategies."""
        fig, ax = plt.subplots(figsize=(14, 7))

        strategy_order = ['Benchmark_60_40', 'Benchmark_80_20', 'Vol_Threshold',
                         '3-Regime_HMM', '3-Regime_HMM_Optimized']

        for strategy in strategy_order:
            if strategy in results:
                values = results[strategy]['values']
                ax.plot(dates, values,
                       label=strategy.replace('_', ' '),
                       color=self.strategy_colors.get(strategy, '#000000'),
                       linestyle=self.strategy_styles.get(strategy, '-'),
                       linewidth=2.5 if 'Optimized' in strategy else 1.8,
                       alpha=0.9)

        # Shade stress periods
        for event, (start, end) in self.config.STRESS_EVENTS.items():
            ax.axvspan(pd.Timestamp(start), pd.Timestamp(end),
                      alpha=0.08, color='red', zorder=0)

        ax.set_title("Cumulative Returns — All Strategies", fontsize=14, fontweight='bold')
        ax.set_ylabel("Portfolio Value ($)", fontsize=12)
        ax.set_xlabel("Date", fontsize=12)
        ax.legend(fontsize=10, loc='upper left')
        ax.grid(alpha=0.3)
        Utils.add_year_grid(ax)
        plt.tight_layout()

        path = os.path.join(self.config.OUTPUT_DIR, "chart_cumulative_returns.png")
        fig.savefig(path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"    Saved: chart_cumulative_returns.png")

    def _plot_drawdowns(self, dates: pd.DatetimeIndex, results: Dict) -> None:
        """Plot drawdowns for all strategies."""
        fig, ax = plt.subplots(figsize=(14, 6))

        strategy_order = ['Benchmark_60_40', 'Benchmark_80_20', 'Vol_Threshold',
                         '3-Regime_HMM', '3-Regime_HMM_Optimized']

        for strategy in strategy_order:
            if strategy in results:
                values = results[strategy]['values']
                cumulative = pd.Series(values)
                rolling_max = cumulative.cummax()
                drawdown = (cumulative - rolling_max) / rolling_max

                ax.plot(dates, drawdown * 100,
                       label=strategy.replace('_', ' '),
                       color=self.strategy_colors.get(strategy, '#000000'),
                       linestyle=self.strategy_styles.get(strategy, '-'),
                       linewidth=2.5 if 'Optimized' in strategy else 1.8,
                       alpha=0.9)

        # Shade stress periods
        for event, (start, end) in self.config.STRESS_EVENTS.items():
            ax.axvspan(pd.Timestamp(start), pd.Timestamp(end),
                      alpha=0.08, color='red', zorder=0)

        ax.set_title("Drawdowns — All Strategies", fontsize=14, fontweight='bold')
        ax.set_ylabel("Drawdown (%)", fontsize=12)
        ax.set_xlabel("Date", fontsize=12)
        ax.legend(fontsize=10, loc='lower right')
        ax.grid(alpha=0.3)
        Utils.add_year_grid(ax)
        plt.tight_layout()

        path = os.path.join(self.config.OUTPUT_DIR, "chart_drawdowns.png")
        fig.savefig(path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"    Saved: chart_drawdowns.png")

    def _plot_rolling_sharpe(self, dates: pd.DatetimeIndex, results: Dict) -> None:
        """Plot 252-day rolling Sharpe ratios."""
        fig, ax = plt.subplots(figsize=(14, 6))

        strategy_order = ['Benchmark_60_40', 'Benchmark_80_20', 'Vol_Threshold',
                         '3-Regime_HMM', '3-Regime_HMM_Optimized']

        # Use time-varying T-bill rate if available, else constant RF
        rf_series = getattr(Config, '_tbill_series', None)

        for strategy in strategy_order:
            if strategy in results:
                values = results[strategy]['values']
                returns = pd.Series(values, index=dates).pct_change().dropna()

                # 252-day rolling Sharpe using time-varying RF rate
                rolling_std = returns.rolling(252).std() * np.sqrt(252)
                if rf_series is not None:
                    daily_rf = rf_series.reindex(returns.index).ffill().fillna(self.config.RISK_FREE_RATE) / 252
                    excess = returns - daily_rf
                    rolling_excess = excess.rolling(252).mean() * 252
                    rolling_sharpe = rolling_excess / rolling_std
                else:
                    rolling_mean = returns.rolling(252).mean() * 252
                    rolling_sharpe = (rolling_mean - self.config.RISK_FREE_RATE) / rolling_std

                ax.plot(dates[1:], rolling_sharpe,
                       label=strategy.replace('_', ' '),
                       color=self.strategy_colors.get(strategy, '#000000'),
                       linestyle=self.strategy_styles.get(strategy, '-'),
                       linewidth=2.5 if 'Optimized' in strategy else 1.8,
                       alpha=0.9)

        ax.axhline(0, color='black', linestyle='--', linewidth=1, alpha=0.5)
        ax.set_title("Rolling 252-Day Sharpe Ratio — All Strategies", fontsize=14, fontweight='bold')
        ax.set_ylabel("Sharpe Ratio", fontsize=12)
        ax.set_xlabel("Date", fontsize=12)
        ax.legend(fontsize=10, loc='upper left')
        ax.grid(alpha=0.3)
        Utils.add_year_grid(ax)
        plt.tight_layout()

        path = os.path.join(self.config.OUTPUT_DIR, "chart_rolling_sharpe.png")
        fig.savefig(path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"    Saved: chart_rolling_sharpe.png")

    def _plot_metrics_comparison(self, results: Dict) -> None:
        """Plot performance metrics bar chart."""
        strategy_order = ['Benchmark_60_40', 'Benchmark_80_20', 'Vol_Threshold',
                         '3-Regime_HMM', '3-Regime_HMM_Optimized']
        strategies = [s for s in strategy_order if s in results]

        metrics = ['Sharpe_Ratio', 'Ann_Return', 'Max_Drawdown', 'Calmar_Ratio']
        titles = ['Sharpe Ratio', 'Annual Return', 'Max Drawdown', 'Calmar Ratio']

        fig, axes = plt.subplots(1, 4, figsize=(16, 5))

        for ax, metric, title in zip(axes, metrics, titles):
            vals = [results[s]['metrics'][metric] for s in strategies]
            colors = [self.strategy_colors[s] for s in strategies]

            bars = ax.bar(range(len(strategies)), vals, color=colors,
                          edgecolor='white', linewidth=0.8)

            # Highlight best
            if metric == 'Max_Drawdown':
                best_idx = np.argmax(vals)  # Least negative is best
            else:
                best_idx = np.argmax(vals)
            bars[best_idx].set_edgecolor('black')
            bars[best_idx].set_linewidth(2.5)

            ax.set_title(title, fontsize=11, fontweight='bold')
            ax.set_xticks(range(len(strategies)))
            ax.set_xticklabels([s.replace('_', '\n') for s in strategies], fontsize=7)
            ax.grid(axis='y', alpha=0.3)

            # Add value labels
            for bar, v in zip(bars, vals):
                if metric in ['Ann_Return', 'Max_Drawdown']:
                    label = f"{v:.1%}"
                else:
                    label = f"{v:.2f}"
                ax.text(bar.get_x() + bar.get_width() / 2,
                       bar.get_height() * (1.05 if bar.get_height() > 0 else 0.95),
                       label, ha='center', va='bottom' if bar.get_height() > 0 else 'top',
                       fontsize=8, fontweight='bold')

        fig.suptitle("Performance Metrics Comparison", fontsize=13, fontweight='bold')
        plt.tight_layout()

        path = os.path.join(self.config.OUTPUT_DIR, "chart_metrics_comparison.png")
        fig.savefig(path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"    Saved: chart_metrics_comparison.png")

    def _plot_risk_return_scatter(self, results: Dict) -> None:
        """Plot risk-return scatter plot."""
        fig, ax = plt.subplots(figsize=(10, 7))

        strategy_order = ['Benchmark_60_40', 'Benchmark_80_20', 'Vol_Threshold',
                         '3-Regime_HMM', '3-Regime_HMM_Optimized']

        for strategy in strategy_order:
            if strategy in results:
                m = results[strategy]['metrics']
                ax.scatter(m['Ann_Vol'], m['Ann_Return'],
                          s=200, color=self.strategy_colors.get(strategy, '#000000'),
                          alpha=0.7, edgecolors='black', linewidth=1.5,
                          label=strategy.replace('_', ' '))

                # Add labels
                ax.annotate(strategy.replace('_', '\n'),
                           (m['Ann_Vol'], m['Ann_Return']),
                           xytext=(10, 10), textcoords='offset points',
                           fontsize=8, alpha=0.8)

        ax.set_title("Risk-Return Profile — All Strategies", fontsize=14, fontweight='bold')
        ax.set_xlabel("Annualized Volatility", fontsize=12)
        ax.set_ylabel("Annualized Return", fontsize=12)
        ax.grid(alpha=0.3)
        ax.legend(fontsize=9, loc='lower right')

        # Format axes as percentages
        Utils.pct_fmt(ax, 'x')
        Utils.pct_fmt(ax, 'y')

        plt.tight_layout()

        path = os.path.join(self.config.OUTPUT_DIR, "chart_risk_return_scatter.png")
        fig.savefig(path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"    Saved: chart_risk_return_scatter.png")


# ══════════════════════════════════════════════════════════════════════════════
# MAIN PIPELINE - COLAB FRIENDLY
# ══════════════════════════════════════════════════════════════════════════════

def run_pipeline(mode: str = "all", output_dir: str = None, use_optimization: bool = False,
                risk_aversion: Dict[int, float] = None, run_walk_forward: bool = False,
                walk_forward_initial_train_years: int = 5, walk_forward_test_years: int = 1):
    """
    Main execution pipeline for Google Colab.

    Parameters:
    -----------
    mode : str, default "all"
        Execution mode: "data", "model", "allocation", "comparison", "walk_forward", or "all"
    output_dir : str, optional
        Custom output directory (default: /content/regime_switching_output)
    use_optimization : bool, default False
        If True, use mean-variance optimized weights instead of fixed weights
    risk_aversion : dict, optional
        Risk aversion parameters for optimization {0: 1.0, 1: 2.0, 2: 4.0}
        Only used if use_optimization=True
    run_walk_forward : bool, default False
        If True, run walk-forward analysis
    walk_forward_initial_train_years : int, default 5
        Years of data for FIRST training window in walk-forward analysis
        (subsequent windows expand from start date)
    walk_forward_test_years : int, default 1
        Years of data for each testing window

    Examples:
    ---------
    >>> # Run complete pipeline with fixed weights
    >>> run_pipeline()

    >>> # Run with optimized weights
    >>> run_pipeline(use_optimization=True)

    >>> # Run with walk-forward analysis
    >>> run_pipeline(mode="all", run_walk_forward=True)

    >>> # Custom risk aversion (more conservative)
    >>> run_pipeline(use_optimization=True, risk_aversion={0: 2.0, 1: 3.0, 2: 5.0})

    >>> # Just prepare data
    >>> run_pipeline(mode="data")

    >>> # Custom output directory
    >>> run_pipeline(output_dir="/content/drive/MyDrive/results")
    """
    # Update config
    if output_dir:
        Config.OUTPUT_DIR = output_dir

    config = Config()

    print("\n" + "="*80)
    print("REGIME-SWITCHING ASSET ALLOCATION")
    if use_optimization:
        print("Using OPTIMIZED mean-variance portfolio weights")
    if run_walk_forward:
        print("Including WALK-FORWARD ANALYSIS for out-of-sample testing")
    print("="*80)
    print(f"Mode: {mode}")
    print(f"Output: {config.OUTPUT_DIR}")

    # Phase 1: Data Preparation
    if mode in ["data", "all"]:
        data_prep = DataPreparation(config)
        master = data_prep.prepare_data()
    else:
        # Load existing data
        data_path = os.path.join(config.OUTPUT_DIR, "cleaned_data.csv")
        if not os.path.exists(data_path):
            print(f"\nERROR: {data_path} not found. Run with mode='data' first.")
            return None
        master = pd.read_csv(data_path, index_col="Date", parse_dates=True)
        print(f"\n[Loaded] {data_path}")

    # Phase 2: HMM Modeling
    if mode in ["model", "all", "walk_forward"]:
        hmm_model = HMMModel(config)
        master = hmm_model.train(master)
    elif mode in ["allocation", "comparison"]:
        # Load existing model results
        data_path = os.path.join(config.OUTPUT_DIR, "data_with_regimes.csv")
        if not os.path.exists(data_path):
            print(f"\nERROR: {data_path} not found. Run with mode='model' first.")
            return None
        master = pd.read_csv(data_path, index_col="Date", parse_dates=True)
        print(f"\n[Loaded] {data_path}")

    # Phase 3: Allocation Strategy
    results = {}
    if mode in ["allocation", "all"]:
        if use_optimization:
            # Use optimized weights
            optimized_alloc = OptimizedAllocationStrategy(config)
            portfolio_df, results_df, optimized_weights = optimized_alloc.run_optimized_backtest(
                master, risk_aversion_by_regime=risk_aversion
            )
            results["allocation"] = {
                "portfolio": portfolio_df,
                "metrics": results_df,
                "optimized_weights": optimized_weights
            }
        else:
            # Use fixed weights
            allocation = AllocationStrategy(config)
            portfolio_df, results_df = allocation.run(master)
            results["allocation"] = {
                "portfolio": portfolio_df,
                "metrics": results_df
            }

    # Phase 4: Model Comparison
    if mode in ["comparison", "all"]:
        comparison = EnhancedModelComparison(config)
        try:
            summary_df, comparison_results = comparison.run_comparison(master, include_optimized=use_optimization)
        except TypeError as e:
            # Fallback for compatibility
            print(f"  Note: Using fallback comparison mode")
            summary_df = comparison.run_comparison(master)
            comparison_results = {}

        results["comparison"] = {
            "summary": summary_df,
            "details": comparison_results
        }

    # Phase 5: Walk-Forward Analysis (optional)
    if mode in ["walk_forward", "all"] and run_walk_forward:
        walk_forward = WalkForwardAnalysis(config)
        walk_forward_results = walk_forward.run_walk_forward(
            master,
            initial_train_years=walk_forward_initial_train_years,
            test_years=walk_forward_test_years
        )
        results["walk_forward"] = walk_forward_results

    print("\n" + "="*80)
    print("PIPELINE COMPLETE")
    print("="*80)
    print(f"Results saved to: {config.OUTPUT_DIR}")

    # Return results for interactive use
    if mode == "data":
        return master
    elif mode == "model":
        return master
    elif mode == "allocation":
        return results["allocation"]
    elif mode == "comparison":
        return results["comparison"]
    elif mode == "walk_forward":
        return results.get("walk_forward")
    else:  # "all"
        return results


# ══════════════════════════════════════════════════════════════════════════════
# CONVENIENCE FUNCTIONS FOR INTERACTIVE USE
# ══════════════════════════════════════════════════════════════════════════════

def quick_start():
    """
    Quick start - runs complete pipeline with default settings.
    Perfect for first-time users in Colab.
    """
    print("🚀 Running complete pipeline with default settings...")
    print("   This will take 5-10 minutes to download data and train models.\n")
    return run_pipeline(mode="all")


def quick_start_optimized():
    """
    Quick start with OPTIMIZED mean-variance portfolio weights.
    Uses historical return/covariance statistics to optimize regime-specific allocations.
    """
    print("🚀 Running complete pipeline with OPTIMIZED weights...")
    print("   This will take 5-10 minutes to download data and train models.")
    print("   Using mean-variance optimization for regime-specific portfolios.\n")
    return run_pipeline(mode="all", use_optimization=True)


def quick_start_with_walk_forward():
    """
    Complete pipeline with OPTIMIZED weights AND walk-forward analysis.
    This validates out-of-sample performance using rolling train/test windows.
    Takes 10-15 minutes.
    """
    print("🚀 Running complete pipeline with OPTIMIZED weights + WALK-FORWARD analysis...")
    print("   This will take 10-15 minutes to download data, train models, and validate.")
    print("   Walk-forward analysis tests out-of-sample performance robustness.\n")
    return run_pipeline(mode="all", use_optimization=True, run_walk_forward=True)


def prepare_data_only():
    """Download and prepare data without training models."""
    print("📊 Preparing data only...")
    return run_pipeline(mode="data")


def train_model_only():
    """Train HMM model (requires prepared data)."""
    print("🧠 Training HMM model...")
    return run_pipeline(mode="model")


def run_backtest_only(use_optimization: bool = False):
    """
    Run allocation backtest (requires trained model).

    Args:
        use_optimization: If True, use mean-variance optimized weights
    """
    if use_optimization:
        print("💼 Running OPTIMIZED allocation backtest...")
        return run_pipeline(mode="allocation", use_optimization=True)
    else:
        print("💼 Running allocation backtest...")
        return run_pipeline(mode="allocation")


def compare_strategies(use_optimization: bool = True):
    """
    Compare multiple strategies (requires trained model).

    Args:
        use_optimization: If True, include optimized strategy in comparison
    """
    print("📈 Comparing strategies...")
    return run_pipeline(mode="comparison", use_optimization=use_optimization)


def run_walk_forward_only(initial_train_years: int = 5, test_years: int = 1):
    """
    Run walk-forward analysis only (requires trained model).

    Uses EXPANDING windows: training set grows, test set slides forward.
    Example: Train 2015-2020→Test 2020-2021, Train 2015-2021→Test 2021-2022, etc.

    Args:
        initial_train_years: Years for first training window (subsequent windows expand)
        test_years: Years for each testing window
    """
    print(f"🔄 Running walk-forward analysis ({initial_train_years}y initial train, {test_years}y test)...")
    return run_pipeline(mode="walk_forward", run_walk_forward=True,
                       walk_forward_initial_train_years=initial_train_years,
                       walk_forward_test_years=test_years)


# ══════════════════════════════════════════════════════════════════════════════
# AUTO-EXECUTION FOR COLAB
# ══════════════════════════════════════════════════════════════════════════════

if __name__ == "__main__":
    # When run as script in Colab, provide helpful message
    print("\n" + "="*80)
    print("REGIME-SWITCHING ASSET ALLOCATION - Ready!")
    print("="*80)
    print("\nThis script is designed for Google Colab. To use it:")
    print("\n1. Quick start options:")
    print("   >>> results = quick_start()                           # Fixed weights")
    print("   >>> results = quick_start_optimized()                 # Optimized weights ⭐")
    print("   >>> results = quick_start_with_walk_forward()         # Optimized + validation 🔥")
    print("\n2. Run step-by-step:")
    print("   >>> data = prepare_data_only()")
    print("   >>> data_with_regimes = train_model_only()")
    print("   >>> portfolio = run_backtest_only(use_optimization=True)")
    print("   >>> comparison = compare_strategies(use_optimization=True)")
    print("   >>> walk_forward = run_walk_forward_only(train_years=5, test_years=1)")
    print("\n3. Custom execution:")
    print("   >>> run_pipeline(mode='all', use_optimization=True, run_walk_forward=True)")
    print("   >>> run_pipeline(output_dir='/content/drive/MyDrive/results')")
    print("\n4. Custom risk aversion (optimization only):")
    print("   >>> run_pipeline(use_optimization=True,")
    print("   ...              risk_aversion={0: 2.0, 1: 3.0, 2: 5.0})")
    print("\n" + "="*80)
    print("\n💡 RECOMMENDED: Try quick_start_with_walk_forward() to validate performance!")
    print("   This tests whether your strategy actually works out-of-sample.")

#results = quick_start()                    # Fixed weights")

#results = quick_start_optimized()          # Optimized weights ⭐")

results = quick_start_with_walk_forward()

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Regime-Switching Asset Allocation - Visualization & Reporting Module

Creates publication-quality charts and tables for presentations and blog posts.
Run this AFTER completing the main pipeline to generate all visuals.

Usage:
    1. Run main pipeline first: results = quick_start_with_walk_forward()
    2. Then run: create_presentation_materials()
"""

import os
import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import matplotlib.patches as mpatches
import seaborn as sns
from datetime import datetime

# ══════════════════════════════════════════════════════════════════════════════
# CONFIGURATION
# ══════════════════════════════════════════════════════════════════════════════

OUTPUT_DIR = "/content/regime_switching_output"
CHARTS_DIR = os.path.join(OUTPUT_DIR, "presentation_charts")
TABLES_DIR = os.path.join(OUTPUT_DIR, "presentation_tables")

# Chart styling - IMPROVED for high visibility
plt.style.use('default')  # Use default for better control
sns.set_palette("husl")

# Enhanced color scheme with high contrast
COLORS = {
    "Benchmark_60_40": "#2C3E50",      # Dark blue-gray
    "Benchmark_80_20": "#E74C3C",      # Bright red
    "Vol_Threshold": "#3498DB",        # Bright blue
    "3-Regime_HMM": "#F39C12",         # Orange
    "3-Regime_HMM_Optimized": "#27AE60",  # Green
}

# Distinct line styles for clarity
LINE_STYLES = {
    "Benchmark_60_40": "-",            # Solid
    "Benchmark_80_20": "--",           # Dashed
    "Vol_Threshold": "-.",             # Dash-dot
    "3-Regime_HMM": ":",               # Dotted
    "3-Regime_HMM_Optimized": "-",     # Solid (thicker)
}

# Line widths for emphasis
LINE_WIDTHS = {
    "Benchmark_60_40": 2.0,
    "Benchmark_80_20": 2.0,
    "Vol_Threshold": 2.5,
    "3-Regime_HMM": 2.5,
    "3-Regime_HMM_Optimized": 3.5,  # Thickest for main strategy
}

STRESS_EVENTS = {
    "COVID-19": ("2020-02-01", "2020-05-31"),
    "2022 Bear": ("2022-01-01", "2022-10-31"),
    "2018 Q4": ("2018-10-01", "2018-12-31"),
}

# ══════════════════════════════════════════════════════════════════════════════
# UTILITY FUNCTIONS
# ══════════════════════════════════════════════════════════════════════════════

def ensure_dirs():
    """Create output directories."""
    os.makedirs(CHARTS_DIR, exist_ok=True)
    os.makedirs(TABLES_DIR, exist_ok=True)

def add_year_grid(ax):
    """Add year-based grid."""
    ax.xaxis.set_major_formatter(mdates.DateFormatter("%Y"))
    ax.xaxis.set_major_locator(mdates.YearLocator())
    ax.grid(alpha=0.3, linestyle='--', linewidth=0.5)
    plt.setp(ax.xaxis.get_majorticklabels(), rotation=0)

def annotate_stress(ax, alpha=0.1):
    """Shade stress periods."""
    for label, (start, end) in STRESS_EVENTS.items():
        ax.axvspan(pd.Timestamp(start), pd.Timestamp(end),
                  alpha=alpha, color='red', zorder=0)
        # Add label
        mid = pd.Timestamp(start) + (pd.Timestamp(end) - pd.Timestamp(start)) / 2
        ymin, ymax = ax.get_ylim()
        ax.text(mid, ymax * 0.98, label, ha='center', va='top',
               fontsize=8, alpha=0.7, style='italic')

def save_chart(fig, name):
    """Save chart with high DPI."""
    path = os.path.join(CHARTS_DIR, name)
    fig.savefig(path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close(fig)
    print(f"  ✓ {name}")

# ══════════════════════════════════════════════════════════════════════════════
# CHART 1: REGIME TIMELINE WITH MARKET CONTEXT
# ══════════════════════════════════════════════════════════════════════════════

def create_regime_timeline():
    """Create comprehensive regime timeline chart."""
    print("\n[Charts] Creating regime timeline...")

    # Load data with error handling
    data_path = os.path.join(OUTPUT_DIR, "data_with_regimes.csv")
    if not os.path.exists(data_path):
        print(f"  ! {data_path} not found, skipping")
        return

    data = pd.read_csv(data_path, index_col="Date", parse_dates=True)

    fig, axes = plt.subplots(3, 1, figsize=(16, 10),
                            gridspec_kw={'height_ratios': [1, 2, 1.5]},
                            sharex=True)

    # Panel 1: Regime bands
    ax1 = axes[0]
    regime_colors = {0: "#27AE60", 1: "#F39C12", 2: "#E74C3C"}
    regime_labels = {0: "Bull Market", 1: "Normal Market", 2: "Crisis"}

    for regime in [0, 1, 2]:
        if 'Regime' not in data.columns:
            print("  ! Regime column not found")
            plt.close(fig)
            return
        mask = data['Regime'] == regime
        ax1.fill_between(data.index, 0, 1, where=mask,
                        transform=ax1.get_xaxis_transform(),
                        color=regime_colors[regime], alpha=0.9,
                        label=regime_labels[regime])

    ax1.set_ylim(0, 1)
    ax1.set_yticks([])
    ax1.set_ylabel("Market\nRegime", fontsize=11, fontweight='bold')
    ax1.legend(loc='upper right', ncol=3, fontsize=9, framealpha=0.95)
    ax1.set_title("Market Regime Detection & Portfolio Performance (2015-2024)",
                 fontsize=14, fontweight='bold', pad=20)

# Panel 2: Portfolio value comparison
    ax2 = axes[1]

    if "XIU.TO_Return" in data.columns:
        returns = data["XIU.TO_Return"].fillna(0)
        cumret = (1 + returns).cumprod() * 100000
        ax2.plot(data.index, cumret, color='#2C3E50', linewidth=2.5,
                label='Canadian Equity (XIU)', alpha=1.0, zorder=10)

        # FIX: Shade regimes using the axis transform for height (0 to 1)
        # but setting explicit Y-limits so the data stays visible.
        for regime, color in regime_colors.items():
            mask = data['Regime'] == regime
            ax2.fill_between(data.index, 0, 1, where=mask,
                            transform=ax2.get_xaxis_transform(),
                            alpha=0.08, color=color, zorder=1)

        # Keep the Y-axis tight to the actual portfolio values
        ax2.set_ylim(cumret.min() * 0.9, cumret.max() * 1.1)

    annotate_stress(ax2, alpha=0.12)
    ax2.set_ylabel("Portfolio Value ($)", fontsize=11, fontweight='bold')
    ax2.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:.0f}K'))
    ax2.grid(alpha=0.3, linestyle='--', linewidth=0.5)

    # Panel 3: Market volatility
    ax3 = axes[2]

    if "Market_RolVol20" in data.columns:
        vol_pct = data["Market_RolVol20"] * 100
        ax3.plot(data.index, vol_pct, color='#E74C3C', linewidth=2.0,
                label='20-Day Rolling Volatility (Annualized)', zorder=10)
        ax3.axhline(20, color='#2C3E50', linestyle='--', linewidth=1.5,
                   alpha=0.7, label='20% Threshold')

        # FIX: Again, use the 0-1 transform for shading height
        for regime, color in regime_colors.items():
            mask = data['Regime'] == regime
            ax3.fill_between(data.index, 0, 1, where=mask,
                            transform=ax3.get_xaxis_transform(),
                            alpha=0.08, color=color, zorder=1)

        # Keep volatility axis focused (e.g., 0 to max vol + buffer)
        ax3.set_ylim(0, vol_pct.max() * 1.1)

    ax3.set_ylabel("Volatility (%)", fontsize=11, fontweight='bold')
    ax3.set_xlabel("Date", fontsize=11, fontweight='bold')
    ax3.legend(loc='upper right', fontsize=9, framealpha=0.95)
    ax3.grid(alpha=0.3, linestyle='--', linewidth=0.5)
    add_year_grid(ax3)

    plt.tight_layout()
    save_chart(fig, "01_regime_timeline.png")

# ══════════════════════════════════════════════════════════════════════════════
# CHART 2: STRATEGY COMPARISON - CUMULATIVE RETURNS
# ══════════════════════════════════════════════════════════════════════════════


def _load_strategy_portfolios() -> Dict[str, pd.Series]:
    """
    Load actual backtest portfolio values from saved CSVs.

    Returns a dict mapping strategy name -> pd.Series of portfolio values
    indexed by date. Falls back gracefully if a CSV is not found.

    Files produced by the main pipeline:
      - model_comparison_summary.csv  (full-sample backtest values are NOT
        stored there — we reconstruct from the individual portfolio CSVs)
      - portfolio_backtest.csv        (3-Regime HMM fixed weights)
      - portfolio_optimized.csv       (3-Regime HMM Optimized)

    The benchmark 60/40 value series is stored in both portfolio CSVs as
    "Benchmark_Value". The 80/20 benchmark and Vol Threshold are only
    available as final metrics in model_comparison_summary.csv, not as full
    time series; for those strategies we fall back to reconstructing from
    the model_comparison_summary data — or skip if unavailable.
    """
    portfolios = {}

    # Load fixed-weight HMM portfolio (contains benchmark series too)
    fixed_path = os.path.join(OUTPUT_DIR, "portfolio_backtest.csv")
    if os.path.exists(fixed_path):
        df = pd.read_csv(fixed_path, index_col="Date", parse_dates=True)
        if "Portfolio_Value" in df.columns:
            portfolios["3-Regime_HMM"] = df["Portfolio_Value"]
        if "Benchmark_Value" in df.columns:
            portfolios["Benchmark_60_40"] = df["Benchmark_Value"]

    # Load optimized HMM portfolio
    opt_path = os.path.join(OUTPUT_DIR, "portfolio_optimized.csv")
    if os.path.exists(opt_path):
        df = pd.read_csv(opt_path, index_col="Date", parse_dates=True)
        if "Portfolio_Value" in df.columns:
            portfolios["3-Regime_HMM_Optimized"] = df["Portfolio_Value"]
        # Also grab benchmark from here if we didn't get it above
        if "Benchmark_60_40" not in portfolios and "Benchmark_Value" in df.columns:
            portfolios["Benchmark_60_40"] = df["Benchmark_Value"]

    # 80/20 and Vol_Threshold are not saved as full time-series by the pipeline.
    # They are available only as aggregate metrics in model_comparison_summary.csv.
    # Omit them from time-series charts; they will still appear in bar charts.

    return portfolios

def create_cumulative_returns_chart():
    """
    Create cumulative returns chart using actual backtest portfolio CSVs.

    Loads portfolio_backtest.csv (3-Regime HMM fixed weights) and
    portfolio_optimized.csv (3-Regime HMM Optimized) which are written by
    the main pipeline. The 60/40 benchmark value series is embedded in both
    files as Benchmark_Value.
    """
    print("\n[Charts] Creating cumulative returns chart...")

    portfolios = _load_strategy_portfolios()

    if not portfolios:
        print("  ! No portfolio CSVs found in output directory. "
              "Run the main pipeline first (quick_start_with_walk_forward()).")
        return

    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 10),
                                   gridspec_kw={"height_ratios": [3, 1]},
                                   sharex=True)

    strategy_order = ["Benchmark_60_40", "3-Regime_HMM", "3-Regime_HMM_Optimized"]

    for strategy in strategy_order:
        if strategy not in portfolios:
            continue
        series = portfolios[strategy]
        ax1.plot(series.index, series,
                label=strategy.replace("_", " "),
                color=COLORS.get(strategy, "#000000"),
                linestyle=LINE_STYLES.get(strategy, "-"),
                linewidth=LINE_WIDTHS.get(strategy, 2.0),
                alpha=1.0, zorder=10 if "Optimized" in strategy else 5)

    annotate_stress(ax1, alpha=0.1)
    ax1.set_title("Cumulative Returns: All Strategies ($100K Initial Investment)",
                 fontsize=14, fontweight="bold", pad=15)
    ax1.set_ylabel("Portfolio Value ($)", fontsize=11, fontweight="bold")
    ax1.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f"${x/1000:.0f}K"))
    ax1.legend(loc="upper left", fontsize=10, framealpha=0.95,
              edgecolor="black", fancybox=False)
    ax1.grid(alpha=0.3, linestyle="--", linewidth=0.5)

    # Bottom panel: excess return of HMM strategies vs 60/40
    if "Benchmark_60_40" in portfolios:
        bench = portfolios["Benchmark_60_40"]
        for strategy in ["3-Regime_HMM", "3-Regime_HMM_Optimized"]:
            if strategy not in portfolios:
                continue
            strat = portfolios[strategy].reindex(bench.index)
            relative = (strat / bench - 1) * 100
            ax2.plot(relative.index, relative,
                    label=strategy.replace("_", " "),
                    color=COLORS.get(strategy, "#000000"),
                    linestyle=LINE_STYLES.get(strategy, "-"),
                    linewidth=LINE_WIDTHS.get(strategy, 2.5),
                    alpha=1.0, zorder=10)

        # Shade positive excess for the optimized strategy
        if "3-Regime_HMM_Optimized" in portfolios:
            opt = portfolios["3-Regime_HMM_Optimized"].reindex(bench.index)
            relative_opt = (opt / bench - 1) * 100
            ax2.fill_between(relative_opt.index, 0, relative_opt,
                             where=(relative_opt > 0),
                             alpha=0.2, color=COLORS["3-Regime_HMM_Optimized"])

    ax2.axhline(0, color="#2C3E50", linestyle="--", linewidth=1.5, alpha=0.7)
    ax2.set_ylabel("Excess Return\nvs 60/40 (%)", fontsize=10, fontweight="bold")
    ax2.set_xlabel("Date", fontsize=11, fontweight="bold")
    ax2.legend(loc="upper left", fontsize=9, framealpha=0.95)
    ax2.grid(alpha=0.3, linestyle="--", linewidth=0.5)
    add_year_grid(ax2)

    plt.tight_layout()
    save_chart(fig, "02_cumulative_returns.png")

# ══════════════════════════════════════════════════════════════════════════════
# CHART 3: DRAWDOWN ANALYSIS
# ══════════════════════════════════════════════════════════════════════════════

def create_drawdown_chart():
    """
    Create drawdown chart using actual backtest portfolio CSVs.

    Loads portfolio values from portfolio_backtest.csv and
    portfolio_optimized.csv produced by the main pipeline.
    """
    print("\n[Charts] Creating drawdown chart...")

    portfolios = _load_strategy_portfolios()

    if not portfolios:
        print("  ! No portfolio CSVs found. Run the main pipeline first.")
        return

    fig, axes = plt.subplots(2, 1, figsize=(16, 10), sharex=True)

    strategy_order_all = ["Benchmark_60_40", "3-Regime_HMM", "3-Regime_HMM_Optimized"]

    def calc_drawdown(series: pd.Series) -> pd.Series:
        cummax = series.cummax()
        return (series / cummax - 1) * 100

    # Panel 1: All available strategies
    ax1 = axes[0]
    for strategy in strategy_order_all:
        if strategy not in portfolios:
            continue
        drawdown = calc_drawdown(portfolios[strategy])
        ax1.plot(drawdown.index, drawdown,
                label=strategy.replace("_", " "),
                color=COLORS.get(strategy, "#000000"),
                linewidth=2.5 if "Optimized" in strategy else 1.8,
                alpha=0.9)

    annotate_stress(ax1, alpha=0.1)
    ax1.set_title("Drawdown Analysis: Risk Management Comparison",
                 fontsize=14, fontweight="bold", pad=15)
    ax1.set_ylabel("Drawdown (%)", fontsize=11, fontweight="bold")
    ax1.legend(loc="lower right", fontsize=10, framealpha=0.95)
    ax1.grid(alpha=0.3)

    # Panel 2: Focus on benchmark vs regime-switching strategies (filled)
    ax2 = axes[1]
    for strategy in ["Benchmark_60_40", "3-Regime_HMM", "3-Regime_HMM_Optimized"]:
        if strategy not in portfolios:
            continue
        drawdown = calc_drawdown(portfolios[strategy])
        ax2.fill_between(drawdown.index, drawdown, 0,
                        alpha=0.3, color=COLORS.get(strategy, "#000000"),
                        label=strategy.replace("_", " "))
        ax2.plot(drawdown.index, drawdown,
                color=COLORS.get(strategy, "#000000"),
                linewidth=2.0, alpha=0.9)

    annotate_stress(ax2, alpha=0.1)
    ax2.set_ylabel("Drawdown (%)", fontsize=11, fontweight="bold")
    ax2.set_xlabel("Date", fontsize=11, fontweight="bold")
    ax2.legend(loc="lower right", fontsize=10, framealpha=0.95)
    add_year_grid(ax2)
    ax2.grid(alpha=0.3)

    plt.tight_layout()
    save_chart(fig, "03_drawdown_analysis.png")

# ══════════════════════════════════════════════════════════════════════════════
# CHART 4: ROLLING METRICS
# ══════════════════════════════════════════════════════════════════════════════

def create_rolling_metrics_chart():
    """
    Create rolling performance metrics chart using actual backtest portfolio CSVs.

    Loads portfolio values from portfolio_backtest.csv and
    portfolio_optimized.csv produced by the main pipeline. Uses the
    time-varying T-bill rate (from RiskFree_Ann column in data_with_regimes.csv)
    for the rolling Sharpe calculation, consistent with the main backtest.
    """
    print("\n[Charts] Creating rolling metrics chart...")

    portfolios = _load_strategy_portfolios()

    if not portfolios:
        print("  ! No portfolio CSVs found. Run the main pipeline first.")
        return

    # Load T-bill rate if available
    rf_series = None
    data_path = os.path.join(OUTPUT_DIR, "data_with_regimes.csv")
    if os.path.exists(data_path):
        meta = pd.read_csv(data_path, index_col="Date", parse_dates=True)
        if "RiskFree_Ann" in meta.columns:
            rf_series = meta["RiskFree_Ann"]

    rf_const = 0.02  # fallback

    fig, axes = plt.subplots(3, 1, figsize=(16, 12), sharex=True)

    strategies = ["Benchmark_60_40", "3-Regime_HMM", "3-Regime_HMM_Optimized"]

    # Panel 1: Rolling 252-day Sharpe (time-varying RF rate)
    ax1 = axes[0]
    for strategy in strategies:
        if strategy not in portfolios:
            continue
        series = portfolios[strategy]
        returns = series.pct_change().dropna()
        rolling_std = returns.rolling(252).std() * np.sqrt(252)

        if rf_series is not None:
            daily_rf = rf_series.reindex(returns.index).ffill().fillna(rf_const) / 252
            excess = returns - daily_rf
            rolling_excess_ann = excess.rolling(252).mean() * 252
            rolling_sharpe = rolling_excess_ann / rolling_std
        else:
            rolling_mean = returns.rolling(252).mean() * 252
            rolling_sharpe = (rolling_mean - rf_const) / rolling_std

        ax1.plot(returns.index, rolling_sharpe,
                label=strategy.replace("_", " "),
                color=COLORS.get(strategy, "#000000"),
                linewidth=2.5 if "Optimized" in strategy else 2.0,
                alpha=0.9)

    ax1.axhline(0, color="gray", linestyle="--", linewidth=1, alpha=0.5)
    ax1.axhline(1, color="green", linestyle=":", linewidth=1, alpha=0.5, label="Sharpe = 1.0")
    annotate_stress(ax1, alpha=0.08)
    ax1.set_title("Rolling Performance Metrics (252-Day Windows)",
                 fontsize=14, fontweight="bold", pad=15)
    ax1.set_ylabel("Sharpe Ratio", fontsize=11, fontweight="bold")
    ax1.legend(loc="upper left", fontsize=9)
    ax1.grid(alpha=0.3)

    # Panel 2: Rolling annualised returns
    ax2 = axes[1]
    for strategy in strategies:
        if strategy not in portfolios:
            continue
        returns = portfolios[strategy].pct_change().dropna()
        rolling_return = returns.rolling(252).mean() * 252 * 100
        ax2.plot(returns.index, rolling_return,
                label=strategy.replace("_", " "),
                color=COLORS.get(strategy, "#000000"),
                linewidth=2.0, alpha=0.9)

    ax2.axhline(0, color="gray", linestyle="--", linewidth=1, alpha=0.5)
    annotate_stress(ax2, alpha=0.08)
    ax2.set_ylabel("Annualized\nReturn (%)", fontsize=11, fontweight="bold")
    ax2.legend(loc="upper left", fontsize=9)
    ax2.grid(alpha=0.3)

    # Panel 3: Rolling volatility
    ax3 = axes[2]
    for strategy in strategies:
        if strategy not in portfolios:
            continue
        returns = portfolios[strategy].pct_change().dropna()
        rolling_vol = returns.rolling(252).std() * np.sqrt(252) * 100
        ax3.plot(returns.index, rolling_vol,
                label=strategy.replace("_", " "),
                color=COLORS.get(strategy, "#000000"),
                linewidth=2.0, alpha=0.9)

    annotate_stress(ax3, alpha=0.08)
    ax3.set_ylabel("Annualized\nVolatility (%)", fontsize=11, fontweight="bold")
    ax3.set_xlabel("Date", fontsize=11, fontweight="bold")
    ax3.legend(loc="upper left", fontsize=9)
    add_year_grid(ax3)
    ax3.grid(alpha=0.3)

    plt.tight_layout()
    save_chart(fig, "04_rolling_metrics.png")

# ══════════════════════════════════════════════════════════════════════════════
# CHART 5: PERFORMANCE METRICS BAR CHART
# ══════════════════════════════════════════════════════════════════════════════

def create_metrics_bar_chart():
    """Create comprehensive metrics comparison bar chart."""
    print("\n[Charts] Creating metrics bar chart...")

    # Load comparison data with better error handling
    try:
        comparison = pd.read_csv(os.path.join(OUTPUT_DIR, "model_comparison_summary.csv"))
    except:
        try:
            print("  ! model_comparison_summary.csv not found, using walk-forward results")
            wf = pd.read_csv(os.path.join(OUTPUT_DIR, "walk_forward_results.csv"))
            # Average numeric columns only
            numeric_cols = wf.select_dtypes(include=[np.number]).columns
            comparison = wf.groupby('Strategy')[numeric_cols].mean().reset_index()
        except Exception as e:
            print(f"  ! Could not load any comparison data: {e}")
            return

    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    axes = axes.flatten()

    metrics = [
        ('Sharpe_Ratio', 'Sharpe Ratio', False),
        ('Ann_Return', 'Annual Return (%)', True),
        ('Ann_Vol', 'Annual Volatility (%)', True),
        ('Max_Drawdown', 'Max Drawdown (%)', True),
        ('Calmar_Ratio', 'Calmar Ratio', False),
    ]

    strategy_order = ['Benchmark_60_40', 'Benchmark_80_20', 'Vol_Threshold',
                     '3-Regime_HMM', '3-Regime_HMM_Optimized']

    # Filter to available strategies
    available = [s for s in strategy_order if s in comparison['Strategy'].values]

    for idx, (metric, title, is_pct) in enumerate(metrics):
        ax = axes[idx]

        if metric not in comparison.columns:
            ax.text(0.5, 0.5, f'{metric}\nNot Available', ha='center', va='center')
            ax.set_title(title)
            ax.axis('off')
            continue

        # Get values
        values = []
        colors_list = []
        labels = []

        for strategy in available:
            row = comparison[comparison['Strategy'] == strategy]
            if len(row) > 0:
                val = row[metric].iloc[0]
                values.append(val * 100 if is_pct else val)
                colors_list.append(COLORS.get(strategy, '#000000'))
                labels.append(strategy.replace('_', '\n'))

        # Create bars
        bars = ax.bar(range(len(values)), values, color=colors_list,
                     edgecolor='white', linewidth=1.5, alpha=0.85)

        # Highlight best
        if len(values) > 0:
            if metric != 'Max_Drawdown':
                best_idx = np.argmax(values)
            else:
                best_idx = np.argmax(values)  # Least negative
            bars[best_idx].set_edgecolor('black')
            bars[best_idx].set_linewidth(3)
            bars[best_idx].set_alpha(1.0)

        # Labels
        ax.set_title(title, fontsize=12, fontweight='bold', pad=10)
        ax.set_xticks(range(len(values)))
        ax.set_xticklabels(labels, fontsize=8)
        ax.grid(axis='y', alpha=0.3, linestyle='--')

        # Value labels on bars
        for bar, val in zip(bars, values):
            height = bar.get_height()
            label_text = f"{val:.1f}%" if is_pct else f"{val:.2f}"
            ax.text(bar.get_x() + bar.get_width()/2, height,
                   label_text, ha='center', va='bottom' if height > 0 else 'top',
                   fontsize=9, fontweight='bold')

    # Remove extra subplot
    fig.delaxes(axes[5])

    fig.suptitle("Performance Metrics Comparison: All Strategies",
                fontsize=15, fontweight='bold', y=0.995)
    plt.tight_layout(rect=[0, 0, 1, 0.98])
    save_chart(fig, "05_metrics_comparison.png")

# ══════════════════════════════════════════════════════════════════════════════
# CHART 6: RISK-RETURN SCATTER
# ══════════════════════════════════════════════════════════════════════════════

def create_risk_return_scatter():
    """Create risk-return scatter plot."""
    print("\n[Charts] Creating risk-return scatter...")

    # Load comparison data
    try:
        comparison = pd.read_csv(os.path.join(OUTPUT_DIR, "model_comparison_summary.csv"))
    except:
        wf = pd.read_csv(os.path.join(OUTPUT_DIR, "walk_forward_results.csv"))
        # Select only numeric columns before aggregating to avoid the
        # 'agg function failed [how->mean, dtype->object]' error caused by
        # date/string columns being passed to .mean()
        numeric_cols = ['Ann_Vol', 'Ann_Return', 'Sharpe_Ratio', 'Max_Drawdown', 'Calmar_Ratio']
        available = [c for c in numeric_cols if c in wf.columns]
        comparison = wf.groupby('Strategy')[available].mean().reset_index()

    fig, ax = plt.subplots(figsize=(12, 8))

    strategy_order = ['Benchmark_60_40', 'Benchmark_80_20', 'Vol_Threshold',
                     '3-Regime_HMM', '3-Regime_HMM_Optimized']

    for strategy in strategy_order:
        row = comparison[comparison['Strategy'] == strategy]
        if len(row) == 0:
            continue

        vol = row['Ann_Vol'].iloc[0] * 100
        ret = row['Ann_Return'].iloc[0] * 100
        sharpe = row['Sharpe_Ratio'].iloc[0]

        # Plot point
        size = 300 if 'Optimized' in strategy else 200
        ax.scatter(vol, ret, s=size,
                  color=COLORS.get(strategy, '#000000'),
                  alpha=0.7, edgecolors='black', linewidth=2,
                  label=strategy.replace('_', ' '), zorder=3)

        # Add label with Sharpe
        label = strategy.replace('_', '\n')
        ax.annotate(f"{label}\n(Sharpe: {sharpe:.2f})",
                   (vol, ret), xytext=(15, 15), textcoords='offset points',
                   fontsize=9, bbox=dict(boxstyle='round,pad=0.5',
                                        facecolor=COLORS.get(strategy, '#000000'),
                                        alpha=0.2, edgecolor='black'),
                   arrowprops=dict(arrowstyle='->', color='black', lw=1))

    # Add Sharpe ratio lines
    for sharpe_val in [0.5, 1.0, 1.5]:
        vols = np.linspace(ax.get_xlim()[0], ax.get_xlim()[1], 100)
        rets = 2 + sharpe_val * vols  # Risk-free rate = 2%
        ax.plot(vols, rets, 'k--', alpha=0.2, linewidth=1, zorder=1)
        ax.text(ax.get_xlim()[1] * 0.95, 2 + sharpe_val * ax.get_xlim()[1] * 0.95,
               f'Sharpe={sharpe_val}', fontsize=8, alpha=0.5, ha='right')

    ax.set_title("Risk-Return Profile: Efficient Frontier Analysis",
                fontsize=14, fontweight='bold', pad=15)
    ax.set_xlabel("Annualized Volatility (%)", fontsize=12, fontweight='bold')
    ax.set_ylabel("Annualized Return (%)", fontsize=12, fontweight='bold')
    ax.legend(loc='lower right', fontsize=10, framealpha=0.95)
    ax.grid(alpha=0.3, linestyle='--')

    plt.tight_layout()
    save_chart(fig, "06_risk_return_scatter.png")

# ══════════════════════════════════════════════════════════════════════════════
# CHART 7: REGIME ALLOCATION PIE CHARTS
# ══════════════════════════════════════════════════════════════════════════════

def create_regime_allocation_chart():
    """Create pie charts showing allocations by regime."""
    print("\n[Charts] Creating regime allocation chart...")

    fig, axes = plt.subplots(1, 3, figsize=(18, 6))

    # Define allocations (from your fixed weights)
    allocations = {
        0: {"Equity (XIU+VFV+XEF)": 70, "Bonds (XBB)": 20, "Gold (CGL)": 10},
        1: {"Equity (XIU+VFV+XEF)": 60, "Bonds (XBB)": 28, "Gold (CGL)": 12},
        2: {"Equity (XIU+VFV+XEF)": 40, "Bonds (XBB)": 40, "Gold (CGL)": 20},
    }

    regime_labels = ["Bull Market\n(Low Volatility)",
                    "Normal Market\n(Medium Volatility)",
                    "Crisis\n(High Volatility)"]
    regime_colors_pie = {
        "Equity (XIU+VFV+XEF)": "#3498DB",
        "Bonds (XBB)": "#F39C12",
        "Gold (CGL)": "#F1C40F"
    }

    for idx, (regime, alloc) in enumerate(allocations.items()):
        ax = axes[idx]

        values = list(alloc.values())
        labels = list(alloc.keys())
        colors = [regime_colors_pie[l] for l in labels]

        wedges, texts, autotexts = ax.pie(values, labels=labels, colors=colors,
                                          autopct='%1.0f%%', startangle=90,
                                          textprops={'fontsize': 11, 'fontweight': 'bold'},
                                          wedgeprops={'edgecolor': 'white', 'linewidth': 2})

        for autotext in autotexts:
            autotext.set_color('white')
            autotext.set_fontsize(12)
            autotext.set_fontweight('bold')

        ax.set_title(f"Regime {regime}: {regime_labels[idx]}",
                    fontsize=12, fontweight='bold', pad=15)

    fig.suptitle("Target Asset Allocation by Market Regime",
                fontsize=14, fontweight='bold', y=1.02)
    plt.tight_layout()
    save_chart(fig, "07_regime_allocations.png")

# ══════════════════════════════════════════════════════════════════════════════
# CHART 8: WALK-FORWARD VALIDATION
# ══════════════════════════════════════════════════════════════════════════════

def create_walk_forward_chart():
    """Create walk-forward validation visualization."""
    print("\n[Charts] Creating walk-forward validation chart...")

    # Load walk-forward results
    try:
        wf = pd.read_csv(os.path.join(OUTPUT_DIR, "walk_forward_results.csv"))
    except:
        print("  ! walk_forward_results.csv not found, skipping")
        return

    fig, axes = plt.subplots(2, 2, figsize=(16, 10))

    strategies_to_plot = ['Benchmark_60_40', '3-Regime_HMM', '3-Regime_HMM_Optimized']

    # Get unique splits
    splits = wf['Split'].unique()

    # Panel 1: Sharpe ratio by split
    ax1 = axes[0, 0]
    x = np.arange(len(splits))
    width = 0.25

    for i, strategy in enumerate(strategies_to_plot):
        data = wf[wf['Strategy'] == strategy]
        values = [data[data['Split'] == s]['Sharpe_Ratio'].iloc[0] if len(data[data['Split'] == s]) > 0 else 0
                 for s in splits]
        ax1.bar(x + i*width, values, width, label=strategy.replace('_', ' '),
               color=COLORS.get(strategy, '#000000'), alpha=0.8)

    ax1.set_ylabel("Sharpe Ratio", fontsize=11, fontweight='bold')
    ax1.set_title("Out-of-Sample Sharpe Ratio by Test Period",
                 fontsize=12, fontweight='bold')
    ax1.set_xticks(x + width)
    ax1.set_xticklabels(splits, fontsize=9)
    ax1.legend(fontsize=9)
    ax1.grid(axis='y', alpha=0.3)
    ax1.axhline(1.0, color='green', linestyle='--', alpha=0.5, linewidth=1)

    # Panel 2: Annual return by split
    ax2 = axes[0, 1]
    for i, strategy in enumerate(strategies_to_plot):
        data = wf[wf['Strategy'] == strategy]
        values = [data[data['Split'] == s]['Ann_Return'].iloc[0] * 100 if len(data[data['Split'] == s]) > 0 else 0
                 for s in splits]
        ax2.bar(x + i*width, values, width, label=strategy.replace('_', ' '),
               color=COLORS.get(strategy, '#000000'), alpha=0.8)

    ax2.set_ylabel("Annual Return (%)", fontsize=11, fontweight='bold')
    ax2.set_title("Out-of-Sample Returns by Test Period",
                 fontsize=12, fontweight='bold')
    ax2.set_xticks(x + width)
    ax2.set_xticklabels(splits, fontsize=9)
    ax2.legend(fontsize=9)
    ax2.grid(axis='y', alpha=0.3)

    # Panel 3: Max drawdown by split
    ax3 = axes[1, 0]
    for i, strategy in enumerate(strategies_to_plot):
        data = wf[wf['Strategy'] == strategy]
        values = [data[data['Split'] == s]['Max_Drawdown'].iloc[0] * 100 if len(data[data['Split'] == s]) > 0 else 0
                 for s in splits]
        ax3.bar(x + i*width, values, width, label=strategy.replace('_', ' '),
               color=COLORS.get(strategy, '#000000'), alpha=0.8)

    ax3.set_ylabel("Max Drawdown (%)", fontsize=11, fontweight='bold')
    ax3.set_title("Out-of-Sample Max Drawdown by Test Period",
                 fontsize=12, fontweight='bold')
    ax3.set_xticks(x + width)
    ax3.set_xticklabels(splits, fontsize=9)
    ax3.legend(fontsize=9)
    ax3.grid(axis='y', alpha=0.3)

    # Panel 4: Consistency analysis
    ax4 = axes[1, 1]

    # Calculate win rates vs benchmark
    benchmark_data = wf[wf['Strategy'] == 'Benchmark_60_40']
    win_rates = []
    strategy_names = []

    for strategy in ['Benchmark_80_20', 'Vol_Threshold', '3-Regime_HMM', '3-Regime_HMM_Optimized']:
        if strategy not in wf['Strategy'].values:
            continue

        strat_data = wf[wf['Strategy'] == strategy]
        wins = 0
        total = 0

        for split in splits:
            bench_sharpe = benchmark_data[benchmark_data['Split'] == split]['Sharpe_Ratio']
            strat_sharpe = strat_data[strat_data['Split'] == split]['Sharpe_Ratio']

            if len(bench_sharpe) > 0 and len(strat_sharpe) > 0:
                if strat_sharpe.iloc[0] > bench_sharpe.iloc[0]:
                    wins += 1
                total += 1

        if total > 0:
            win_rate = (wins / total) * 100
            win_rates.append(win_rate)
            strategy_names.append(strategy.replace('_', '\n'))

    bars = ax4.barh(range(len(win_rates)), win_rates,
                   color=[COLORS.get(s.replace('\n', '_'), '#000000') for s in strategy_names],
                   alpha=0.8, edgecolor='black', linewidth=1.5)

    ax4.set_xlabel("Win Rate vs 60/40 Benchmark (%)", fontsize=11, fontweight='bold')
    ax4.set_title("Consistency: % of Splits Outperforming 60/40",
                 fontsize=12, fontweight='bold')
    ax4.set_yticks(range(len(win_rates)))
    ax4.set_yticklabels(strategy_names, fontsize=9)
    ax4.axvline(50, color='gray', linestyle='--', alpha=0.5, linewidth=1)
    ax4.axvline(70, color='green', linestyle=':', alpha=0.5, linewidth=1)
    ax4.grid(axis='x', alpha=0.3)

    # Add value labels
    for i, (bar, val) in enumerate(zip(bars, win_rates)):
        ax4.text(val + 2, bar.get_y() + bar.get_height()/2,
                f'{val:.0f}%', va='center', fontsize=10, fontweight='bold')

    fig.suptitle("Walk-Forward Analysis: Out-of-Sample Validation Results",
                fontsize=14, fontweight='bold', y=0.995)
    plt.tight_layout(rect=[0, 0, 1, 0.98])
    save_chart(fig, "08_walk_forward_validation.png")

# ══════════════════════════════════════════════════════════════════════════════
# TABLE 1: COMPREHENSIVE PERFORMANCE TABLE
# ══════════════════════════════════════════════════════════════════════════════

def create_performance_table():
    """Create comprehensive performance comparison table."""
    print("\n[Tables] Creating performance table...")

    # Load comparison data
    try:
        comparison = pd.read_csv(os.path.join(OUTPUT_DIR, "model_comparison_summary.csv"))
    except:
        wf = pd.read_csv(os.path.join(OUTPUT_DIR, "walk_forward_results.csv"))
        # Select only numeric columns before aggregating to avoid dtype errors
        numeric_cols = ['Ann_Vol', 'Ann_Return', 'Sharpe_Ratio', 'Max_Drawdown', 'Calmar_Ratio']
        available = [c for c in numeric_cols if c in wf.columns]
        comparison = wf.groupby('Strategy')[available].mean().reset_index()

    # Format table
    table_data = []

    strategy_order = ['Benchmark_60_40', 'Benchmark_80_20', 'Vol_Threshold',
                     '3-Regime_HMM', '3-Regime_HMM_Optimized']

    for strategy in strategy_order:
        row = comparison[comparison['Strategy'] == strategy]
        if len(row) == 0:
            continue

        table_data.append({
            'Strategy': strategy.replace('_', ' '),
            'Sharpe Ratio': f"{row['Sharpe_Ratio'].iloc[0]:.3f}",
            'Annual Return': f"{row['Ann_Return'].iloc[0]*100:.1f}%",
            'Annual Vol': f"{row['Ann_Vol'].iloc[0]*100:.1f}%",
            'Max Drawdown': f"{row['Max_Drawdown'].iloc[0]*100:.1f}%",
            'Calmar Ratio': f"{row['Calmar_Ratio'].iloc[0]:.3f}",
        })

    df_table = pd.DataFrame(table_data)

    # Save as CSV
    output_path = os.path.join(TABLES_DIR, "performance_summary.csv")
    df_table.to_csv(output_path, index=False)

    # Save as formatted text
    output_path_txt = os.path.join(TABLES_DIR, "performance_summary.txt")
    with open(output_path_txt, 'w') as f:
        f.write("="*90 + "\n")
        f.write("PERFORMANCE SUMMARY - ALL STRATEGIES\n")
        f.write("="*90 + "\n\n")
        f.write(df_table.to_string(index=False))
        f.write("\n\n" + "="*90 + "\n")

    print(f"  ✓ performance_summary.csv")
    print(f"  ✓ performance_summary.txt")

# ══════════════════════════════════════════════════════════════════════════════
# TABLE 2: WALK-FORWARD RESULTS TABLE
# ══════════════════════════════════════════════════════════════════════════════

def create_walk_forward_table():
    """Create detailed walk-forward results table."""
    print("\n[Tables] Creating walk-forward table...")

    try:
        wf = pd.read_csv(os.path.join(OUTPUT_DIR, "walk_forward_results.csv"))
    except:
        print("  ! walk_forward_results.csv not found, skipping")
        return

    # Pivot table
    pivot_sharpe = wf.pivot(index='Split', columns='Strategy', values='Sharpe_Ratio')
    pivot_return = wf.pivot(index='Split', columns='Strategy', values='Ann_Return')
    pivot_dd = wf.pivot(index='Split', columns='Strategy', values='Max_Drawdown')

    # Save pivots
    pivot_sharpe.to_csv(os.path.join(TABLES_DIR, "walk_forward_sharpe.csv"))
    pivot_return.to_csv(os.path.join(TABLES_DIR, "walk_forward_returns.csv"))
    pivot_dd.to_csv(os.path.join(TABLES_DIR, "walk_forward_drawdowns.csv"))

    # Create summary
    summary = wf.groupby('Strategy').agg({
        'Sharpe_Ratio': ['mean', 'std', 'min', 'max'],
        'Ann_Return': ['mean', 'std'],
        'Max_Drawdown': ['mean', 'min'],
    }).round(3)

    summary.to_csv(os.path.join(TABLES_DIR, "walk_forward_summary.csv"))

    print(f"  ✓ walk_forward_sharpe.csv")
    print(f"  ✓ walk_forward_returns.csv")
    print(f"  ✓ walk_forward_drawdowns.csv")
    print(f"  ✓ walk_forward_summary.csv")

# ══════════════════════════════════════════════════════════════════════════════
# TABLE 3: REGIME STATISTICS
# ══════════════════════════════════════════════════════════════════════════════

def create_regime_stats_table():
    """Create regime statistics table."""
    print("\n[Tables] Creating regime statistics table...")

    # Load data
    data = pd.read_csv(os.path.join(OUTPUT_DIR, "data_with_regimes.csv"),
                      index_col="Date", parse_dates=True)

    # Calculate stats by regime
    regime_stats = []

    for regime in [0, 1, 2]:
        mask = data['Regime'] == regime
        regime_data = data[mask]

        n_days = len(regime_data)
        pct_time = n_days / len(data) * 100

        # Average volatility
        if 'Market_RolVol20' in regime_data.columns:
            avg_vol = regime_data['Market_RolVol20'].mean() * 100
        else:
            avg_vol = None

        # Average return
        if 'XIU.TO_Return' in regime_data.columns:
            daily_ret = regime_data['XIU.TO_Return'].mean()
            ann_ret = daily_ret * 252 * 100
        else:
            ann_ret = None

        regime_stats.append({
            'Regime': f"Regime {regime}",
            'Label': ['Bull Market', 'Normal Market', 'Crisis'][regime],
            'Days': n_days,
            '% of Time': f"{pct_time:.1f}%",
            'Avg Volatility': f"{avg_vol:.1f}%" if avg_vol else "N/A",
            'Avg Return': f"{ann_ret:.1f}%" if ann_ret else "N/A",
        })

    df_regime = pd.DataFrame(regime_stats)

    # Save
    output_path = os.path.join(TABLES_DIR, "regime_statistics.csv")
    df_regime.to_csv(output_path, index=False)

    print(f"  ✓ regime_statistics.csv")

# ══════════════════════════════════════════════════════════════════════════════
# MAIN EXECUTION
# ══════════════════════════════════════════════════════════════════════════════

def create_presentation_materials():
    """Generate all presentation materials."""
    print("\n" + "="*80)
    print("CREATING PRESENTATION MATERIALS")
    print("="*80)

    ensure_dirs()

    print("\n📊 Generating Charts...")
    print("-"*80)

    try:
        create_regime_timeline()
    except Exception as e:
        print(f"  ✗ regime_timeline failed: {e}")

    try:
        create_cumulative_returns_chart()
    except Exception as e:
        print(f"  ✗ cumulative_returns failed: {e}")

    try:
        create_drawdown_chart()
    except Exception as e:
        print(f"  ✗ drawdown_chart failed: {e}")

    try:
        create_rolling_metrics_chart()
    except Exception as e:
        print(f"  ✗ rolling_metrics failed: {e}")

    try:
        create_metrics_bar_chart()
    except Exception as e:
        print(f"  ✗ metrics_bar_chart failed: {e}")

    try:
        create_risk_return_scatter()
    except Exception as e:
        print(f"  ✗ risk_return_scatter failed: {e}")

    try:
        create_regime_allocation_chart()
    except Exception as e:
        print(f"  ✗ regime_allocation failed: {e}")

    try:
        create_walk_forward_chart()
    except Exception as e:
        print(f"  ✗ walk_forward_chart failed: {e}")

    print("\n📋 Generating Tables...")
    print("-"*80)

    try:
        create_performance_table()
    except Exception as e:
        print(f"  ✗ performance_table failed: {e}")

    try:
        create_walk_forward_table()
    except Exception as e:
        print(f"  ✗ walk_forward_table failed: {e}")

    try:
        create_regime_stats_table()
    except Exception as e:
        print(f"  ✗ regime_stats_table failed: {e}")

    print("\n" + "="*80)
    print("✅ PRESENTATION MATERIALS COMPLETE")
    print("="*80)
    print(f"\nCharts saved to: {CHARTS_DIR}")
    print(f"Tables saved to: {TABLES_DIR}")
    print("\nGenerated files:")
    print("  Charts:")
    print("    01_regime_timeline.png")
    print("    02_cumulative_returns.png")
    print("    03_drawdown_analysis.png")
    print("    04_rolling_metrics.png")
    print("    05_metrics_comparison.png")
    print("    06_risk_return_scatter.png")
    print("    07_regime_allocations.png")
    print("    08_walk_forward_validation.png")
    print("\n  Tables:")
    print("    performance_summary.csv/.txt")
    print("    walk_forward_sharpe.csv")
    print("    walk_forward_returns.csv")
    print("    walk_forward_drawdowns.csv")
    print("    walk_forward_summary.csv")
    print("    regime_statistics.csv")

# ══════════════════════════════════════════════════════════════════════════════
# HELPER: DISPLAY CHARTS IN COLAB
# ══════════════════════════════════════════════════════════════════════════════

def display_all_charts():
    """Display all charts in Colab notebook."""
    from IPython.display import Image, display

    print("\n" + "="*80)
    print("DISPLAYING ALL CHARTS")
    print("="*80)

    charts = [
        "01_regime_timeline.png",
        "02_cumulative_returns.png",
        "03_drawdown_analysis.png",
        "04_rolling_metrics.png",
        "05_metrics_comparison.png",
        "06_risk_return_scatter.png",
        "07_regime_allocations.png",
        "08_walk_forward_validation.png",
    ]

    for chart in charts:
        path = os.path.join(CHARTS_DIR, chart)
        if os.path.exists(path):
            print(f"\n{chart}:")
            display(Image(path))
        else:
            print(f"\n{chart}: NOT FOUND")

# ══════════════════════════════════════════════════════════════════════════════
# AUTO-RUN INSTRUCTIONS
# ══════════════════════════════════════════════════════════════════════════════

if __name__ == "__main__":
    print("\n" + "="*80)
    print("PRESENTATION MATERIALS GENERATOR - Ready!")
    print("="*80)
    print("\nRun this AFTER completing the main pipeline.")
    print("\nUsage:")
    print("  >>> create_presentation_materials()    # Generate all charts and tables")
    print("  >>> display_all_charts()               # Display charts in notebook")
    print("\n" + "="*80)

create_presentation_materials()

