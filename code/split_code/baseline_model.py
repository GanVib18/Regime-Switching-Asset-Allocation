# -*- coding: utf-8 -*-
"""baseline_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15906tXRfVCwJBg8HRrfjYZk658OTp3H6

## Hidden Markov Modelling

### Install Dependencies
"""

!pip install hmmlearn

"""### Import Libraries"""

import os
import warnings
warnings.filterwarnings("ignore")

import joblib
import textwrap
from datetime import datetime

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import matplotlib.dates as mdates
import seaborn as sns
from hmmlearn.hmm import GaussianHMM
from sklearn.preprocessing import StandardScaler

"""### Helper Fn."""

def add_year_grid(ax: plt.Axes) -> None:
    """Attach annual x-tick grid (matches Phase 1 chart style)."""
    ax.xaxis.set_major_formatter(mdates.DateFormatter("%Y"))
    ax.xaxis.set_major_locator(mdates.YearLocator())
    ax.grid(alpha=0.3)
    plt.setp(ax.xaxis.get_majorticklabels(), rotation=0)


def pct_fmt(ax: plt.Axes, axis: str = "y") -> None:
    """Format an axis as percentage."""
    fmt = plt.FuncFormatter(lambda x, _: f"{x:.0%}")
    if axis == "y":
        ax.yaxis.set_major_formatter(fmt)
    else:
        ax.xaxis.set_major_formatter(fmt)


def regime_legend_patches(labels: dict, colors: dict) -> list:
    """Build matplotlib legend patches for regime colours."""
    return [mpatches.Patch(color=colors[r], label=labels[r])
            for r in sorted(labels)]

"""### Configuration"""

# Path to the file produced by data_prep.py
INPUT_FILE  = "/content/cleaned_data.csv"
OUTPUT_DIR  = "/content"

# HMM hyper-parameters
N_REGIMES      = 3           # States
N_ITER         = 2000        # EM iterations (more = more stable convergence)
N_INIT         = 20          # Number of random restarts (pick best log-likelihood)
RANDOM_STATE   = 42

# Train / validation / test cut-offs (must match plan)
TRAIN_END      = "2023-12-31"
VAL_END        = "2024-12-31"
# Everything after VAL_END is the test set (2025-to-date)

# Regime colour scheme (consistent throughout the project)
REGIME_LABELS = {0: "Low-Vol (Bull)",
                 1: "Medium-Vol (Normal)",
                 2: "High-Vol (Crisis)"}
REGIME_COLORS = {0: "#2ECC71", 1: "#F39C12", 2: "#E74C3C"}

"""### Load Data"""

print(f"\n[1/8]  Loading cleaned data from {INPUT_FILE} ...")

if not os.path.exists(INPUT_FILE):
    raise FileNotFoundError(
        f"Cannot find {INPUT_FILE}.\n"
        "Please run data_prep.py (Phase 1) first to generate cleaned_data.csv."
    )

master = pd.read_csv(INPUT_FILE, index_col="Date", parse_dates=True)
master.sort_index(inplace=True)
print(f"       Loaded  : {master.shape[0]} rows × {master.shape[1]} cols")
print(f"       Range   : {master.index[0].date()} → {master.index[-1].date()}")

# Features to feed into the HMM
# These were all computed and saved by data_prep.py
# The model uses market-level signals — individual-ETF vols are kept for
# downstream portfolio construction but we keep the HMM input compact to
# avoid overfitting.
FEATURE_COLS = [
    "Market_RolVol20",
    "Bond_RolVol20",
    "Equity_Bond_Corr20",
    "Equity_Mom60",
    "Bond_Mom60",
    "Gold_Mom60",
]

# Optional extra features — included only if present in cleaned_data.csv
OPTIONAL_FEATURES = [
    "Gold_Equity_Corr20",   # gold-equity correlation (risk-off signal)
    "WTI_Oil_Return",       # oil return (Canadian-market specific)
]

# Known market-stress events for sanity-checking regime labels
STRESS_EVENTS = {
    "COVID Crash":     ("2020-02-01", "2020-05-31"),
    "2022 Rate Shock": ("2022-01-01", "2022-12-31"),
    "2018 Q4":         ("2018-10-01", "2018-12-31"),
    "2020 Recovery":   ("2020-06-01", "2020-12-31"),  # should be Bull/Normal
}

"""### Feature Selection"""

print("\n[2/8]  Selecting HMM features ...")

# Core features — must be present
missing_core = [c for c in FEATURE_COLS if c not in master.columns]
if missing_core:
    raise KeyError(
        f"Core feature columns missing from cleaned_data.csv: {missing_core}\n"
        "Check that data_prep.py ran successfully."
    )

# Optional features — add silently if present
optional_present = [c for c in OPTIONAL_FEATURES if c in master.columns]
all_features = FEATURE_COLS + optional_present

print(f"       Core features    : {FEATURE_COLS}")
print(f"       Optional added   : {optional_present}")
print(f"       Total features   : {len(all_features)}")

# Subsample to the feature set, drop any residual NaNs
data = master[all_features].dropna()
print(f"       Rows after dropna: {len(data)}")

"""### Data Splits"""

print("\n[3/8]  Splitting data into train / validation / test ...")

train_mask = data.index <= TRAIN_END
val_mask   = (data.index > TRAIN_END) & (data.index <= VAL_END)
test_mask  = data.index > VAL_END

X_train = data.loc[train_mask].values
X_val   = data.loc[val_mask].values
X_test  = data.loc[test_mask].values

print(f"       Train : {train_mask.sum():>4} rows  "
      f"({data.index[train_mask][0].date()} → {data.index[train_mask][-1].date()})")
print(f"       Val   : {val_mask.sum():>4} rows  "
      f"({data.index[val_mask][0].date()} → {data.index[val_mask][-1].date()})"
      if val_mask.any() else "       Val   : 0 rows  (not yet available)")
print(f"       Test  : {test_mask.sum():>4} rows  "
      f"({data.index[test_mask][0].date()} → {data.index[test_mask][-1].date()})"
      if test_mask.any() else "       Test  : 0 rows  (not yet available)")

if len(X_train) == 0:
    raise RuntimeError("Training set is empty — check TRAIN_END date and input data range.")

"""### Standardization"""

# HMMs are sensitive to feature scale; standardising to zero-mean / unit-variance
# speeds convergence and prevents any single feature from dominating.

print("\n[4/8]  Standardising features (fit on train, transform all) ...")

scaler = StandardScaler()
X_train_sc = scaler.fit_transform(X_train)           # fit ONLY on training data
X_val_sc   = scaler.transform(X_val)  if len(X_val)  > 0 else np.empty((0, X_train.shape[1]))
X_test_sc  = scaler.transform(X_test) if len(X_test) > 0 else np.empty((0, X_train.shape[1]))

print(f"       Scaler means (train): {np.round(scaler.mean_, 4)}")
print(f"       Scaler std   (train): {np.round(scaler.scale_, 4)}")

"""### Fit Gaussian HMM"""

# We run N_INIT random initialisations and keep the model with the highest
# training log-likelihood — this avoids bad local optima in the EM algorithm.

print(f"\n[5/8]  Fitting Gaussian HMM "
      f"(n_regimes={N_REGIMES}, n_init={N_INIT}, n_iter={N_ITER}) ...")

best_model  = None
best_loglik = -np.inf
loglik_history = []

for i in range(N_INIT):
    model = GaussianHMM(
        n_components=N_REGIMES,
        covariance_type="full",   # full covariance captures cross-feature correlations
        n_iter=N_ITER,
        random_state=RANDOM_STATE + i,
        tol=1e-5,
        verbose=False,
    )
    try:
        model.fit(X_train_sc)
        ll = model.score(X_train_sc)     # log-likelihood on training set
        loglik_history.append(ll)
        if ll > best_loglik:
            best_loglik = ll
            best_model  = model
        if (i + 1) % 5 == 0:
            print(f"       Init {i+1:>3}/{N_INIT}  "
                  f"log-likelihood: {ll:>12.2f}  "
                  f"(best so far: {best_loglik:>12.2f})")
    except Exception as exc:
        print(f"       Init {i+1:>3}/{N_INIT}  FAILED: {exc}")

if best_model is None:
    raise RuntimeError("All HMM initialisations failed — check feature data.")

print(f"\n       Best log-likelihood : {best_loglik:.4f}")
print(f"       Converged          : {best_model.monitor_.converged}")
print(f"       Iterations used    : {best_model.monitor_.iter}")

# Convenience alias
hmm = best_model

"""### Decode Regimes"""

print("\n[6/8]  Decoding regimes (Viterbi) and extracting posteriors ...")

def decode_segment(model: GaussianHMM,
                   X_sc: np.ndarray,
                   idx: pd.DatetimeIndex) -> pd.DataFrame:
    """
    Run Viterbi decoding and posterior probability extraction on a segment.
    Returns a DataFrame with columns: Regime, P(R0), P(R1), P(R2).
    """
    if len(X_sc) == 0:
        cols = ["Regime"] + [f"P_Regime{r}" for r in range(model.n_components)]
        return pd.DataFrame(columns=cols, index=idx)

    logprob, states = model.decode(X_sc, algorithm="viterbi")
    posteriors      = model.predict_proba(X_sc)     # shape (T, n_regimes)

    df = pd.DataFrame(index=idx)
    df["Regime"] = states
    for r in range(model.n_components):
        df[f"P_Regime{r}"] = posteriors[:, r]
    df["HMM_LogProb"] = logprob / len(X_sc)        # per-step log-likelihood
    return df


train_idx = data.index[train_mask]
val_idx   = data.index[val_mask]
test_idx  = data.index[test_mask]

regime_train = decode_segment(hmm, X_train_sc, train_idx)
regime_val   = decode_segment(hmm, X_val_sc,   val_idx)
regime_test  = decode_segment(hmm, X_test_sc,  test_idx)

regime_all = pd.concat([regime_train, regime_val, regime_test])
regime_all.sort_index(inplace=True)

"""### Label Regimes Semantically"""

# The HMM states are numbered arbitrarily (0, 1, 2).
# We identify which state corresponds to which market regime by inspecting
# the mean Market_RolVol20 in each state on the TRAINING set.

print("\n[7/8]  Labelling regimes by volatility level ...")

# Compute mean of Market_RolVol20 per raw HMM state on training data
vol_col_idx    = all_features.index("Market_RolVol20")   # position in feature array
raw_vol_train  = X_train[:, vol_col_idx]                 # un-scaled values

state_vol_means = {}
for s in range(N_REGIMES):
    mask = regime_train["Regime"].values == s
    if mask.sum() > 0:
        state_vol_means[s] = raw_vol_train[mask].mean()
    else:
        state_vol_means[s] = 0.0

# Sort states by ascending volatility → 0 = low-vol, 1 = mid, 2 = high-vol
sorted_states   = sorted(state_vol_means, key=state_vol_means.get)
state_to_regime = {sorted_states[0]: 0,   # lowest vol  → Regime 0 (Bull)
                   sorted_states[1]: 1,   # middle vol  → Regime 1 (Normal)
                   sorted_states[2]: 2}   # highest vol → Regime 2 (Crisis)

# Remap regime IDs in the DataFrame and the posterior columns
regime_all["Regime"] = regime_all["Regime"].map(state_to_regime)

# Reorder posterior columns to match new semantic labelling
old_p_cols  = [f"P_Regime{s}" for s in sorted_states]
new_p_cols  = [f"P_Regime{r}" for r in range(N_REGIMES)]
p_rename    = dict(zip(old_p_cols, new_p_cols))
regime_all.rename(columns=p_rename, inplace=True)

print("       State → Regime mapping (ascending vol):")
for old_s, new_r in state_to_regime.items():
    print(f"         HMM state {old_s}  →  "
          f"Regime {new_r}: {REGIME_LABELS[new_r]}  "
          f"(mean Market_RolVol20 = {state_vol_means[old_s]:.2%})")

"""### Transition Probability Matrix"""

# Extract the transition matrix from the HMM and reorder to match regime labels
raw_trans = hmm.transmat_                  # shape (N, N), rows = from, cols = to

# Build a re-ordered matrix
ordered_states = sorted_states             # [low-vol state, mid state, high-vol state]
trans_matrix   = np.zeros((N_REGIMES, N_REGIMES))
for new_from, old_from in enumerate(ordered_states):
    for new_to, old_to in enumerate(ordered_states):
        trans_matrix[new_from, new_to] = raw_trans[old_from, old_to]

trans_df = pd.DataFrame(
    trans_matrix,
    index   = [f"From: {REGIME_LABELS[r]}" for r in range(N_REGIMES)],
    columns = [f"To: {REGIME_LABELS[r]}"   for r in range(N_REGIMES)],
)

print("\n       Transition Probability Matrix:")
print(trans_df.to_string())

"""### Regime Characteristics"""

# Compute mean return and mean volatility in each regime (training set only)

ret_cols_master = [c for c in master.columns if c.endswith("_Return")]

# Merge regimes back into master for analysis
master_with_reg = master.join(regime_all[["Regime"]], how="left")

regime_stats = {}
for r in range(N_REGIMES):
    mask = master_with_reg["Regime"] == r
    subset = master_with_reg.loc[mask]
    regime_stats[r] = {
        "label"      : REGIME_LABELS[r],
        "n_days"     : int(mask.sum()),
        "pct_time"   : mask.sum() / mask.notna().sum(),
        "mean_mkt_vol": subset["Market_RolVol20"].mean() if "Market_RolVol20" in subset else np.nan,
        "mean_eq_bond_corr": subset["Equity_Bond_Corr20"].mean() if "Equity_Bond_Corr20" in subset else np.nan,
    }
    # Mean daily return and annualised vol per ETF
    for col in ret_cols_master:
        ticker_key = col.replace("_Return", "")
        regime_stats[r][f"mean_ret_{ticker_key}"]  = subset[col].mean()   if col in subset else np.nan
        regime_stats[r][f"ann_ret_{ticker_key}"]   = subset[col].mean() * 252 if col in subset else np.nan
        regime_stats[r][f"ann_vol_{ticker_key}"]   = subset[col].std() * np.sqrt(252) if col in subset else np.nan

"""### Stress Event Coverage"""

print("\n       Sanity Check — known stress events vs detected regimes:")
print(f"       {'Event':<20}  {'Crisis days':>11}  {'Crisis %':>9}")
print("       " + "-" * 45)

for event, (start, end) in STRESS_EVENTS.items():
    event_mask = (master_with_reg.index >= start) & (master_with_reg.index <= end)
    if event_mask.sum() == 0:
        continue
    crisis_frac = (master_with_reg.loc[event_mask, "Regime"] == 2).mean()
    print(f"       {event:<20}  "
          f"{int(crisis_frac * event_mask.sum()):>8} days  "
          f"{crisis_frac:>9.0%}")

"""### Persistence Check"""

# A good HMM should show persistent regimes (avg duration >> 1 day)

print("\n       Regime Persistence (avg consecutive days):")
reg_series = regime_all["Regime"].dropna()
runs, durations = [], []
prev, run = reg_series.iloc[0], 1
for r in reg_series.iloc[1:]:
    if r == prev:
        run += 1
    else:
        runs.append(prev)
        durations.append(run)
        prev, run = r, 1
runs.append(prev); durations.append(run)

dur_df = pd.DataFrame({"Regime": runs, "Duration": durations})
for r in range(N_REGIMES):
    avg_dur = dur_df.loc[dur_df["Regime"] == r, "Duration"].mean()
    print(f"         Regime {r} ({REGIME_LABELS[r]:<25}): {avg_dur:.1f} days avg")

"""### Save Artefacts"""

print(f"\n[8/8]  Saving outputs to {OUTPUT_DIR} ...")

# 14a. Serialised model + scaler
model_path = os.path.join(OUTPUT_DIR, "model_hmm.pkl")
joblib.dump({"hmm": hmm, "scaler": scaler,
             "state_to_regime": state_to_regime,
             "feature_cols": all_features,
             "n_regimes": N_REGIMES},
            model_path)
print(f"       OK - model_hmm.pkl")

# 14b. Data with regime labels
out_data = master.join(regime_all, how="left")
out_data.to_csv(os.path.join(OUTPUT_DIR, "data_with_regimes.csv"))
print(f"       OK - data_with_regimes.csv")

# 14c. Transition matrix
trans_df.to_csv(os.path.join(OUTPUT_DIR, "regime_transition_matrix.csv"))
print(f"       OK - regime_transition_matrix.csv")

# 14d. Text summary report
report_path = os.path.join(OUTPUT_DIR, "hmm_summary.txt")
with open(report_path, "w") as f:
    f.write(f"REGIME-SWITCHING ASSET ALLOCATION — PHASE 2 HMM REPORT\n")
    f.write(f"Generated : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    f.write("=" * 65 + "\n\n")
    f.write(f"Model     : Gaussian HMM  |  States: {N_REGIMES}  |  "
            f"Covariance: full\n")
    f.write(f"Converged : {hmm.monitor_.converged}  |  "
            f"Iterations: {hmm.monitor_.iter}\n")
    f.write(f"Train log-likelihood: {best_loglik:.4f}\n\n")
    f.write("TRANSITION MATRIX\n")
    f.write(trans_df.to_string() + "\n\n")
    f.write("REGIME CHARACTERISTICS (full dataset)\n")
    f.write(f"  {'Regime':<26}  {'Days':>6}  {'%Time':>6}  "
            f"{'Avg Mkt Vol':>11}  {'EQ-Bond Corr':>12}\n")
    f.write("  " + "-" * 68 + "\n")
    for r, stats in regime_stats.items():
        f.write(f"  {REGIME_LABELS[r]:<26}  "
                f"{stats['n_days']:>6}  "
                f"{stats['pct_time']:>6.1%}  "
                f"{stats['mean_mkt_vol']:>11.2%}  "
                f"{stats['mean_eq_bond_corr']:>12.4f}\n")
    f.write("\nSTRESS EVENT SANITY CHECK\n")
    for event, (start, end) in STRESS_EVENTS.items():
        event_mask = (master_with_reg.index >= start) & (master_with_reg.index <= end)
        if event_mask.sum() == 0:
            continue
        crisis_frac = (master_with_reg.loc[event_mask, "Regime"] == 2).mean()
        f.write(f"  {event:<20}  {crisis_frac:.0%} classified as Crisis\n")

print(f"       OK - hmm_summary.txt")

"""### Charts"""

# --- Chart 1: Regime Timeline -------------------------------------------
fig, axes = plt.subplots(3, 1, figsize=(14, 9),
                          gridspec_kw={"height_ratios": [1, 2, 1.5]},
                          sharex=True)

# Top panel: discrete regime bands
ax = axes[0]
for r in range(N_REGIMES):
    mask = out_data["Regime"] == r
    ax.fill_between(out_data.index, 0, 1,
                    where=mask, transform=ax.get_xaxis_transform(),
                    color=REGIME_COLORS[r], alpha=0.85)
ax.set_yticks([])
ax.set_ylabel("Regime", fontsize=9)
ax.legend(handles=regime_legend_patches(REGIME_LABELS, REGIME_COLORS),
          loc="upper right", fontsize=7, ncol=3)
ax.set_title("Regime Timeline — Hidden Markov Model (3 States)",
             fontsize=13, fontweight="bold")

# Middle panel: XIU return + regime shading
if "XIU.TO_Return" in out_data.columns:
    ret_series = out_data["XIU.TO_Return"].fillna(0)
    cumret     = (1 + ret_series).cumprod()
    ax2 = axes[1]
    ax2.plot(out_data.index, cumret, color="#264653", linewidth=1.2,
             label="XIU.TO (TSX 60) cumulative return")
    for r, color in REGIME_COLORS.items():
        mask = out_data["Regime"] == r
        ax2.fill_between(out_data.index, cumret.min(), cumret.max(),
                         where=mask, transform=ax2.get_xaxis_transform(),
                         alpha=0.12, color=color)
    ax2.set_ylabel("Cumulative Return (base=1)")
    ax2.legend(loc="upper left", fontsize=8)
    ax2.grid(alpha=0.3)

# Bottom panel: market rolling volatility with regime shading
if "Market_RolVol20" in out_data.columns:
    ax3 = axes[2]
    ax3.plot(out_data.index, out_data["Market_RolVol20"],
             color="#2C3E50", linewidth=1.0, label="20-day Rolling Vol (annualised)")
    for r, color in REGIME_COLORS.items():
        mask = out_data["Regime"] == r
        ax3.fill_between(out_data.index,
                         0, out_data["Market_RolVol20"].max() * 1.1,
                         where=mask, transform=ax3.get_xaxis_transform(),
                         alpha=0.12, color=color)
    ax3.set_ylabel("Annualised Vol")
    pct_fmt(ax3)
    ax3.legend(loc="upper right", fontsize=8)
    add_year_grid(ax3)
    ax3.grid(alpha=0.3)

for event, (start, end) in STRESS_EVENTS.items():
    for panel in axes:
        panel.axvspan(pd.Timestamp(start), pd.Timestamp(end),
                      alpha=0.08, color="black", zorder=0)

plt.tight_layout()
fig.savefig(os.path.join(OUTPUT_DIR, "chart_regime_timeline.png"),
            dpi=150, bbox_inches="tight")
plt.close()
print(f"       OK - chart_regime_timeline.png")


# --- Chart 2: Posterior Probability Over Time ----------------------------
p_cols = [f"P_Regime{r}" for r in range(N_REGIMES) if f"P_Regime{r}" in out_data.columns]

if p_cols:
    fig, ax = plt.subplots(figsize=(14, 4))
    bottom = np.zeros(len(out_data))
    for r, col in enumerate(p_cols):
        values = out_data[col].fillna(0).values
        ax.fill_between(out_data.index, bottom, bottom + values,
                        color=REGIME_COLORS[r], alpha=0.82, label=REGIME_LABELS[r])
        bottom += values
    ax.set_title("Regime Posterior Probabilities Over Time",
                 fontsize=13, fontweight="bold")
    ax.set_ylabel("Probability")
    ax.set_ylim(0, 1)
    ax.legend(loc="upper right", fontsize=8, ncol=3)
    add_year_grid(ax)
    plt.tight_layout()
    fig.savefig(os.path.join(OUTPUT_DIR, "chart_regime_posterior.png"),
                dpi=150, bbox_inches="tight")
    plt.close()
    print(f"       OK - chart_regime_posterior.png")


# --- Chart 3: Transition Probability Heatmap ----------------------------
fig, ax = plt.subplots(figsize=(7, 5))
short_labels = [f"R{r}: {REGIME_LABELS[r][:10]}" for r in range(N_REGIMES)]
sns.heatmap(trans_matrix, annot=True, fmt=".2%", cmap="YlOrRd",
            xticklabels=short_labels, yticklabels=short_labels,
            linewidths=0.5, annot_kws={"size": 10}, ax=ax, vmin=0, vmax=1)
ax.set_title("Regime Transition Probability Matrix\n(Row = From, Col = To)",
             fontsize=12, fontweight="bold")
ax.set_xlabel("To Regime")
ax.set_ylabel("From Regime")
plt.tight_layout()
fig.savefig(os.path.join(OUTPUT_DIR, "chart_transition_heatmap.png"),
            dpi=150, bbox_inches="tight")
plt.close()
print(f"       OK - chart_transition_heatmap.png")


# --- Chart 4: Regime Characteristics (mean annualised return + vol) ------
# Use XIU (Canadian equity) as the headline return series
eq_ticker = "XIU.TO"
eq_ret_col = f"{eq_ticker}_Return"
bond_ticker = "XBB.TO"
bond_ret_col = f"{bond_ticker}_Return"

if eq_ret_col in out_data.columns:
    fig, axes = plt.subplots(1, 3, figsize=(14, 5))
    regime_names = [REGIME_LABELS[r] for r in range(N_REGIMES)]
    regime_colors_list = [REGIME_COLORS[r] for r in range(N_REGIMES)]

    # Panel 1: annualised equity return per regime
    ann_rets = [regime_stats[r].get(f"ann_ret_{eq_ticker}", np.nan) for r in range(N_REGIMES)]
    bars = axes[0].bar(regime_names, ann_rets, color=regime_colors_list, edgecolor="white")
    axes[0].axhline(0, color="black", linewidth=0.8)
    axes[0].set_title(f"Mean Ann. Return\n({eq_ticker.replace('.TO', '')} equity)",
                      fontsize=11, fontweight="bold")
    axes[0].set_ylabel("Annualised Return")
    pct_fmt(axes[0])
    axes[0].tick_params(axis="x", labelsize=8)

    # Panel 2: annualised equity vol per regime
    ann_vols = [regime_stats[r].get(f"ann_vol_{eq_ticker}", np.nan) for r in range(N_REGIMES)]
    axes[1].bar(regime_names, ann_vols, color=regime_colors_list, edgecolor="white")
    axes[1].set_title(f"Mean Ann. Volatility\n({eq_ticker.replace('.TO', '')} equity)",
                      fontsize=11, fontweight="bold")
    axes[1].set_ylabel("Annualised Volatility")
    pct_fmt(axes[1])
    axes[1].tick_params(axis="x", labelsize=8)

    # Panel 3: equity-bond correlation per regime
    eq_bond_corr = [regime_stats[r].get("mean_eq_bond_corr", np.nan) for r in range(N_REGIMES)]
    axes[2].bar(regime_names, eq_bond_corr, color=regime_colors_list, edgecolor="white")
    axes[2].axhline(0, color="black", linewidth=0.8)
    axes[2].set_title("Mean Equity–Bond Correlation\n(XIU vs XBB, 20-day rolling)",
                      fontsize=11, fontweight="bold")
    axes[2].set_ylabel("Correlation")
    axes[2].set_ylim(-1, 1)
    axes[2].tick_params(axis="x", labelsize=8)

    for ax in axes:
        ax.grid(axis="y", alpha=0.3)
        ax.spines[["top", "right"]].set_visible(False)
        plt.setp(ax.xaxis.get_majorticklabels(), rotation=15, ha="right")

    fig.suptitle("Regime Characteristics — Gaussian HMM (Training Set)",
                 fontsize=13, fontweight="bold", y=1.01)
    plt.tight_layout()
    fig.savefig(os.path.join(OUTPUT_DIR, "chart_regime_characteristics.png"),
                dpi=150, bbox_inches="tight")
    plt.close()
    print(f"       OK - chart_regime_characteristics.png")

"""### Summary"""

print(f"\n  Model              : Gaussian HMM, {N_REGIMES} regimes, full covariance")
print(f"  Features used      : {all_features}")
print(f"  Training LL        : {best_loglik:.4f}")
print(f"  Converged          : {hmm.monitor_.converged}")
print(f"\n  Regime Breakdown (full dataset):")
for r in range(N_REGIMES):
    stats = regime_stats[r]
    print(f"    {REGIME_LABELS[r]:<26}: "
          f"{stats['n_days']:>4} days  ({stats['pct_time']:.1%})  "
          f"Avg Vol = {stats['mean_mkt_vol']:.1%}")

print(f"\n  Transition Probabilities (diagonal = regime persistence):")
for r in range(N_REGIMES):
    print(f"    {REGIME_LABELS[r]:<26}: "
          f"P(stay) = {trans_matrix[r, r]:.2%}")

print(f"\n  Key Outputs:")
print(f"    {OUTPUT_DIR}/model_hmm.pkl              ← use in Phase 3+")
print(f"    {OUTPUT_DIR}/data_with_regimes.csv      ← use in Phase 3+")
print(f"    {OUTPUT_DIR}/regime_transition_matrix.csv")
print(f"    {OUTPUT_DIR}/hmm_summary.txt")
print(f"    {OUTPUT_DIR}/chart_regime_timeline.png")
print(f"    {OUTPUT_DIR}/chart_regime_posterior.png")
print(f"    {OUTPUT_DIR}/chart_transition_heatmap.png")
print(f"    {OUTPUT_DIR}/chart_regime_characteristics.png")