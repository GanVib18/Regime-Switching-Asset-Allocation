# -*- coding: utf-8 -*-
"""data_prep.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IZ3y-M5C33AIvYxjiQYfizcmi5k8zwIY

## Data Collection & Preparation

### Install Dependencies
"""

!pip install yfinance

"""### Import Libraries"""

import os
import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import seaborn as sns
import yfinance as yf

"""### Stock Config."""

ETF_CONFIG = {
    "XIU.TO": "iShares S&P/TSX 60 (XIU)",       # Canadian equities
    "VFV.TO": "Vanguard S&P 500 Index (VFV)",    # U.S. equities (CAD-hedged)
    "XEF.TO": "iShares MSCI EAFE (XEF)",         # International developed markets
    "XBB.TO": "iShares Cdn Bond (XBB)",          # Canadian investment-grade bonds
    "CGL-C.TO":  "iShares Gold Bullion (CGL)",      # Gold (CAD, unhedged)
}

# Asset-class groupings — used in feature engineering below
EQUITY_TICKERS = ["XIU.TO", "VFV.TO", "XEF.TO"]
BOND_TICKERS   = ["XBB.TO"]
GOLD_TICKERS   = ["CGL-C.TO"]

OIL_TICKER     = "CL=F"        # WTI Crude Futures
START_DATE     = "2015-01-01"
END_DATE       = "2025-01-01"
ROLLING_WINDOW  = 20            # days
MOMENTUM_WINDOW = 60            # days

OUTPUT_DIR     = "/content"

"""### Helper Fn."""

def _normalise_index(df: pd.DataFrame) -> pd.DataFrame:
    """
    Strip timezone info and normalise the index to tz-naive midnight timestamps.

    yfinance >=0.2 returns a DatetimeIndex with UTC timezone; older versions
    return tz-naive timestamps.  Either way we normalise to midnight so that
    all DataFrames can be joined on a common, unambiguous date index.
    """
    idx = df.index
    if isinstance(idx, pd.DatetimeIndex):
        if idx.tz is not None:
            idx = idx.tz_convert("UTC").tz_localize(None)   # strip tz
        df.index = idx.normalize()   # set time component to 00:00:00
    df.index.name = "Date"
    return df


def _flatten_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    yfinance >=0.2 returns MultiIndex columns even for a single ticker.
    Keep only the first level (the field name: Open, High, Low, Close, Volume).
    """
    if isinstance(df.columns, pd.MultiIndex):
        df.columns = df.columns.get_level_values(0)
    return df


def download_single(ticker: str, start: str, end: str) -> pd.Series | None:
    """
    Download adjusted close prices for one ticker.
    Returns a named pd.Series indexed by tz-naive dates, or None on failure.
    """
    try:
        # multi_level_index=False is a yfinance >=0.2.55 flag;
        # safe to pass — older versions raise TypeError which we catch
        raw = yf.download(
            ticker,
            start=start,
            end=end,
            auto_adjust=True,
            progress=False,
            multi_level_index=False,
        )
    except TypeError:
        raw = yf.download(
            ticker, start=start, end=end,
            auto_adjust=True, progress=False,
        )

    if raw is None or raw.empty:
        return None

    raw = _flatten_columns(raw)
    raw = _normalise_index(raw)

    if "Close" not in raw.columns:
        return None

    return raw["Close"].rename(ticker)

"""### Download Adjusted Close Prices"""

print(f"\n[1/5]  Downloading price data ({START_DATE} to {END_DATE}) ...")

etf_tickers  = list(ETF_CONFIG.keys())
price_series = {}
failed       = []

for ticker in etf_tickers:
    print(f"       Fetching {ticker} ...", end=" ", flush=True)
    s = download_single(ticker, START_DATE, END_DATE)
    if s is None:
        print("WARNING - no data returned")
        failed.append(ticker)
    else:
        price_series[ticker] = s
        print(f"OK  {len(s)} rows")

if failed:
    print(f"\n  WARNING - Failed tickers: {failed}")
if not price_series:
    raise RuntimeError(
        "No ETF data could be downloaded. Check your internet connection."
    )

# Inner join: only keep dates where ALL ETFs have prices
prices_raw = pd.concat(price_series.values(), axis=1, join="inner")
prices_raw.sort_index(inplace=True)

prices_raw.to_csv(f"{OUTPUT_DIR}/raw_prices.csv")
print(f"\n  OK - Raw prices saved  ->  {OUTPUT_DIR}/raw_prices.csv")
print(f"      Shape: {prices_raw.shape}  |  "
      f"{prices_raw.index[0].date()} -> {prices_raw.index[-1].date()}")

"""### Calculate Daily Returns"""

print("\n[2/5]  Calculating daily returns ...")

daily_returns = prices_raw.pct_change().iloc[1:]   # (P_t - P_{t-1}) / P_{t-1}

# Sanity-check: flag suspicious single-day moves (>15% = likely data error)
EXTREME_THRESHOLD = 0.15
extreme = (daily_returns.abs() > EXTREME_THRESHOLD).any(axis=1)
if extreme.any():
    print(f"\n  WARNING - Suspicious daily moves (>15%) on {extreme.sum()} date(s):")
    print(daily_returns[extreme].to_string())
else:
    print("  OK - No extreme single-day moves detected (>15% threshold)")

daily_returns.to_csv(f"{OUTPUT_DIR}/daily_returns.csv")
print(f"  OK - Daily returns saved  ->  {OUTPUT_DIR}/daily_returns.csv")

"""### Download Oil Prices"""

print("\n[3/5]  Downloading WTI crude oil prices ...")

oil_returns = None
try:
    s_oil = download_single(OIL_TICKER, START_DATE, END_DATE)
    if s_oil is not None and not s_oil.empty:
        oil_returns = s_oil.pct_change().rename("WTI_Oil_Return")
        print(f"  OK - Oil downloaded: {len(s_oil)} rows")
    else:
        print("  WARNING - No oil data returned - skipping oil feature")
except Exception as exc:
    print(f"  WARNING - Oil download failed ({exc}) - skipping oil feature")

"""### Feature Engineering"""

print("\n[4/5]  Engineering features ...")

# Base the features index on daily_returns so everything aligns
features = pd.DataFrame(index=daily_returns.index)

# ── Per-ETF 20-day rolling annualised volatility ──────────────────────────
for ticker in etf_tickers:
    if ticker not in daily_returns.columns:
        continue
    features[f"{ticker}_RolVol20"] = (
        daily_returns[ticker]
        .rolling(ROLLING_WINDOW)
        .std()
        * np.sqrt(252)          # annualise: daily std * sqrt(252 trading days)
    )
print("  OK - Per-ETF 20-day rolling volatility computed")

# ── Equity-Bond rolling correlation (XIU vs XBB) ─────────────────────────
# XIU = broad Canadian equity; XBB = broad Canadian bond.
# This pair cleanly captures the equity-bond flight-to-safety dynamic.
if "XIU.TO" in daily_returns.columns and "XBB.TO" in daily_returns.columns:
    features["Equity_Bond_Corr20"] = (
        daily_returns["XIU.TO"]
        .rolling(ROLLING_WINDOW)
        .corr(daily_returns["XBB.TO"])
    )
    print("  OK - Equity-Bond 20-day rolling correlation computed (XIU vs XBB)")

# ── Market-wide rolling vol (equal-weight average of all equity ETFs) ────
equity_tickers_present = [t for t in EQUITY_TICKERS if t in daily_returns.columns]
if equity_tickers_present:
    avg_equity = daily_returns[equity_tickers_present].mean(axis=1)
    features["Market_RolVol20"] = (
        avg_equity.rolling(ROLLING_WINDOW).std() * np.sqrt(252)
    )
    print("  OK - Market-wide 20-day rolling volatility computed "
          f"(from: {', '.join(t.replace('.TO','').replace('.C','') for t in equity_tickers_present)})")

# ── Bond rolling vol ──────────────────────────────────────────────────────
bond_tickers_present = [t for t in BOND_TICKERS if t in daily_returns.columns]
if bond_tickers_present:
    avg_bond = daily_returns[bond_tickers_present].mean(axis=1)
    features["Bond_RolVol20"] = (
        avg_bond.rolling(ROLLING_WINDOW).std() * np.sqrt(252)
    )
    print("  OK - Bond 20-day rolling volatility computed")

# ── Gold rolling vol — useful regime signal (gold vol spikes in crises) ───
gold_tickers_present = [t for t in GOLD_TICKERS if t in daily_returns.columns]
if gold_tickers_present:
    avg_gold = daily_returns[gold_tickers_present].mean(axis=1)
    features["Gold_RolVol20"] = (
        avg_gold.rolling(ROLLING_WINDOW).std() * np.sqrt(252)
    )
    print("  OK - Gold 20-day rolling volatility computed")

# ── Gold-Equity rolling correlation — crisis signal ───────────────────────
# Gold tends to become negatively correlated with equities in risk-off regimes
if gold_tickers_present and "XIU.TO" in daily_returns.columns:
    features["Gold_Equity_Corr20"] = (
        daily_returns[gold_tickers_present[0]]
        .rolling(ROLLING_WINDOW)
        .corr(daily_returns["XIU.TO"])
    )
    print("  OK - Gold-Equity 20-day rolling correlation computed")

# ── 5-day momentum per ETF ────────────────────────────────────────────────
for ticker in etf_tickers:
    if ticker in daily_returns.columns:
        features[f"{ticker}_Mom5"] = daily_returns[ticker].rolling(5).sum()
print("  OK - 5-day momentum computed")

# ── 60-day momentum per ETF (regime signal for slow-burn bear markets) ────
# Mirrors the per-ETF vol logic: individual series first, then aggregates.
# Bond_Mom60 and simultaneous Equity_Mom60 both negative = rate-shock signature
for ticker in etf_tickers:
    if ticker in daily_returns.columns:
        features[f"{ticker}_Mom60"] = daily_returns[ticker].rolling(MOMENTUM_WINDOW).sum()
print("  OK - Per-ETF 60-day momentum computed")

# Aggregate equity momentum (equal-weighted, mirrors Market_RolVol20)
equity_tickers_present = [t for t in EQUITY_TICKERS if t in daily_returns.columns]
if equity_tickers_present:
    features["Equity_Mom60"] = (
        daily_returns[equity_tickers_present]
        .mean(axis=1)
        .rolling(MOMENTUM_WINDOW)
        .sum()
    )
    print("  OK - Aggregate equity 60-day momentum computed "
          f"(from: {', '.join(t.replace('.TO','') for t in equity_tickers_present)})")

# Bond momentum separately (sustained bond selloff = rate shock regime)
bond_tickers_present = [t for t in BOND_TICKERS if t in daily_returns.columns]
if bond_tickers_present:
    features["Bond_Mom60"] = (
        daily_returns[bond_tickers_present]
        .mean(axis=1)
        .rolling(MOMENTUM_WINDOW)
        .sum()
    )
    print("  OK - Bond 60-day momentum computed")

# Gold momentum (gold rallying while equities fall = risk-off confirmation)
gold_tickers_present = [t for t in GOLD_TICKERS if t in daily_returns.columns]
if gold_tickers_present:
    features["Gold_Mom60"] = (
        daily_returns[gold_tickers_present]
        .mean(axis=1)
        .rolling(MOMENTUM_WINDOW)
        .sum()
    )
    print("  OK - Gold 60-day momentum computed")

# ── Rolling 60-day drawdown — key for separating acute vs slow-burn crises ──
# COVID = massive spike then recovery; 2022 = sustained drawdown, moderate vol
# This feature encodes "how far are we from recent highs" rather than
# "how volatile are daily moves right now"
equity_tickers_present = [t for t in EQUITY_TICKERS if t in daily_returns.columns]
if equity_tickers_present:
    avg_equity_price = prices_raw[equity_tickers_present].mean(axis=1)
    rolling_max = avg_equity_price.rolling(window=60).max()
    drawdown_series = (avg_equity_price - rolling_max) / rolling_max
    # Reindex to features index (features starts after burn-in)
    features["Equity_Drawdown60"] = drawdown_series.reindex(features.index)
    print("  OK - 60-day equity drawdown computed")

if "XBB.TO" in prices_raw.columns:
    bond_rolling_max = prices_raw["XBB.TO"].rolling(window=60).max()
    bond_drawdown = (prices_raw["XBB.TO"] - bond_rolling_max) / bond_rolling_max
    features["Bond_Drawdown60"] = bond_drawdown.reindex(features.index)
    print("  OK - 60-day bond drawdown computed")

# ── WTI Oil returns — align to our ETF trading-day index ─────────────────
if oil_returns is not None:
    oil_aligned = oil_returns.reindex(features.index, method="ffill")
    features["WTI_Oil_Return"] = oil_aligned
    print("  OK - WTI oil returns aligned and added")

# NOTE: Regime column intentionally omitted here — Phase 2 will add it
# to cleaned_data.csv after running the HMM/GMM model.

features.to_csv(f"{OUTPUT_DIR}/features.csv")
print(f"  OK - Features saved  ->  {OUTPUT_DIR}/features.csv")
print(f"      Features shape: {features.shape}")

"""### Clean & Align Data"""

print("\n[5/5]  Cleaning & aligning master dataset ...")

# Rename columns to avoid collisions when joining
prices_renamed = prices_raw.copy()
prices_renamed.columns = [f"{c}_Price" for c in prices_renamed.columns]

returns_renamed = daily_returns.copy()
returns_renamed.columns = [f"{c}_Return" for c in returns_renamed.columns]

# Inner join on index — all three DataFrames share the same normalised
# tz-naive DatetimeIndex so this will match correctly
master = (
    prices_renamed
    .join(returns_renamed, how="inner")
    .join(features,        how="inner")
)
master.sort_index(inplace=True)
print(f"  After inner join        : {len(master):>5} rows")

# Diagnostic: show how many NaNs exist per column before trimming
nan_counts = master.isnull().sum()
nan_cols   = nan_counts[nan_counts > 0]
if not nan_cols.empty:
    print(f"\n  NaN counts before burn-in trim:")
    for col, cnt in nan_cols.items():
        print(f"    {col}: {cnt}")

# Drop burn-in period — use the longest rolling window across all features
# (MOMENTUM_WINDOW=60 > ROLLING_WINDOW=20, so trim 60 rows)
BURN_IN = max(ROLLING_WINDOW, MOMENTUM_WINDOW)
master = master.iloc[BURN_IN:]
print(f"  After burn-in trim      : {len(master):>5} rows  "
      f"(removed first {BURN_IN} rows)")
# Forward-fill small scattered gaps (e.g. oil missing 1-2 days)
master = master.ffill(limit=3)

# Drop remaining NaNs — but EXCLUDE any all-NaN columns (e.g. Regime)
# We use subset= to only check columns that should have real data
data_cols = [c for c in master.columns if master[c].notna().any()]
before = len(master)
master.dropna(subset=data_cols, inplace=True)
dropped = before - len(master)
print(f"  After dropna            : {len(master):>5} rows  "
      f"(removed {dropped} rows with NaNs)")

# Guard against empty result
if master.empty:
    raise RuntimeError(
        "Master DataFrame is empty after cleaning!\n"
        "Debug hint: run the script with verbose=True and check the NaN "
        "counts printed above.  Most likely cause: the inner join on "
        "prices/returns/features produced no overlapping dates."
    )

master.to_csv(f"{OUTPUT_DIR}/cleaned_data.csv")
print(f"\n  OK - Cleaned data saved  ->  {OUTPUT_DIR}/cleaned_data.csv")
print(f"      Final shape : {master.shape}")
print(f"      Date range  : {master.index[0].date()} -> {master.index[-1].date()}")

"""### Data Quality"""

ret_cols = [c for c in master.columns if c.endswith("_Return")]
stats    = master[ret_cols].describe().T
ann_ret  = stats["mean"] * 252
ann_vol  = stats["std"]  * np.sqrt(252)
sharpe   = ann_ret / ann_vol

report_lines = [
    "REGIME-SWITCHING ASSET ALLOCATION — PHASE 1 REPORT",
    "=" * 60,
    f"Date range  : {master.index[0].date()} -> {master.index[-1].date()}",
    f"Trading days: {len(master)}",
    f"Columns     : {master.shape[1]}",
    "",
    "ANNUALISED RETURN STATISTICS",
    f"  {'ETF':<35}  {'Ann Ret':>8}  {'Ann Vol':>8}  {'Sharpe':>7}",
    "  " + "-" * 63,
]

print(f"\n  {'ETF':<35}  {'Ann Ret':>8}  {'Ann Vol':>8}  {'Sharpe':>7}")
print("  " + "-" * 63)

for col in ret_cols:
    ticker = col.replace(".TO_Return", "").replace("_Return", "")
    label  = ETF_CONFIG.get(f"{ticker}.TO", ETF_CONFIG.get(ticker, ticker))
    r, v, s = ann_ret[col], ann_vol[col], sharpe[col]
    line = f"  {label:<35}  {r:>+8.1%}  {v:>8.1%}  {s:>7.2f}"
    print(line)
    report_lines.append(line)

report_lines += ["", "MISSING DATA CHECK"]
missing = master.isnull().sum()
if missing.any():
    for col, cnt in missing[missing > 0].items():
        msg = f"  {col}: {cnt} missing values"
        report_lines.append(msg)
        print(msg)
else:
    msg = "  OK - No missing values in final cleaned dataset"
    report_lines.append(msg)
    print(msg)

with open(f"{OUTPUT_DIR}/data_summary.txt", "w") as f:
    f.write("\n".join(report_lines))
print(f"\n  OK - Summary report saved  ->  {OUTPUT_DIR}/data_summary.txt")

"""### Charts"""

COLORS = ["#E63946", "#457B9D", "#2A9D8F", "#E9C46A", "#F4A261"]

def add_year_grid(ax):
    ax.xaxis.set_major_formatter(mdates.DateFormatter("%Y"))
    ax.xaxis.set_major_locator(mdates.YearLocator())
    ax.grid(alpha=0.3)
    plt.setp(ax.xaxis.get_majorticklabels(), rotation=0)


# ── Chart 1: Normalised Price Performance ────────────────────────────────
price_cols  = [c for c in master.columns if c.endswith("_Price")]
norm_prices = master[price_cols].div(master[price_cols].iloc[0]) * 100
labels      = list(ETF_CONFIG.values())

fig, ax = plt.subplots(figsize=(13, 6))
for i, col in enumerate(price_cols):
    ax.plot(norm_prices.index, norm_prices[col],
            label=labels[i] if i < len(labels) else col,
            color=COLORS[i % len(COLORS)], linewidth=1.5)
ax.set_title("Normalised Price Performance (Base = 100)",
             fontsize=14, fontweight="bold")
ax.set_ylabel("Indexed Price")
ax.legend(loc="upper left", fontsize=8)
add_year_grid(ax)
plt.tight_layout()
plt.savefig(f"{OUTPUT_DIR}/chart_prices.png", dpi=150, bbox_inches="tight")
plt.close()
print(f"  OK - {OUTPUT_DIR}/chart_prices.png")


# ── Chart 2: Rolling Volatility (per ETF) ────────────────────────────────
# Exclude the aggregate market/bond/gold vol columns — those are summary signals
vol_cols = [c for c in master.columns
            if c.endswith("_RolVol20")
            and not any(x in c for x in ["Market", "Bond", "Gold_RolVol20"])]

fig, ax = plt.subplots(figsize=(13, 5))
for i, col in enumerate(vol_cols):
    short = (col.replace(".TO_RolVol20", "")
               .replace(".C_RolVol20", "")
               .replace("_RolVol20", ""))
    ax.plot(master.index, master[col], label=short,
            color=COLORS[i % len(COLORS)], linewidth=1.2, alpha=0.85)
ax.axhline(0.20, color="gray", linestyle="--", linewidth=1, label="20% level")
ax.set_title("20-Day Rolling Annualised Volatility by ETF",
             fontsize=14, fontweight="bold")
ax.set_ylabel("Annualised Volatility")
ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f"{x:.0%}"))
ax.legend(loc="upper right", fontsize=8)
add_year_grid(ax)
plt.tight_layout()
plt.savefig(f"{OUTPUT_DIR}/chart_volatility.png", dpi=150, bbox_inches="tight")
plt.close()
print(f"  OK - {OUTPUT_DIR}/chart_volatility.png")


# ── Chart 3: Equity-Bond Correlation ─────────────────────────────────────
if "Equity_Bond_Corr20" in master.columns:
    corr_s = master["Equity_Bond_Corr20"]
    fig, ax = plt.subplots(figsize=(13, 4))
    ax.plot(master.index, corr_s, color="#264653", linewidth=1.2)
    ax.axhline(0, color="red", linestyle="--", linewidth=1)
    ax.fill_between(master.index, corr_s, 0,
                    where=(corr_s < 0), alpha=0.2, color="green",
                    label="Negative (diversifying)")
    ax.fill_between(master.index, corr_s, 0,
                    where=(corr_s >= 0), alpha=0.2, color="red",
                    label="Positive (correlated)")
    ax.set_title("XIU vs XBB — 20-Day Rolling Correlation\n"
                 "(Negative = bonds diversify equity risk)",
                 fontsize=13, fontweight="bold")
    ax.set_ylabel("Correlation")
    ax.set_ylim(-1, 1)
    ax.legend(fontsize=9)
    add_year_grid(ax)
    plt.tight_layout()
    plt.savefig(f"{OUTPUT_DIR}/chart_correlation.png", dpi=150, bbox_inches="tight")
    plt.close()
    print(f"  OK - {OUTPUT_DIR}/chart_correlation.png")


# ── Chart 4: Return Correlation Heatmap ──────────────────────────────────
short_names = {c: (c.replace(".TO_Return", "")
                    .replace(".C_Return", "")
                    .replace("_Return", ""))
               for c in ret_cols}
corr_matrix = master[ret_cols].rename(columns=short_names).corr()

fig, ax = plt.subplots(figsize=(8, 6))
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap="RdYlGn",
            center=0, vmin=-1, vmax=1, ax=ax,
            linewidths=0.5, annot_kws={"size": 10})
ax.set_title("Full-Period Return Correlation Matrix",
             fontsize=13, fontweight="bold")
plt.tight_layout()
plt.savefig(f"{OUTPUT_DIR}/chart_heatmap.png", dpi=150, bbox_inches="tight")
plt.close()
print(f"  OK - {OUTPUT_DIR}/chart_heatmap.png")


# ── Chart 5: Gold-Equity & Gold-Bond Correlation ─────────────────────────
# Gold's correlation to equities is one of the cleanest regime indicators:
# it goes negative (gold rallies while stocks fall) in risk-off regimes.
gold_corr_cols = [c for c in master.columns if "Gold_Equity" in c or "Gold" in c
                  and "Corr" in c]
if "Gold_Equity_Corr20" in master.columns:
    fig, ax = plt.subplots(figsize=(13, 4))
    ax.plot(master.index, master["Gold_Equity_Corr20"],
            color="#F4A261", linewidth=1.3, label="Gold vs XIU (Canadian equity)")
    if "Equity_Bond_Corr20" in master.columns:
        ax.plot(master.index, master["Equity_Bond_Corr20"],
                color="#457B9D", linewidth=1.0, alpha=0.6,
                linestyle="--", label="Equity vs Bond (reference)")
    ax.axhline(0, color="red", linestyle=":", linewidth=1)
    ax.fill_between(master.index, master["Gold_Equity_Corr20"], 0,
                    where=(master["Gold_Equity_Corr20"] < 0),
                    alpha=0.2, color="#F4A261", label="Gold decorrelating (risk-off signal)")
    ax.set_title("Gold-Equity 20-Day Rolling Correlation\n"
                 "(Negative = gold acting as safe haven)",
                 fontsize=13, fontweight="bold")
    ax.set_ylabel("Correlation")
    ax.set_ylim(-1, 1)
    ax.legend(fontsize=9)
    add_year_grid(ax)
    plt.tight_layout()
    plt.savefig(f"{OUTPUT_DIR}/chart_gold_correlation.png", dpi=150, bbox_inches="tight")
    plt.close()
    print(f"  OK - {OUTPUT_DIR}/chart_gold_correlation.png")